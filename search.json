[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This comes from the file data.qmd.\nYour first steps in this project will be to find data to work on.\nI recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset_ignore folder. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Add the files just like you would any other file.\nIf you create a folder named data this will cause problems.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour load_and_clean_data.R file is how you will load and clean your data. Here is a an example of a very simple one.\n\nsource(\n  \"scripts/load_and_clean_data.R\",\n  echo = TRUE # Use echo=FALSE or omit it to avoid code output  \n)\n\n\n&gt; library(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n&gt; loan_data &lt;- read_csv(here::here(\"dataset\", \"loan_refusal.csv\"))\n\n\nRows: 20 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): bank\ndbl (4): min, white, himin, hiwhite\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n&gt; loan_data_clean &lt;- loan_data\n\n&gt; write_csv(loan_data_clean, file = here::here(\"dataset\", \n+     \"loan_refusal_clean.csv\"))\n\n&gt; save(loan_data_clean, file = here::here(\"dataset/loan_refusal.RData\"))\n\n\nYou should never use absolute paths (eg. /Users/danielsussman/path/to/project/ or C:\\MA415\\\\Final_Project\\).\nYou might consider using the here function from the here package to avoid path problems.\n\n\nLoad and clean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which could be plain text files or .RData files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use [cleaning script](/scripts/load_and_clean_data.R) wich appears as cleaning script."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones and summarize the rest.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your load_and_clean_data.R file.\nRrename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your load_and_clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "data.html#background",
    "href": "data.html#background",
    "title": "Data",
    "section": "Background",
    "text": "Background\n\nPolice Fatality Dataset\nThe source of the original dataset is Fatal Encounters, a blog created by Dr. Brian Burghart, a journalist and part-time researcher for the University of Southern California. Dr. Burghart and his associates compiled media reports and police records to create the database. The data represents deaths that happened when police were present or were caused by police. The goal of Fatal Encounters is to create a national database of people killed during interactions that anyone can use for whatever purpose. This dataset has 36 variables, including location, date, descriptors of the deceased such as race and age, and other information surrounding the fatalities. \nThe cleaned dataset was posted by Chris Awram to the data.world site. He cleaned the Fatal Encounters dataset and combined it with additional data from Gun Violence Archive and Data Society, which sourced its data from the Washington Post. The purpose of this cleaned dataset was to shed light on altercations with the police in which individuals were killed. This dataset subsets the Fatal Encounters dataset, discarding observations where the deceased individual was not killed by police or by suicide, but only in the presence of a police officer. For example, the death recorded by Unique ID 31495 in the Fatal Encounters dataset resulted from a car accident that was witnessed by a police officer. This observation was filtered out from the cleaned dataset because the deceased was neither killed by a police officer nor a victim of suicide.\nThis dataset has 12 variables, including a unique ID for each observation (uid). The variables name, age, gender, and race describe attributes of the deceased. Date, city, and state describe the location and time of the altercation. Manner_of_death indicates whether the individual died by being shot, tasered, both, or some other method. Armed represents what weapon, if any, the individual was carrying, whether a gun, knife, or some other weapon. Mental_illness indicates whether the individual was mentally ill and flee indicates whether the individual attempted to flee the altercation. This cleaned dataset removed some supplemental details from the Fatal Encounter dataset, such as the address of incident, the corresponding latitude and longitude, the agencies involved, and a description of the incident.\n\n\nU.S. Cities Demographic Dataset\nWe supplemented our police fatality dataset by joining it with a U.S. cities demographic dataset. This demographic dataset was sourced from the U.S. Census Bureau’s 2015 American Community Survey and posted to the opendatasoft site by the U.S. Census Bureau. U.S. cities with populations greater than or equal to 65,000 are included in this dataset. There are 12 variables, which cover location, age, sex, and race, among other subjects. The variables City, State, and State Code describe location. Many of the variables are self-explanatory: Total Population, Male Population, Female Population, Number of Veterans, Median Age, and Average Household Size. Foreign-born provides a count of the number of individuals not born in the U.S. The variables Race and Count are connected, with Count describing the number of individuals of a certain race. There are multiple observations per City, each with a different Race and Count."
  },
  {
    "objectID": "data.html#cleaning",
    "href": "data.html#cleaning",
    "title": "Data",
    "section": "Cleaning",
    "text": "Cleaning\nThe police fatality dataset from data.world required no additional cleaning when first downloaded. Our process of loading the police fatality data can be found here. Before we could combine datasets, we first needed to pivot the U.S. demographic dataset so that there was only one observation for each city. To do so, we used pivot_longer to change the Race variable into multiple Race variables (e.g. Asian, White), each with different values from Count. We then combined the police and demographic datasets by using inner_join with City as the primary key. We set the relationship to “many-to-many” so that fatalities from the same city all had the demographic data added. In the join, the State variables from each dataset got renamed to State.x and State.y. We used select to remove the redundant State.y variable and rename to rename State.x back to State. Some of the observations were missing Race values for the deceased individuals. As race is our main subject of interest, we removed these observations with filter. Our process of combining the datasets can be found here."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nPost 5 Team 2\n\n\n\n\n\nPost 5 from team 2\n\n\n\n\n\n\nApr 15, 2024\n\n\n\n\n\n\n  \n\n\n\n\nblog post 4\n\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2024\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 3\n\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2024\n\n\n\n\n\n\n  \n\n\n\n\nPost 2\n\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\n\n\n\n\n  \n\n\n\n\nPost 1\n\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nTeam 2\n\n\n\n\n\n\n  \n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post.\n\n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nTest Post\n\n\n\n\n\nTesting\n\n\n\n\n\n\nFeb 23, 2024\n\n\nCarolyn Beigh\n\n\n\n\n\n\n  \n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting.\n\n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-03-22-post-2-/post-2-.html",
    "href": "posts/2024-03-22-post-2-/post-2-.html",
    "title": "Post 2",
    "section": "",
    "text": "We are working with the US Police Fatalities data set. For this post, we Set up data loading and cleaning in the load_and_clean_data.R file. The required rds file was created. Since the data set we found was already cleaned, we did not do anything specific to clean the data further.\nAnswer all the questions from the data background part and update it to the data page in the website. \n\nWrite some paragraphs about data equity.(still working on it, so this is not updated)."
  },
  {
    "objectID": "posts/2024-04-15-post-5-team-2/post-5-team-2.html",
    "href": "posts/2024-04-15-post-5-team-2/post-5-team-2.html",
    "title": "Post 5 Team 2",
    "section": "",
    "text": "We combined police fatalities data set with us cities demographics data set with an R script called combining-data sets. The us cities demographics data set is originally one of the data sets comes from our brain storming in the Post 1 and we consider it is interested to see some interaction between our current data set. In order to make these two data set fit, we pivot the cities demographics data set so that there is only one observation for each city and count for the each races for their own column. Then, we inner join to keep only the data that has a match from each data set with city variable, and rename unnecessary name. Finally, we filter out all the observations with missing Race values as race is the main variable of interest."
  },
  {
    "objectID": "posts/2024-04-07-blog-post-4/blog-post-4.html",
    "href": "posts/2024-04-07-blog-post-4/blog-post-4.html",
    "title": "blog post 4",
    "section": "",
    "text": "Impact of Gender on Fatalities in Different States This plot focuses to explore the relationship between gender and fatalities across different states. This could help identify if certain states have more pronounced gender disparities in police-related fatalities.\n\nsuppressPackageStartupMessages(library(tidyverse))\n\nprc &lt;- read_csv(\"dataset/police_fatalities.csv\", show_col_types = FALSE)\n\nprc %&gt;%\n  na.omit() %&gt;%\n  count(State, Gender) %&gt;%\n  ggplot(aes(x = reorder(State, n), y = n, fill = Gender)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Police Fatalities by State and Gender\",\n       x = \"State\",\n       y = \"Number of Fatalities\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nYearly Trend of Police Fatalities by Race This plot the trend of police fatalities over the years, broken down by race. This can show if certain races have been more affected over time and how the trends have changed.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(readr)\n\npolice_data_clean &lt;- read_csv(\"dataset/police_fatalities.csv\", show_col_types = FALSE)\n\nannual_trends_by_race &lt;- police_data_clean %&gt;%\n  mutate(Date = mdy(Date), Year = year(Date)) %&gt;%\n  drop_na(Year, Race) %&gt;%\n  count(Year, Race) %&gt;%\n  spread(key = Race, value = n, fill = 0) %&gt;%\n  gather(key = 'Race', value = 'Fatalities', -Year)\n\nannual_trends_by_race %&gt;%\n  ggplot(aes(x = Year, y = Fatalities, color = Race)) +\n  geom_line() + \n  geom_point() + \n  labs(title = \"Yearly Trend of Police Fatalities by Race\",\n       x = \"Year\",\n       y = \"Number of Fatalities\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.title = element_blank()) \n\n\n\n\nAge Histogram: Displays the frequency distribution of individuals’ ages involved in the incidents.\nBar Plot of Manner of Death by Gender and Race: Compares the number of incidents by manner of death across different races and genders.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(broom)\n\nlibrary(nnet)\n\ndata &lt;- read.csv(\"dataset/police_fatalities.csv\") %&gt;%\n  mutate(\n    Date = mdy(Date), \n    Gender = as.factor(Gender),\n    Race = as.factor(Race),\n    Manner_of_death = as.factor(Manner_of_death),\n    Armed = as.factor(Armed),\n    Mental_illness = as.factor(Mental_illness),\n    Flee = as.factor(Flee)\n  )\n\nsummary_statistics &lt;- data %&gt;%\n  summarise(\n    Average_Age = mean(Age, na.rm = TRUE),\n    SD_Age = sd(Age, na.rm = TRUE),\n    Count = n()\n  )\n\nage_histogram &lt;- ggplot(data, aes(x = Age)) +\n  geom_histogram(binwidth = 5, fill = \"blue\", color = \"black\") +\n  theme_minimal() +\n  labs(title = \"Distribution of Age\", x = \"Age\", y = \"Count\")\n\ndeath_by_demographics &lt;- ggplot(data, aes(x = Race, fill = Gender)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal() +\n  labs(title = \"Manner of Death by Gender and Race\", x = \"Race\", y = \"Count\")\n\nlogistic_model &lt;- glm(Armed ~ Age + Gender + Race + Mental_illness, \n                      data = data, family = \"binomial\")\nlogistic_model_summary &lt;- summary(logistic_model)\n\nmultinom_model &lt;- multinom(Manner_of_death ~ Age + Gender + Race + Mental_illness, data = data)\n\n# weights:  48 (33 variable)\ninitial  value 16993.196279 \niter  10 value 3526.384408\niter  20 value 3018.452673\niter  30 value 2924.878850\niter  40 value 2899.497411\niter  50 value 2899.477616\niter  60 value 2899.473481\niter  60 value 2899.473459\niter  60 value 2899.473459\nfinal  value 2899.473459 \nconverged\n\nmultinom_model_summary &lt;- summary(multinom_model)\n\nlogistic_model_table &lt;- tidy(logistic_model)\n\nprint(age_histogram)\n\nWarning: Removed 233 rows containing non-finite values (`stat_bin()`).\n\n\n\n\nprint(death_by_demographics)\n\n\n\nprint(logistic_model_table)\n\n# A tibble: 11 × 5\n   term               estimate std.error statistic  p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)        -0.693     0.919      -0.754 4.51e- 1\n 2 Age                 0.00396   0.00146     2.71  6.70e- 3\n 3 GenderFemale        0.374     0.922       0.406 6.85e- 1\n 4 GenderMale          0.518     0.918       0.564 5.73e- 1\n 5 RaceAsian           0.0773    0.157       0.492 6.23e- 1\n 6 RaceBlack           0.248     0.0523      4.73  2.24e- 6\n 7 RaceHispanic        0.124     0.0583      2.12  3.38e- 2\n 8 RaceNative          0.238     0.180       1.32  1.87e- 1\n 9 RaceOther          -0.262     0.293      -0.894 3.72e- 1\n10 RaceWhite           0.350     0.0465      7.53  5.10e-14\n11 Mental_illnessTRUE  0.216     0.0456      4.73  2.23e- 6"
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2024-03-29-blog-post-3/blog-post-3.html",
    "href": "posts/2024-03-29-blog-post-3/blog-post-3.html",
    "title": "Blog Post 3",
    "section": "",
    "text": "Data for Equity\nPrinciple 1: One of the important principles in the process of data processing analysis is to be transparent about the limits of the data. While we are examining and cleaning the data, we noticed that there are many limitations with our data set in the research of US Police Involved Fatalities. Specifically, our data is effective while we are trying to examine the relationship between the police involvement and the fatalities of citizens. However, the data is definitely time-sensitive and our finding through this data set may not imply the Police involved fatalities in the past years. Besides, there is also missing data like whether the suspect is armed. If ARMED shows as blank then there is no record of whether the suspect is armed or not, which may play an important role in our data analysis and our result.\nPrinciple 2: For the beneficence principle regarding our dataset, it contains only the name of each police fatalities with no other sensitive information displayed. This could ensure the privacy of the person as well as their families. All other columns of the dataset are only served for the purpose of analyzing without disclosing much information about a single person. The dataset also contains a column called “uid” that can be uniquely identified across the dataset and can be used to substitute the function of names in certain occasions.\nPrinciple 3: another important principle is the Inclusivity of the data in representation. We use this principle to emphasize the importance of representing all affected communities fairly and accurately in the dataset. In the US Police Fatalities dataset, it literally means all the data reflected the demographic characteristics of those killed by police deaths, including race, age, gender, and mental health status. It can be essential in that it is an effective way to gain an insight into potential biases or differences in the ways diverse social groups may be affected by police action.\nPrinciple 4: We also need to mention accountability when we use data in real world practice. In the practice of data, we need to address any harm that the dataset and its analysis may cause, particularly to the minority communities. In the dataset US Police Fatalities, there’s a significant risk that improper interpretation of the data could underscore the stereotypes or contribute to unjust narratives about certain groups.\nWeekly Summary:\nWe finished the Data Equity part.\nWe thought about 3-4 principles and how they could be relevant to our US Police Fatalities dataset. These principles help our data set to be used ethically and constructively.\nThe main aspect of our data principles is the potential for abuse or misuse based on the potential biased narratives in our dataset.\nWe started to do some data exploration with plots and tables. It might be interesting if we combined our dataset with another containing more population information for the states. We have a homicide dataset that we might want to add to our police fatality dataset. We also thought about how to construct our data plan and what questions we want to ask of our dataset.\nExploration\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nprc &lt;- read_csv(\"dataset/police_fatalities.csv\")\n\nRows: 12491 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Name, Gender, Race, Date, City, State, Manner_of_death, Armed\ndbl (2): UID, Age\nlgl (2): Mental_illness, Flee\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nprc |&gt; \n  na.omit() |&gt;  \n  ggplot(aes(x = Race, fill = Gender)) + \n  geom_bar()\n\n\n\n\n\nprc |&gt; na.omit() |&gt; group_by(State) |&gt; summarize(n = n()) |&gt; arrange(-n)\n\n# A tibble: 51 × 2\n   State     n\n   &lt;chr&gt; &lt;int&gt;\n 1 CA      966\n 2 TX      494\n 3 FL      310\n 4 AZ      194\n 5 NV      153\n 6 NY      137\n 7 WA      137\n 8 CO      125\n 9 OH      123\n10 IL      121\n# ℹ 41 more rows\n\n\nBelow is a plot displaying the average age of police fatalities each year, separated by race.\n\nsuppressWarnings({\nlibrary(tidyverse)\nlibrary(lubridate)\n\npolice_data_clean &lt;- read_csv(here::here(\"dataset\", \"police_fatalities.csv\"))\n\npolice_fatalities_summary &lt;- police_data_clean %&gt;%\n  mutate(Date = dmy(Date),  \n         Year = year(Date)) %&gt;%  \n  group_by(Year, Race) %&gt;% \n  summarise(average_age = mean(Age, na.rm = TRUE)) \n\npolice_fatalities_summary %&gt;%\n  ggplot(aes(x = Year, y = average_age, color = Race)) +\n  geom_line() +  \n  geom_point() + \n  labs(\n    title = \"Average Age of Police Fatalities by Year and Race\",\n    x = \"Year\",\n    y = \"Average Age\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) \n})\n\nRows: 12491 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Name, Gender, Race, Date, City, State, Manner_of_death, Armed\ndbl (2): UID, Age\nlgl (2): Mental_illness, Flee\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'Year'. You can override using the `.groups` argument.\n\n\nWarning: Removed 7 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 7 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2024-03-04-post-1/post-1.html",
    "href": "posts/2024-03-04-post-1/post-1.html",
    "title": "Post 1",
    "section": "",
    "text": "US Police Fatalities\nOriginal source link, Cleaned data link\nThe original data has 36 columns and 31,498 rows, including a header. The cleaned data has 12 columns and 12,491 rows, not including a header. The original data was compiled by Fatal Encounters which used media reports and police records to create the database The data includes deaths that happened when police were present or were caused by police. The goal of Fatal Encounters is to create a national database of people killed during interactions that anyone can use for whatever purpose. The cleaned data also lists the Gun Violence Archive and Data Society as sources. Both the Fatal Encounters and the cleaned data are able to be loaded into R without issue. Some obvious questions center around race and whether certain races experience more police fatalities than others. Similar questions can be asked about gender. We might also be interested in how police encounters change overtime, and that might be indicative of changes in police policy or training. Mental health might be another variable to study. There’s been some debate on whether police should be called in response to mental health crises and a closer look at the data might shed some light on that issue. The main challenge would be cleaning the data if we chose to use the original data set. Another challenge would be combining that data with other datasets that might add new perspectives. For example, adding demographic data for cities in the dataset or additional information about police training.\nCOVID-19 National Case Surveillance\nSource: https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data/vbim-akqf/about_data\nThis data set has 104,544,006 rows, 12 columns. This dataset is sourced from database of CDCP, Centers for Disease Control and Prevention, provided by CDC Data, Analytics and Visualization Task Force. The dataset is collected mainly for the tracking purpose of COVID-19 cases, and it includes detailed information of each infected patient, like demographics, any exposure history, disease severity indicators and outcomes, presence of any underlying medical conditions and risk behaviors. The only possible challenge might be the huge sample size.\nMain questions to address:\nAre there any correlation between COVID-19 cases and geographical differences? Which race and aged group are most likely to be infected by COVID-19?\nUSA Unemployment Rates by Demographics & Race Origin: https://fred.stlouisfed.org/series/CNP16OV, https://www.epi.org/data/ cleaned:https://www.kaggle.com/datasets/asaniczka/unemployment-rates-by-demographics-1978-2023\nRows:537, columns: 122 The data is sourced from the Economic Policy Institute’s State of Working America Data Library and economic research conducted by the Federal Reserve Bank of St. Louis. Questions: how unemployment rates have changed for different groups of people over time. Look into how education levels can affect unemployment rates. Use the data to create visuals that show how unemployment rates differ across all sorts of factors.\nHomicides data over the past decade in 50 US cities Original source: https://www.kaggle.com/datasets/joebeachcapital/homicides This dataset has 52179 rows and 12 columns. The Washington Post collected data on more than 52,000 criminal homicides over the past decade in 50 of the largest American cities. The data included the location of the killing, whether an arrest was made and, in most cases, basic demographic information about each victim. Reporters received data in many formats, including paper, and worked for months to clean and standardize it, comparing homicide counts and aggregate closure rates with FBI data to ensure the records were as accurate as possible.\nOn the geographical side, the main question I hope to address would probably be finding the cities that have the highest and lowest homicide rates over the past decade. Also I might want to address how homicide rates correlate with other city-specific factors, such as population density, economic indicators, or police force size.\nFor the data quality and completeness of this dataset, I think some of the challenges would be to verify the accuracy of demographic information and the potential underreporting of certain categories of homicides."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a website for the final project for MA[46]15 Data Science with R by Team TEAM 2. The members of this team are below.\n\nChongjun Shan\n\n\nCarolyn Beigh\nCarolyn is an undergraduate majoring in Marine Science with Biology and Statistics minors.\n\n\nHaotian Wu\nHaotian is an undergraduate double majoring in Pure & Applied Mathematics and Computer Science.\n\n\nChen Yang\nChen is a graduate MA from economic department.\n\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.Rmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this pge should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nInteractive\nYou will also be required to make an interactive dashboard like this one.\nYour Big Data page should include a link to an interactive dashboard. The dashboard should be created either using Shiny or FlexDashboard (or another tool with professor’s approval). This interactive component should in some way support your thesis from your big picture page. Good interactives often provide both high-level understanding of the data while allowing a user to investigate specific scenarios, observations, subgroups, etc.\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions from the Big Picture? Plotly with default hover text will get no credit. Be creative!\n\n\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  }
]