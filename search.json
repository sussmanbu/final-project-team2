[
  {
    "objectID": "data.html#background",
    "href": "data.html#background",
    "title": "Data",
    "section": "Background",
    "text": "Background\n\nPolice Fatality Dataset\nThe source of the original dataset is Fatal Encounters, a blog created by Dr. Brian Burghart, a journalist and part-time researcher for the University of Southern California. Dr. Burghart and his associates compiled media reports and police records to create the database. The data represents deaths that occurred when police were present or were caused by police, including “on-duty, off-duty, criminal, line-of-duty, local, federal, intentional, [and] accidental.” The goal of Fatal Encounters is to create a national database that anyone can use for whatever purpose. This dataset has 36 variables, though a few are redundant, such as Unique ID and Unique identifier (redundant), or temporary. Some of the main variables are listed in the table below. Many of the variables are self-evident and therefore are lacking a description.\n\n\n\n\n\n\n\n\n\nCategory\nType\nName\nDescription\n\n\n\n\nVictim\nCharacter\nName\n-\n\n\nVictim\nInteger\nAge\n-\n\n\nVictim\nCharacter\nGender\n-\n\n\nVictim\nCharacter\nURL of image (PLS NO HOTLINKS)\nPhoto of the victim\n\n\nIncident\nNumeric (Date)\nDate of injury resulting in death (month/day/year)\n-\n\n\nLocation\nCharacter\nLocation of injury (address)\n-\n\n\nLocation\nCharacter\nLocation of death (city)\n-\n\n\nLocation\nCharacter\nState\nInputted as state abbreviations\n\n\nIncident\nCharacter\nAgency or agencies involved\nPolice departments, shrieff’s offices, etc involved in the incident\n\n\nIncident\nCharacter\nArmed/Unarmed\nWhether the victim was armed or unarmed during the incident\n\n\nIncident\nCharacter\nAlleged weapon\nIf the victim was armed, categorizes the alleged weapon\n\n\nIncident\nCharacter\nAggressive physical movement\nDescription of aggressive physical movement the victim performed (e.g. Advanced toward officer(s))\n\n\nIncident\nCharacter\nFleeing/Not fleeing\nWhether the victim attempted to flee, and if so by what method (e.g. Vehicle)\n\n\nIncident\nCharacter\nBrief description\nA brief description of the incident\n\n\nIncident\nCharacter\nSupporting document link\nA link to a new article reporting the incident or a photo of an official document describing the incident\n\n\n\nThe cleaned dataset was posted by Chris Awram to the data.world site. He cleaned the Fatal Encounters dataset and combined it with additional data from Gun Violence Archive and Data Society, which sourced its data from the Washington Post. The purpose of this cleaned dataset was to shed light on altercations with the police in which individuals were killed. This dataset subsets the Fatal Encounters dataset, discarding observations where the deceased individual was not killed by police or by suicide, but only in the presence of a police officer. For example, the death recorded by Unique ID 31495 in the Fatal Encounters dataset resulted from a car accident that was witnessed by a police officer. This observation was filtered out from the cleaned dataset because the deceased was neither killed by a police officer nor a victim of suicide.\nThis dataset has 12 variables, including a unique ID for each observation (uid). The variables are listed in the table below.\n\n\n\n\n\n\n\n\n\nCategory\nType\nName\nDescription\n\n\n\n\nVictim\nCharacter\nname\n-\n\n\nVictim\nInteger\nage\n-\n\n\nVictim\nCharacter\ngender\n-\n\n\nVictim\nCharacter\nrace\n-\n\n\nIncident\nCharacter\ndate\n-\n\n\nLocation\nCharacter\ncity\n-\n\n\nLocation\nCharacter\nstate\n-\n\n\nIncident\nCharacter\nmanner_of_death\n\n\n\nIncident\nCharacter\narmed\nWhat weapon, if any, the individual was carrying (e.g. gun)\n\n\nIncident\nLogical\nmental_illness\nWhether the individual was displaying symptoms of mental illness\n\n\nIncident\nLogical\nflee\nWhether the individual attempted to flee the altercaation\n\n\n\nThis cleaned dataset removed some supplemental details from the Fatal Encounter dataset, such as the address of incident, the corresponding latitude and longitude, the agencies involved, and a description of the incident.\n\n\nU.S. Cities Demographic Dataset\nWe supplemented our police fatality dataset by joining it with a U.S. cities demographic dataset. This demographic dataset was sourced from the U.S. Census Bureau’s 2015 American Community Survey and posted to the opendatasoft site by the U.S. Census Bureau. U.S. cities with populations greater than or equal to 65,000 are included in this dataset. This dataset has 12 variables, all listed in the table below. The variables Race and Count are connected, with Count describing the number of individuals of a certain race. There are multiple observations per City, each with a different Race and Count.\n\n\n\n\n\n\n\n\n\nCategory\nType\nName\nDescription\n\n\n\n\nLocation\nCharacter\nCity\n-\n\n\nLocation\nCharacter\nState\n-\n\n\nDemographic\nCharacter\nRace\n-\n\n\nDemographic\nInteger\nCount\nNumber of individuals of a certain race\n\n\nDemographic\nInteger\nMedian Age\n-\n\n\nDemographic\nInteger\nMale Population\n-\n\n\nDemographic\nInteger\nFemale Population\n-\n\n\nDemographic\nInteger\nTotal Population\n-\n\n\nDemographic\nInteger\nNumber of Veterans\n-\n\n\nDemographic\nInteger\nForeign-born\nNumber of foreign-born individuals\n\n\nDemographic\nNumeric\nAverage Household Size\n-\n\n\nLocation\nCharacter\nState Code\nAbbreviation of state name"
  },
  {
    "objectID": "data.html#cleaning",
    "href": "data.html#cleaning",
    "title": "Data",
    "section": "Cleaning",
    "text": "Cleaning\nThe police fatality dataset from data.world required no additional cleaning when first downloaded. Our process of loading the police fatality data can be found here. Before we could combine datasets, we first needed to pivot the U.S. demographic dataset so that there was only one observation for each city. To do so, we used pivot_longer to change the Race variable into multiple Race variables (e.g. Asian, White), each with different values from Count. We originally joined the datasets using City as the primary key, but found issues as there are some cities with the same name but in different states. We then combined the police and demographic datasets by using inner_join with City as well as State Code. We set the relationship to “many-to-many” so that fatalities from the same city all had the demographic data added. As we were looking over the data, we noticed that some observations were of the same individual, but with slightly different information concerning the fatal incident. For example, two observations with different unique IDs but with the same victim information (name, race, etc) and location might have conflicting information over whether the individual fled the altercation. We decided the remove all duplicated observations as we had no way of verifying which observation was accurate (e.g. whether the individual fled). We did this by first using group_by(Name, City, State) to group the same altercations and then using filter to remove groups with more than one observation. We also noticed that some of the observations were missing Race values for the deceased individuals. As race is our main subject of interest, we removed these observations with filter. Our process of combining the datasets can be found here."
  },
  {
    "objectID": "big_picture.html#introduction",
    "href": "big_picture.html#introduction",
    "title": "Big Picture",
    "section": "Introduction:",
    "text": "Introduction:\nOn a windy fall evening in Minneapolis, Minnesota in 2021, a community gathered, candles in hand, and their faces revealed their pain. They were not just mourning for a loss, but also protesting for a pattern. Among them was one old lady, who held a placard in hand that read, ‘Justice for Daunte.’ Daunte Wright, a 20-year-old young Black man, had been stopped and shot by the police during a traffic violation earlier this month. The officer involved resigned and was charged with second-degree manslaughter pending investigation, but to those gathered, this was a familiar end to a grievously familiar story.\nDaunte’s story is not isolated. Across the United States, the tapestry of names, faces, and faded candles tells a similar tale. Data collected nationally reveals a stark reality: racial disparities pervade police-related fatalities, with Black individuals disproportionately represented. These are not just statistics; they are sons and daughters, friends and neighbors, whose stories demand a closer look.\nThis analysis aims to peel back the layers of data surrounding police-related fatalities to uncover how deeply race is intertwined with these fatal encounters. By examining the numbers, we seek to understand not only the scope of the disparities but also the human stories behind the statistics."
  },
  {
    "objectID": "big_picture.html#racial-breakdown-in-police-fatalities",
    "href": "big_picture.html#racial-breakdown-in-police-fatalities",
    "title": "Big Picture",
    "section": "Racial Breakdown in Police Fatalities:",
    "text": "Racial Breakdown in Police Fatalities:\n\n\n\n\n\nThis bar chart provides a stark visual comparison between the racial composition of police-related fatalities and the general population. The proportional representation of different racial groups in fatal encounters with law enforcement is measured against their proportion in the general U.S. population. The bars in red represent the general population percentages, while the blue bars represent those of police fatalities.\nFrom a glance, it is evident that certain groups appear more frequently in police fatality statistics than would be expected from their numbers in the general population. The discrepancy is particularly notable among Black or African-American individuals, whose representation in fatalities is substantially higher than in the population at large. This contrast suggests a troubling disparity that warrants a deeper investigation into the causes and conditions that lead to such a disproportionate impact."
  },
  {
    "objectID": "big_picture.html#dissecting-disparities",
    "href": "big_picture.html#dissecting-disparities",
    "title": "Big Picture",
    "section": "Dissecting Disparities",
    "text": "Dissecting Disparities\nThe implications of this visualization are profound. They highlight a need to examine not only the interactions leading to these fatal outcomes but also the broader systemic issues at play. Such disparities may stem from a complex interplay of socioeconomic factors, biases within law enforcement, and the lived realities of racialized communities.\nUnderstanding the ‘why’ behind these numbers is crucial. It beckons policymakers, social scientists, and community leaders to grapple with uncomfortable questions about equity, justice, and the role of policing in society. As we embark on this analytical journey, the data becomes more than just numbers; it becomes a narrative of lives affected and a roadmap for potential reform.\nThis plot serves as the opening chapter in our broader analysis, a quantifiable backdrop against which we’ll explore individual stories, regional specifics, and temporal shifts. It’s a visualization that doesn’t just inform but also challenges us to seek explanations and solutions for a more equitable future."
  },
  {
    "objectID": "big_picture.html#mapping-the-racial-divide-across-the-united-states",
    "href": "big_picture.html#mapping-the-racial-divide-across-the-united-states",
    "title": "Big Picture",
    "section": "Mapping the Racial Divide Across the United States",
    "text": "Mapping the Racial Divide Across the United States\n\n\n\n\n\nIn a revealing portrait of racial disparities, a new heat map of the United States uncovers the disproportionate representation of Black or African-American individuals in police fatalities across states. The visualization, borne out of recent comprehensive data analysis, brings to light the percentage of Black fatalities in relation to total police-involved deaths state by state, painting a picture of contrast that traverses the nation’s expanse.\nThe darkest shades on the map highlight regions with the highest percentages, pointing to a grim reality where the likelihood of police fatalities involving Black individuals far exceeds their population proportion. States like Illinois and Maryland emerge with a notably higher incidence, a reflection that compels introspection into the social and systemic underpinnings that perpetuate this national crisis.\nThe data-driven approach offers a clear indication of the underlying patterns of racial bias in law enforcement, raising questions about the factors driving such disparities. From the legislation corridors to community forums, the map fuels an ongoing debate on the urgent need for reform and accountability in policing methods. It not only serves as a tool for activists and policymakers to pinpoint areas for change but also acts as a stark reminder of the ongoing struggles against racial injustice that continue to divide communities across the country.\nThis heat map stands as a critical piece in the larger mosaic of racial dynamics within the United States, providing visual evidence that underscores the stark reality faced by the Black community. It beckons a call to action for a concerted effort to address and dismantle the systemic inequities that cast a long shadow over the promise of justice and equality for all."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a website for the final project for MA[46]15 Data Science with R by Team TEAM 2. The members of this team are below.\n\nChongjun Shan\nChongjun is an undergraduate majoring in Computer Science.\n\n\nCarolyn Beigh\nCarolyn is an undergraduate majoring in Marine Science with Biology and Statistics minors.\n\n\nHaotian Wu\nHaotian is an undergraduate double majoring in Pure & Applied Mathematics and Computer Science.\n\n\nYangming Hu\nYangming is a undergraduate majoring in Economics and Mathematics.\n\n\nChen Yang\nChen is a graduate MA from economic department."
  },
  {
    "objectID": "posts/2024-03-04-post-1/post-1.html",
    "href": "posts/2024-03-04-post-1/post-1.html",
    "title": "Post 1",
    "section": "",
    "text": "US Police Fatalities\nOriginal source link, Cleaned data link\nThe original data has 36 columns and 31,498 rows, including a header. The cleaned data has 12 columns and 12,491 rows, not including a header. The original data was compiled by Fatal Encounters which used media reports and police records to create the database The data includes deaths that happened when police were present or were caused by police. The goal of Fatal Encounters is to create a national database of people killed during interactions that anyone can use for whatever purpose. The cleaned data also lists the Gun Violence Archive and Data Society as sources. Both the Fatal Encounters and the cleaned data are able to be loaded into R without issue. Some obvious questions center around race and whether certain races experience more police fatalities than others. Similar questions can be asked about gender. We might also be interested in how police encounters change overtime, and that might be indicative of changes in police policy or training. Mental health might be another variable to study. There’s been some debate on whether police should be called in response to mental health crises and a closer look at the data might shed some light on that issue. The main challenge would be cleaning the data if we chose to use the original data set. Another challenge would be combining that data with other datasets that might add new perspectives. For example, adding demographic data for cities in the dataset or additional information about police training.\nCOVID-19 National Case Surveillance\nSource: https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data/vbim-akqf/about_data\nThis data set has 104,544,006 rows, 12 columns. This dataset is sourced from database of CDCP, Centers for Disease Control and Prevention, provided by CDC Data, Analytics and Visualization Task Force. The dataset is collected mainly for the tracking purpose of COVID-19 cases, and it includes detailed information of each infected patient, like demographics, any exposure history, disease severity indicators and outcomes, presence of any underlying medical conditions and risk behaviors. The only possible challenge might be the huge sample size.\nMain questions to address:\nAre there any correlation between COVID-19 cases and geographical differences? Which race and aged group are most likely to be infected by COVID-19?\nUSA Unemployment Rates by Demographics & Race Origin: https://fred.stlouisfed.org/series/CNP16OV, https://www.epi.org/data/ cleaned:https://www.kaggle.com/datasets/asaniczka/unemployment-rates-by-demographics-1978-2023\nRows:537, columns: 122 The data is sourced from the Economic Policy Institute’s State of Working America Data Library and economic research conducted by the Federal Reserve Bank of St. Louis. Questions: how unemployment rates have changed for different groups of people over time. Look into how education levels can affect unemployment rates. Use the data to create visuals that show how unemployment rates differ across all sorts of factors.\nHomicides data over the past decade in 50 US cities Original source: https://www.kaggle.com/datasets/joebeachcapital/homicides This dataset has 52179 rows and 12 columns. The Washington Post collected data on more than 52,000 criminal homicides over the past decade in 50 of the largest American cities. The data included the location of the killing, whether an arrest was made and, in most cases, basic demographic information about each victim. Reporters received data in many formats, including paper, and worked for months to clean and standardize it, comparing homicide counts and aggregate closure rates with FBI data to ensure the records were as accurate as possible.\nOn the geographical side, the main question I hope to address would probably be finding the cities that have the highest and lowest homicide rates over the past decade. Also I might want to address how homicide rates correlate with other city-specific factors, such as population density, economic indicators, or police force size.\nFor the data quality and completeness of this dataset, I think some of the challenges would be to verify the accuracy of demographic information and the potential underreporting of certain categories of homicides."
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2024-04-07-blog-post-4/blog-post-4.html",
    "href": "posts/2024-04-07-blog-post-4/blog-post-4.html",
    "title": "blog post 4",
    "section": "",
    "text": "Impact of Gender on Fatalities in Different States This plot focuses to explore the relationship between gender and fatalities across different states. This could help identify if certain states have more pronounced gender disparities in police-related fatalities.\n\nsuppressPackageStartupMessages(library(tidyverse))\n\nprc &lt;- read_csv(\"dataset/police_fatalities.csv\", show_col_types = FALSE)\n\nprc %&gt;%\n  na.omit() %&gt;%\n  count(State, Gender) %&gt;%\n  ggplot(aes(x = reorder(State, n), y = n, fill = Gender)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Police Fatalities by State and Gender\",\n       x = \"State\",\n       y = \"Number of Fatalities\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nYearly Trend of Police Fatalities by Race This plot the trend of police fatalities over the years, broken down by race. This can show if certain races have been more affected over time and how the trends have changed.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(readr)\n\npolice_data_clean &lt;- read_csv(\"dataset/police_fatalities.csv\", show_col_types = FALSE)\n\nannual_trends_by_race &lt;- police_data_clean %&gt;%\n  mutate(Date = mdy(Date), Year = year(Date)) %&gt;%\n  drop_na(Year, Race) %&gt;%\n  count(Year, Race) %&gt;%\n  spread(key = Race, value = n, fill = 0) %&gt;%\n  gather(key = 'Race', value = 'Fatalities', -Year)\n\nannual_trends_by_race %&gt;%\n  ggplot(aes(x = Year, y = Fatalities, color = Race)) +\n  geom_line() + \n  geom_point() + \n  labs(title = \"Yearly Trend of Police Fatalities by Race\",\n       x = \"Year\",\n       y = \"Number of Fatalities\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.title = element_blank()) \n\n\n\n\nAge Histogram: Displays the frequency distribution of individuals’ ages involved in the incidents.\nBar Plot of Manner of Death by Gender and Race: Compares the number of incidents by manner of death across different races and genders.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(broom)\n\nlibrary(nnet)\n\ndata &lt;- read.csv(\"dataset/police_fatalities.csv\") %&gt;%\n  mutate(\n    Date = mdy(Date), \n    Gender = as.factor(Gender),\n    Race = as.factor(Race),\n    Manner_of_death = as.factor(Manner_of_death),\n    Armed = as.factor(Armed),\n    Mental_illness = as.factor(Mental_illness),\n    Flee = as.factor(Flee)\n  )\n\nsummary_statistics &lt;- data %&gt;%\n  summarise(\n    Average_Age = mean(Age, na.rm = TRUE),\n    SD_Age = sd(Age, na.rm = TRUE),\n    Count = n()\n  )\n\nage_histogram &lt;- ggplot(data, aes(x = Age)) +\n  geom_histogram(binwidth = 5, fill = \"blue\", color = \"black\") +\n  theme_minimal() +\n  labs(title = \"Distribution of Age\", x = \"Age\", y = \"Count\")\n\ndeath_by_demographics &lt;- ggplot(data, aes(x = Race, fill = Gender)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal() +\n  labs(title = \"Manner of Death by Gender and Race\", x = \"Race\", y = \"Count\")\n\nlogistic_model &lt;- glm(Armed ~ Age + Gender + Race + Mental_illness, \n                      data = data, family = \"binomial\")\nlogistic_model_summary &lt;- summary(logistic_model)\n\nmultinom_model &lt;- multinom(Manner_of_death ~ Age + Gender + Race + Mental_illness, data = data)\n\n# weights:  48 (33 variable)\ninitial  value 16993.196279 \niter  10 value 3526.384408\niter  20 value 3018.452673\niter  30 value 2924.878850\niter  40 value 2899.497411\niter  50 value 2899.477616\niter  60 value 2899.473481\niter  60 value 2899.473459\niter  60 value 2899.473459\nfinal  value 2899.473459 \nconverged\n\nmultinom_model_summary &lt;- summary(multinom_model)\n\nlogistic_model_table &lt;- tidy(logistic_model)\n\nprint(age_histogram)\n\nWarning: Removed 233 rows containing non-finite values (`stat_bin()`).\n\n\n\n\nprint(death_by_demographics)\n\n\n\nprint(logistic_model_table)\n\n# A tibble: 11 × 5\n   term               estimate std.error statistic  p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)        -0.693     0.919      -0.754 4.51e- 1\n 2 Age                 0.00396   0.00146     2.71  6.70e- 3\n 3 GenderFemale        0.374     0.922       0.406 6.85e- 1\n 4 GenderMale          0.518     0.918       0.564 5.73e- 1\n 5 RaceAsian           0.0773    0.157       0.492 6.23e- 1\n 6 RaceBlack           0.248     0.0523      4.73  2.24e- 6\n 7 RaceHispanic        0.124     0.0583      2.12  3.38e- 2\n 8 RaceNative          0.238     0.180       1.32  1.87e- 1\n 9 RaceOther          -0.262     0.293      -0.894 3.72e- 1\n10 RaceWhite           0.350     0.0465      7.53  5.10e-14\n11 Mental_illnessTRUE  0.216     0.0456      4.73  2.23e- 6"
  },
  {
    "objectID": "posts/2024-03-29-blog-post-3/blog-post-3.html",
    "href": "posts/2024-03-29-blog-post-3/blog-post-3.html",
    "title": "Blog Post 3",
    "section": "",
    "text": "Data for Equity\nPrinciple 1: One of the important principles in the process of data processing analysis is to be transparent about the limits of the data. While we are examining and cleaning the data, we noticed that there are many limitations with our data set in the research of US Police Involved Fatalities. Specifically, our data is effective while we are trying to examine the relationship between the police involvement and the fatalities of citizens. However, the data is definitely time-sensitive and our finding through this data set may not imply the Police involved fatalities in the past years. Besides, there is also missing data like whether the suspect is armed. If ARMED shows as blank then there is no record of whether the suspect is armed or not, which may play an important role in our data analysis and our result.\nPrinciple 2: For the beneficence principle regarding our dataset, it contains only the name of each police fatalities with no other sensitive information displayed. This could ensure the privacy of the person as well as their families. All other columns of the dataset are only served for the purpose of analyzing without disclosing much information about a single person. The dataset also contains a column called “uid” that can be uniquely identified across the dataset and can be used to substitute the function of names in certain occasions.\nPrinciple 3: another important principle is the Inclusivity of the data in representation. We use this principle to emphasize the importance of representing all affected communities fairly and accurately in the dataset. In the US Police Fatalities dataset, it literally means all the data reflected the demographic characteristics of those killed by police deaths, including race, age, gender, and mental health status. It can be essential in that it is an effective way to gain an insight into potential biases or differences in the ways diverse social groups may be affected by police action.\nPrinciple 4: We also need to mention accountability when we use data in real world practice. In the practice of data, we need to address any harm that the dataset and its analysis may cause, particularly to the minority communities. In the dataset US Police Fatalities, there’s a significant risk that improper interpretation of the data could underscore the stereotypes or contribute to unjust narratives about certain groups.\nWeekly Summary:\nWe finished the Data Equity part.\nWe thought about 3-4 principles and how they could be relevant to our US Police Fatalities dataset. These principles help our data set to be used ethically and constructively.\nThe main aspect of our data principles is the potential for abuse or misuse based on the potential biased narratives in our dataset.\nWe started to do some data exploration with plots and tables. It might be interesting if we combined our dataset with another containing more population information for the states. We have a homicide dataset that we might want to add to our police fatality dataset. We also thought about how to construct our data plan and what questions we want to ask of our dataset.\nExploration\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nprc &lt;- read_csv(\"dataset/police_fatalities.csv\")\n\nRows: 12491 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Name, Gender, Race, Date, City, State, Manner_of_death, Armed\ndbl (2): UID, Age\nlgl (2): Mental_illness, Flee\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nprc |&gt; \n  na.omit() |&gt;  \n  ggplot(aes(x = Race, fill = Gender)) + \n  geom_bar()\n\n\n\n\n\nprc |&gt; na.omit() |&gt; group_by(State) |&gt; summarize(n = n()) |&gt; arrange(-n)\n\n# A tibble: 51 × 2\n   State     n\n   &lt;chr&gt; &lt;int&gt;\n 1 CA      966\n 2 TX      494\n 3 FL      310\n 4 AZ      194\n 5 NV      153\n 6 NY      137\n 7 WA      137\n 8 CO      125\n 9 OH      123\n10 IL      121\n# ℹ 41 more rows\n\n\nBelow is a plot displaying the average age of police fatalities each year, separated by race.\n\nsuppressWarnings({\nlibrary(tidyverse)\nlibrary(lubridate)\n\npolice_data_clean &lt;- read_csv(here::here(\"dataset\", \"police_fatalities.csv\"))\n\npolice_fatalities_summary &lt;- police_data_clean %&gt;%\n  mutate(Date = dmy(Date),  \n         Year = year(Date)) %&gt;%  \n  group_by(Year, Race) %&gt;% \n  summarise(average_age = mean(Age, na.rm = TRUE)) \n\npolice_fatalities_summary %&gt;%\n  ggplot(aes(x = Year, y = average_age, color = Race)) +\n  geom_line() +  \n  geom_point() + \n  labs(\n    title = \"Average Age of Police Fatalities by Year and Race\",\n    x = \"Year\",\n    y = \"Average Age\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) \n})\n\nRows: 12491 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Name, Gender, Race, Date, City, State, Manner_of_death, Armed\ndbl (2): UID, Age\nlgl (2): Mental_illness, Flee\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'Year'. You can override using the `.groups` argument.\n\n\nWarning: Removed 7 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 7 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2024-04-24-blog-post-7/blog-post-7.html",
    "href": "posts/2024-04-24-blog-post-7/blog-post-7.html",
    "title": "Blog Post 7",
    "section": "",
    "text": "source(\"combining-datasets.R\")\nlibrary(dplyr)\nlibrary(tibble)  # For more robust data frame operations\n\n# Assuming police_dem is already loaded in your R environment\n\n# Calculating total deaths and racial composition percentages\ndeath_counts &lt;- police_dem %&gt;%\n  group_by(State, City) %&gt;%\n  summarize(\n    TotalDeaths = n(),  # Count of deaths\n    .groups = 'drop'  # Drop grouping structure after summarizing\n  )\n\nracial_composition &lt;- police_dem %&gt;%\n  group_by(State, City, Race) %&gt;%\n  summarize(\n    Count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  group_by(State, City) %&gt;%\n  mutate(\n    Percentage = Count / sum(Count) * 100\n  )\n\n# Calculate discrepancies (Example: comparing racial percentages with total deaths)\ndiscrepancies &lt;- racial_composition %&gt;%\n  left_join(death_counts, by = c(\"State\", \"City\")) %&gt;%\n  mutate(\n    Discrepancy = abs(Percentage - (TotalDeaths / sum(TotalDeaths) * 100))\n  ) %&gt;%\n  arrange(desc(Discrepancy))\n\n# Identify and display locations with the highest discrepancies\ntop_discrepancies &lt;- discrepancies %&gt;%\n  top_n(1, Discrepancy)\n\nprint(top_discrepancies)\n\n# A tibble: 558 × 7\n# Groups:   State, City [456]\n   State                City      Race  Count Percentage TotalDeaths Discrepancy\n   &lt;chr&gt;                &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;      &lt;dbl&gt;       &lt;int&gt;       &lt;dbl&gt;\n 1 District of Columbia Washingt… Blac…    24       92.3          26        59.0\n 2 Pennsylvania         Philadel… Blac…    15       83.3          18        58.3\n 3 Louisiana            New Orle… Blac…    25       86.2          29        52.9\n 4 California           Oakland   Blac…    30       75            40        50  \n 5 California           Santa Ana Hisp…    18       75            24        50  \n 6 Nevada               Henderson White    14       73.7          19        48.7\n 7 Oregon               Portland  White    28       73.7          38        48.7\n 8 Illinois             Chicago   Blac…    67       73.6          91        48.6\n 9 Nevada               Reno      White    19       73.1          26        48.1\n10 Washington           Spokane   White    14       77.8          18        44.4\n# ℹ 548 more rows\n\n\n\nlibrary(dplyr)\n\n# Assuming police_dem is already loaded in your R environment\n\n# Calculating total deaths for each race\nracial_composition &lt;- police_dem %&gt;%\n  group_by(Race) %&gt;%\n  summarize(\n    TotalDeaths = n(),  # Count of deaths per race\n    .groups = 'drop'  # Drop grouping structure after summarizing\n  )\n\n# Calculate the death rate for each race\nracial_composition &lt;- racial_composition %&gt;%\n  mutate(\n    DeathRate = TotalDeaths / sum(TotalDeaths) * 100  # Convert to percentage\n  ) %&gt;%\n  arrange(desc(DeathRate))\n\n# Display the death rates for all races\nprint(racial_composition)\n\n# A tibble: 5 × 3\n  Race                              TotalDeaths DeathRate\n  &lt;chr&gt;                                   &lt;int&gt;     &lt;dbl&gt;\n1 Black or African-American                1441     35.8 \n2 White                                    1369     34.0 \n3 Hispanic or Latino                       1070     26.6 \n4 Asian                                     101      2.51\n5 American Indian and Alaska Native          41      1.02\n\n# Identify and display the race with the highest death rate\nhighest_death_rate_race &lt;- racial_composition %&gt;%\n  slice_max(DeathRate, n = 1)  # Get the race with the highest death rate\n\nprint(highest_death_rate_race)\n\n# A tibble: 1 × 3\n  Race                      TotalDeaths DeathRate\n  &lt;chr&gt;                           &lt;int&gt;     &lt;dbl&gt;\n1 Black or African-American        1441      35.8"
  },
  {
    "objectID": "posts/2024-04-24-blog-6/blog-6.html",
    "href": "posts/2024-04-24-blog-6/blog-6.html",
    "title": "Post 6",
    "section": "",
    "text": "We further cleaned our data since we found that there are repeated police filing for the same event, which caused our dataset to have overlapped individual records but different details. Since we cannot confirm which filing is more accurate, we decided to remove all the data that has repeated filing. Looking at our combined dataset with the demographic information of each city, we thought about calculating the mortality rate of each race in each city, and even the cities that have abnormal rates compared to the “general pattern”.\nOur thesis statement is:\nRace disparity contributes to a great factor of the demographics of police-related fatalities.\nRace Black has the highest mortality rate among all the cities on average."
  },
  {
    "objectID": "posts/2024-04-15-post-5-team-2/post-5-team-2.html",
    "href": "posts/2024-04-15-post-5-team-2/post-5-team-2.html",
    "title": "Post 5",
    "section": "",
    "text": "We combined police fatalities data set with us cities demographics data set with an R script called combining-data sets. The us cities demographics data set is originally one of the data sets comes from our brain storming in the Post 1 and we consider it is interested to see some interaction between our current data set. In order to make these two data set fit, we pivot the cities demographics data set so that there is only one observation for each city and count for the each races for their own column. Then, we inner join to keep only the data that has a match from each data set with city variable, and rename unnecessary name. Finally, we filter out all the observations with missing Race values as race is the main variable of interest."
  },
  {
    "objectID": "posts/2024-03-22-post-2-/post-2-.html",
    "href": "posts/2024-03-22-post-2-/post-2-.html",
    "title": "Post 2",
    "section": "",
    "text": "We are working with the US Police Fatalities data set. For this post, we Set up data loading and cleaning in the load_and_clean_data.R file. The required rds file was created. Since the data set we found was already cleaned, we did not do anything specific to clean the data further.\nAnswer all the questions from the data background part and update it to the data page in the website. \n\nWrite some paragraphs about data equity.(still working on it, so this is not updated)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nPost 7\n\n\n\n\n\nPost 7 from team 2\n\n\n\n\n\n\nMay 1, 2024\n\n\n\n\n\n\n  \n\n\n\n\nPost 6\n\n\n\n\n\nPost 6 from team 2\n\n\n\n\n\n\nApr 24, 2024\n\n\n\n\n\n\n  \n\n\n\n\nPost 5\n\n\n\n\n\nPost 5 from team 2\n\n\n\n\n\n\nApr 15, 2024\n\n\n\n\n\n\n  \n\n\n\n\nPost 4\n\n\n\n\n\nPost 4 from team 2\n\n\n\n\n\n\nApr 7, 2024\n\n\n\n\n\n\n  \n\n\n\n\nPost 3\n\n\n\n\n\nPost 3 from team 2\n\n\n\n\n\n\nMar 29, 2024\n\n\n\n\n\n\n  \n\n\n\n\nPost 2\n\n\n\n\n\nPost 2 from team 2\n\n\n\n\n\n\nMar 22, 2024\n\n\n\n\n\n\n  \n\n\n\n\nPost 1\n\n\n\n\n\nPost 1 from team 2\n\n\n\n\n\n\nMar 4, 2024\n\n\nTeam 2\n\n\n\n\n\n\n  \n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post.\n\n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nTest Post\n\n\n\n\n\nTesting\n\n\n\n\n\n\nFeb 23, 2024\n\n\nCarolyn Beigh\n\n\n\n\n\n\n  \n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting.\n\n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "We describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#introduction",
    "href": "analysis.html#introduction",
    "title": "Analysis",
    "section": "Introduction",
    "text": "Introduction\nIn recent years, police-related fatalities have become a focal point of public and academic debate, underscoring critical concerns about racial disparities within law enforcement across the United States. This analysis seeks to explore the extent to which racial disparity, especially against Black individuals, contributes to the demographics of these fatalities. The urgency of this issue is amplified by ongoing societal calls for justice and reform, making it essential to understand the patterns and factors that drive these disparities.\n#Thesis Statement This research posits that racial disparity is a significant factor influencing the demographics of police-related fatalities, with Black individuals experiencing disproportionately high mortality rates in comparison to other races across various cities.\n\nResearch Questions\nTo provide a structured exploration of this issue, this analysis will address the following key questions:\n\nHow do mortality rates from police-related fatalities among Black individuals compare to those of other races across different cities?\nWhat factors might contribute to any observed disparities in these mortality rates?\nAre there specific geographic or demographic contexts in which these disparities are more pronounced?\n\n\n\nSignificance of the Analysis\nThe findings of this analysis are intended to contribute to the broader dialogue on racial equity and law enforcement practices. By examining the intersection of race and police-related fatalities, this study aims to provide data-driven insights that could inform policy decisions and advocacy efforts aimed at reducing these disparities. Additionally, this analysis will serve as an educational resource for scholars, policymakers, and the public, fostering a deeper understanding of how racial dynamics shape outcomes in law enforcement encounters.\nThrough a combination of quantitative methods and statistical modeling, this analysis page will offer a detailed examination of the available data, aiming to present a clear and comprehensive picture of how race influences the risk of fatality in police interactions. The next sections will describe the data used for this analysis, the methodology employed, and the results obtained,thereby framing the context for a rigorous discussion on this critical social issue."
  },
  {
    "objectID": "analysis.html#data-description",
    "href": "analysis.html#data-description",
    "title": "Analysis",
    "section": "Data Description",
    "text": "Data Description\nThis analysis leverages two key datasets:\nPolice Fatality Dataset & U.S. Cities Demographic Dataset\nTo load the dataset, run the attached R script “combining-datasets.R”.\n\nsource(\"combining-datasets.R\")\nrm(police_f,us_dem,us_dem_wid)\n\nAfter running this R script, the two datasets we used will be merged, cleared and loaded into your environment called police_dem. For more information on these two datasets and merging process, see the data page. data page."
  },
  {
    "objectID": "analysis.html#exploratory-data-analysis-eda",
    "href": "analysis.html#exploratory-data-analysis-eda",
    "title": "Analysis",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nStarted by exploring the distribution of police-related fatalities by race.\n\nfatalities_by_race &lt;- police_dem |&gt;\n  group_by(Race) |&gt;\n  summarize(Fatalities = n()) |&gt;\n  ungroup()\n\nggplot(fatalities_by_race, aes(x = reorder(Race, -Fatalities), y = Fatalities, fill = Race)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Distribution of Police-Related Fatalities by Race\",\n    x = \"Race\",\n    y = \"Number of Fatalities\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nAs expected, the highest number of deaths occurred among blacks or African Americans.\nThe next step is to compare this with demographic data, and it would be interesting to examine the relationship between each racial group’s percentage of total deaths and their percentage of the total population of the sampled city. This would help to determine how specific racial groups are disproportionately affected.\n\nfatalities_by_race_city_state &lt;- police_dem |&gt;\n  group_by(State, City, Race) |&gt;\n  summarize(Fatalities = n(), .groups = 'drop')\n\npopulation_by_race_city_state &lt;- police_dem |&gt;\n  group_by(State, City) |&gt;\n  summarise(\n    White = first(White),\n    `Black or African-American` = first(`Black or African-American`),\n    `Hispanic or Latino` = first(`Hispanic or Latino`),\n    Asian = first(Asian),\n    `American Indian and Alaska Native` = first(`American Indian and Alaska Native`),\n    .groups = 'drop'\n  )|&gt;\n  pivot_longer(cols = -c(City, State), names_to = \"Race\", values_to = \"Population\")\n\ncomparison_df &lt;- \n  left_join(fatalities_by_race_city_state, population_by_race_city_state, by = c(\"State\", \"City\", \"Race\"))|&gt;\n  mutate(\n    FatalityPercentage = (Fatalities / sum(Fatalities)) * 100,\n    PopulationPercentage = (Population / sum(Population)) * 100,\n    diffinPercentage = FatalityPercentage - PopulationPercentage\n  )|&gt;\n  arrange(desc(diffinPercentage))\n\n\nsum_df&lt;- comparison_df|&gt;\n  group_by(Race)|&gt;\n  summarise(sumfatalityp = sum(FatalityPercentage),\n          sumpopulationp = sum(PopulationPercentage))|&gt;\n  pivot_longer(cols = c(sumfatalityp, sumpopulationp), names_to = \"Type\", values_to = \"Percentage\") %&gt;%\n  mutate(Type = recode(Type, 'sumfatalityp' = 'Fatality Percentage', 'sumpopulationp' = 'Population Percentage'))\n\nggplot(sum_df, aes(x = Type, y = Percentage, fill = Type)) +\n  geom_col(position = position_dodge(width = 0.8)) +\n  facet_wrap(~ Race) +\n  labs(\n    title = \"Comparison of Fatality and Population Percentages by Race\",\n    x = NULL,\n    y = \"Percentage\"\n  ) +\n  scale_fill_manual(values = c(\"Fatality Percentage\" = \"red\", \"Population Percentage\" = \"blue\")) +\n  theme(\n    axis.text.x = element_blank(),  \n    axis.ticks.x = element_blank(), \n    strip.background = element_blank(),\n    strip.text.x = element_text(size = 10)\n  ) \n\n\n\n\n\nlibrary(tidycensus)\nlibrary(sf)\n\n\nstates_sf &lt;- get_decennial(geography = \"state\", \n                           year = 2020, \n                           variables = \"H1_001N\", \n                           geometry = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\nstate_level_comparison_df &lt;- comparison_df %&gt;%\n  group_by(State, Race) %&gt;%\n  summarize(Diff_Percentage = mean(diffinPercentage, na.rm = TRUE), .groups = 'drop')\n\nstate_data_sf &lt;- inner_join(states_sf, state_level_comparison_df, by = c(\"NAME\" = \"State\"))\n\n\nstate_data_sf %&gt;%\n  ggplot() +\n  geom_sf(aes(fill = Diff_Percentage), color = NA) +\n  scale_fill_gradient2(\n    low = \"blue\", mid = \"white\", high = \"red\",  # Adjust colors if needed\n    midpoint = 0, \n    limit = c(min(state_data_sf$Diff_Percentage, na.rm = TRUE), max(state_data_sf$Diff_Percentage, na.rm = TRUE)), \n    name = \"Diff in Percentage\",\n    na.value = \"white\"\n  ) +\n  facet_wrap(~ Race) +\n  labs(\n    title = \"Difference in Police Fatality and Population Percentages by Race and State\",\n    subtitle = \"Faceted by Race\",\n    fill = \"Diff in Percentage\"\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(size = 14, hjust = 0.5), \n    plot.subtitle = element_text(size = 12),\n    strip.text = element_text(size = 10)  # Adjust facet label text size\n  ) +\n  coord_sf(xlim = c(-125, -67), ylim = c(24, 50), expand = FALSE)\n\n\n\n\nAs shown in the graph, both White and Hispanic has a smaller police fatality rate comparing to the population percentage. However, for Blan or African-American, there is a higher police fatality rate than the population percentage comparing to other races."
  },
  {
    "objectID": "analysis.html#modelling",
    "href": "analysis.html#modelling",
    "title": "Analysis",
    "section": "Modelling",
    "text": "Modelling\n\nmodel_data&lt;- police_dem|&gt;\n  group_by(City)|&gt;\n  summarise(Fatalities = n(),\n            Median.Age = first(Median.Age),\n            Male.Population = first(Male.Population),\n            Female.Population = first(Female.Population),\n            Foreign.born = first(Foreign.born),\n            Average.Household.Size = first(Average.Household.Size),\n            White = first(White),\n            `Black or African-American` = first(`Black or African-American`),\n            `Hispanic or Latino` = first(`Hispanic or Latino`),\n            Asian = first(Asian),\n            `American Indian and Alaska Native` = first(`American Indian and Alaska Native`)\n            )|&gt;\n  arrange(desc(Fatalities))\nmodel_data &lt;- na.omit(model_data)\nprint(model_data)\n\n# A tibble: 401 × 12\n   City     Fatalities Median.Age Male.Population Female.Population Foreign.born\n   &lt;chr&gt;         &lt;int&gt;      &lt;dbl&gt;           &lt;int&gt;             &lt;int&gt;        &lt;int&gt;\n 1 Los Ang…        252       35           1958998           2012898      1485425\n 2 Houston         179       32.6         1149686           1148942       696210\n 3 Las Veg…        133       37.5          310568            313201       127609\n 4 Chicago          91       34.2         1320015           1400541       573463\n 5 Dallas           78       32.6          639019            661063       326825\n 6 Phoenix          75       33.8          786833            776168       300702\n 7 Fresno           67       30            256130            263942       103453\n 8 Kansas …         61       35.9          228430            246931        37787\n 9 San Die…         59       34.5          693826            701081       373842\n10 Long Be…         52       34.6          238159            236013       127764\n# ℹ 391 more rows\n# ℹ 6 more variables: Average.Household.Size &lt;dbl&gt;, White &lt;int&gt;,\n#   `Black or African-American` &lt;int&gt;, `Hispanic or Latino` &lt;int&gt;, Asian &lt;int&gt;,\n#   `American Indian and Alaska Native` &lt;int&gt;\n\nwrite.csv(model_data, \"model_data.csv\", row.names = FALSE)\n\n\n# Loading necessary libraries\nlibrary(dplyr)\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(caret)\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n# Reading CSV\nmodel_data &lt;- read.csv(\"model_data.csv\")\n\n# Preprocessing\nfeatures &lt;- c(\"Median.Age\", \"Average.Household.Size\", \"White\", \"Black.or.African.American\",\n              \"Hispanic.or.Latino\", \"Asian\", \"Foreign.born\")\nX &lt;- model_data[features]\ny &lt;- model_data$Fatalities\n\nset.seed(42)\ntrain_indices &lt;- createDataPartition(y, p=0.8, list=FALSE)\nX_train &lt;- X[train_indices, ]\ny_train &lt;- y[train_indices]\nX_test &lt;- X[-train_indices, ]\ny_test &lt;- y[-train_indices]\n\n# Models to compare\nmodels &lt;- c(\"lm\", \"rf\", \"gbm\") # Linear regression, random forest, and gradient boosted trees\n\n# Training models and collecting results\nresults &lt;- lapply(models, function(model_type) {\n    model &lt;- train(X_train, y_train, method=model_type, trControl=trainControl(method=\"cv\", number=5))\n    preds &lt;- predict(model, X_test)\n    mse &lt;- mean((y_test - preds)^2)\n    r2 &lt;- cor(y_test, preds)^2\n    list(model=model, mse=mse, r2=r2, preds=preds)\n})\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      437.4290             nan     0.1000   27.4280\n     2      408.7813             nan     0.1000   15.7501\n     3      395.4170             nan     0.1000   15.2359\n     4      376.3634             nan     0.1000   16.9875\n     5      351.7196             nan     0.1000    7.4109\n     6      345.5470             nan     0.1000    8.3016\n     7      336.4917             nan     0.1000    9.9871\n     8      332.8744             nan     0.1000    4.8174\n     9      320.9553             nan     0.1000   12.4436\n    10      313.6190             nan     0.1000    8.6959\n    20      262.8795             nan     0.1000    3.5238\n    40      234.0204             nan     0.1000    0.2298\n    60      222.6959             nan     0.1000   -0.2838\n    80      218.2399             nan     0.1000   -5.5454\n   100      216.1732             nan     0.1000   -0.0145\n   120      207.2354             nan     0.1000   -0.7678\n   140      205.6510             nan     0.1000   -1.6705\n   150      201.0133             nan     0.1000   -6.3028\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      437.0136             nan     0.1000   28.6714\n     2      412.9916             nan     0.1000   25.1130\n     3      397.0818             nan     0.1000   18.7484\n     4      374.2418             nan     0.1000   19.8731\n     5      351.0026             nan     0.1000    5.6884\n     6      344.8605             nan     0.1000    8.4042\n     7      331.5819             nan     0.1000   11.3070\n     8      328.3335             nan     0.1000    4.7279\n     9      326.7874             nan     0.1000    1.4308\n    10      315.7730             nan     0.1000    8.9811\n    20      258.3562             nan     0.1000    0.7373\n    40      227.9485             nan     0.1000   -0.8243\n    60      215.1950             nan     0.1000   -1.6061\n    80      201.0966             nan     0.1000   -6.4751\n   100      190.8814             nan     0.1000   -3.0564\n   120      175.8154             nan     0.1000   -1.4036\n   140      169.4513             nan     0.1000    0.4749\n   150      165.3861             nan     0.1000   -5.3267\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      432.6233             nan     0.1000   32.1600\n     2      393.2092             nan     0.1000   22.0244\n     3      363.3840             nan     0.1000    9.8009\n     4      356.0875             nan     0.1000    9.3279\n     5      339.6079             nan     0.1000   13.3477\n     6      321.0776             nan     0.1000    6.8127\n     7      306.1044             nan     0.1000    1.8733\n     8      292.1206             nan     0.1000   -1.7355\n     9      281.7557             nan     0.1000   -0.0036\n    10      276.1867             nan     0.1000    5.8189\n    20      254.1182             nan     0.1000    1.8211\n    40      216.8760             nan     0.1000   -3.7291\n    60      191.4941             nan     0.1000   -1.5159\n    80      184.8427             nan     0.1000   -6.1842\n   100      174.1077             nan     0.1000   -2.4867\n   120      167.7345             nan     0.1000   -7.8049\n   140      165.9268             nan     0.1000   -3.6783\n   150      158.2061             nan     0.1000   -1.6879\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      479.7651             nan     0.1000   33.2850\n     2      454.4711             nan     0.1000   28.4296\n     3      441.6205             nan     0.1000   17.7437\n     4      431.6195             nan     0.1000   14.3309\n     5      411.9916             nan     0.1000   22.9197\n     6      390.6043             nan     0.1000   17.6815\n     7      378.5919             nan     0.1000   13.8003\n     8      367.8421             nan     0.1000   13.0112\n     9      347.2614             nan     0.1000   -2.5637\n    10      333.1552             nan     0.1000   11.3268\n    20      271.0655             nan     0.1000    3.3427\n    40      234.5828             nan     0.1000   -1.1240\n    60      228.5059             nan     0.1000   -0.6185\n    80      220.5513             nan     0.1000   -0.3232\n   100      214.1065             nan     0.1000   -1.1121\n   120      210.5568             nan     0.1000   -0.3866\n   140      205.3693             nan     0.1000   -6.1464\n   150      203.4729             nan     0.1000   -2.3736\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      455.1920             nan     0.1000   13.9721\n     2      430.3676             nan     0.1000   26.1969\n     3      406.5633             nan     0.1000   27.5667\n     4      388.5935             nan     0.1000   19.8741\n     5      368.2101             nan     0.1000   18.1366\n     6      353.0516             nan     0.1000   12.1328\n     7      336.0848             nan     0.1000    9.0474\n     8      323.7842             nan     0.1000    5.9607\n     9      317.4202             nan     0.1000    6.4874\n    10      306.9155             nan     0.1000   10.1333\n    20      260.4342             nan     0.1000    2.2835\n    40      218.6353             nan     0.1000    1.1010\n    60      196.9307             nan     0.1000   -6.6508\n    80      173.3088             nan     0.1000   -5.6548\n   100      159.8150             nan     0.1000   -3.2670\n   120      149.7924             nan     0.1000   -2.8885\n   140      143.7695             nan     0.1000   -0.7367\n   150      140.0717             nan     0.1000   -1.4103\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      475.9325             nan     0.1000   36.9461\n     2      448.0271             nan     0.1000   28.6347\n     3      406.2833             nan     0.1000   10.9980\n     4      386.7761             nan     0.1000   21.2529\n     5      363.5487             nan     0.1000    8.6477\n     6      338.4246             nan     0.1000    4.9303\n     7      334.2562             nan     0.1000    5.2902\n     8      313.4140             nan     0.1000   -3.8601\n     9      307.6212             nan     0.1000    6.5958\n    10      295.5589             nan     0.1000    4.7556\n    20      251.8456             nan     0.1000    4.8217\n    40      211.9095             nan     0.1000    0.0298\n    60      180.0987             nan     0.1000   -5.7060\n    80      162.8219             nan     0.1000   -3.2345\n   100      145.1374             nan     0.1000   -3.6024\n   120      131.4593             nan     0.1000   -3.2620\n   140      117.6167             nan     0.1000   -2.5541\n   150      111.7955             nan     0.1000   -3.6033\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      473.3174             nan     0.1000   28.3679\n     2      430.4075             nan     0.1000    9.8324\n     3      409.6574             nan     0.1000   21.5491\n     4      392.6435             nan     0.1000   17.9432\n     5      371.7472             nan     0.1000   10.9912\n     6      359.4338             nan     0.1000   14.8202\n     7      347.0458             nan     0.1000   11.2455\n     8      333.5526             nan     0.1000    7.8037\n     9      325.0470             nan     0.1000    8.4675\n    10      317.9314             nan     0.1000    7.3885\n    20      274.4292             nan     0.1000    0.6431\n    40      261.6525             nan     0.1000    0.3428\n    60      249.7966             nan     0.1000    0.4149\n    80      242.9038             nan     0.1000   -1.1810\n   100      237.8929             nan     0.1000   -4.9398\n   120      231.8720             nan     0.1000   -1.2908\n   140      230.1742             nan     0.1000    0.2519\n   150      230.2674             nan     0.1000   -0.9674\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      456.7388             nan     0.1000    6.6303\n     2      425.2170             nan     0.1000   15.0421\n     3      408.6766             nan     0.1000   22.4094\n     4      388.0890             nan     0.1000   21.6560\n     5      368.7248             nan     0.1000   16.5434\n     6      356.2955             nan     0.1000    1.1544\n     7      348.8066             nan     0.1000    8.2248\n     8      340.4878             nan     0.1000    6.8140\n     9      330.7258             nan     0.1000   10.1918\n    10      322.5890             nan     0.1000    9.0106\n    20      278.2150             nan     0.1000    0.1338\n    40      222.7836             nan     0.1000   -3.5305\n    60      208.7545             nan     0.1000   -3.5348\n    80      197.9743             nan     0.1000   -2.1794\n   100      183.8939             nan     0.1000   -2.0073\n   120      174.8256             nan     0.1000   -4.1496\n   140      168.0438             nan     0.1000   -1.7305\n   150      164.9880             nan     0.1000   -2.9299\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      482.6183             nan     0.1000   23.3380\n     2      452.2202             nan     0.1000   30.4648\n     3      422.2330             nan     0.1000   23.6377\n     4      390.7878             nan     0.1000   10.7873\n     5      363.5460             nan     0.1000    9.4894\n     6      356.3730             nan     0.1000    9.0802\n     7      349.2493             nan     0.1000    9.4089\n     8      345.8400             nan     0.1000    4.4633\n     9      333.9615             nan     0.1000    8.1558\n    10      329.8779             nan     0.1000    5.3513\n    20      265.0612             nan     0.1000   -4.8884\n    40      230.0637             nan     0.1000   -6.8938\n    60      202.4519             nan     0.1000   -3.2311\n    80      190.3599             nan     0.1000   -0.3157\n   100      178.5394             nan     0.1000   -1.7702\n   120      173.1583             nan     0.1000   -2.0465\n   140      167.6598             nan     0.1000   -1.7806\n   150      155.5361             nan     0.1000   -0.7206\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      490.1932             nan     0.1000   32.5468\n     2      466.7754             nan     0.1000   30.6002\n     3      435.4390             nan     0.1000    6.5418\n     4      420.8427             nan     0.1000   20.3231\n     5      405.2690             nan     0.1000   18.4092\n     6      393.4622             nan     0.1000   16.6931\n     7      377.9819             nan     0.1000   19.8806\n     8      371.7374             nan     0.1000    9.4264\n     9      354.6290             nan     0.1000   17.2655\n    10      341.8406             nan     0.1000   15.8776\n    20      269.7835             nan     0.1000    0.9709\n    40      234.6212             nan     0.1000   -1.1129\n    60      226.5797             nan     0.1000   -5.5468\n    80      218.9569             nan     0.1000   -3.0751\n   100      212.4731             nan     0.1000   -3.0780\n   120      211.8142             nan     0.1000   -5.9745\n   140      208.4312             nan     0.1000   -1.8982\n   150      206.9127             nan     0.1000   -0.5206\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      469.8570             nan     0.1000   28.9234\n     2      436.5708             nan     0.1000   36.6176\n     3      418.8945             nan     0.1000   20.9368\n     4      381.2299             nan     0.1000   10.6138\n     5      361.6566             nan     0.1000   16.5929\n     6      338.4303             nan     0.1000    9.7756\n     7      323.1864             nan     0.1000   -0.4738\n     8      302.9532             nan     0.1000   -5.9844\n     9      291.4090             nan     0.1000   -1.5574\n    10      282.1210             nan     0.1000    0.4122\n    20      247.9071             nan     0.1000   -3.3651\n    40      212.7690             nan     0.1000   -0.1537\n    60      198.2900             nan     0.1000   -9.7710\n    80      180.6218             nan     0.1000   -2.9950\n   100      170.2059             nan     0.1000   -3.5813\n   120      157.3032             nan     0.1000   -2.3602\n   140      148.7620             nan     0.1000   -3.9696\n   150      145.5535             nan     0.1000   -0.3416\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      469.5617             nan     0.1000   21.8836\n     2      431.3257             nan     0.1000   10.1846\n     3      411.3176             nan     0.1000   21.5701\n     4      376.2284             nan     0.1000   13.0015\n     5      359.6712             nan     0.1000   15.2004\n     6      332.8845             nan     0.1000    7.2782\n     7      310.9632             nan     0.1000    5.4005\n     8      301.3768             nan     0.1000    7.8271\n     9      291.7699             nan     0.1000    5.6566\n    10      288.2420             nan     0.1000    4.5995\n    20      251.6026             nan     0.1000   -3.3573\n    40      219.8411             nan     0.1000   -2.6542\n    60      190.6454             nan     0.1000   -5.0386\n    80      173.9793             nan     0.1000   -3.0266\n   100      160.3436             nan     0.1000   -3.9920\n   120      139.5837             nan     0.1000   -1.4452\n   140      130.2070             nan     0.1000   -1.6593\n   150      128.4903             nan     0.1000   -1.5485\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      164.8054             nan     0.1000   12.2705\n     2      152.5332             nan     0.1000    6.6037\n     3      144.1239             nan     0.1000   10.5197\n     4      134.1379             nan     0.1000   12.4987\n     5      127.9793             nan     0.1000    6.5038\n     6      118.9079             nan     0.1000    9.0926\n     7      109.3585             nan     0.1000    5.3310\n     8      102.6333             nan     0.1000    5.1361\n     9       98.0454             nan     0.1000    4.4538\n    10       92.6427             nan     0.1000    0.9292\n    20       73.5581             nan     0.1000    0.3700\n    40       65.0015             nan     0.1000   -0.0070\n    60       63.0920             nan     0.1000   -1.1024\n    80       60.3337             nan     0.1000   -0.1576\n   100       58.1094             nan     0.1000   -0.6358\n   120       56.2075             nan     0.1000   -0.2490\n   140       54.5152             nan     0.1000    0.0033\n   150       53.4454             nan     0.1000   -0.6865\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      161.1730             nan     0.1000   16.0460\n     2      150.6929             nan     0.1000   12.5683\n     3      137.9740             nan     0.1000   10.9538\n     4      126.8499             nan     0.1000    5.9260\n     5      117.9568             nan     0.1000    7.7742\n     6      111.7302             nan     0.1000    6.6145\n     7      105.6733             nan     0.1000    6.8921\n     8      100.9365             nan     0.1000    5.1279\n     9       98.2876             nan     0.1000    3.4876\n    10       92.3500             nan     0.1000    3.6665\n    20       71.0343             nan     0.1000   -0.0759\n    40       60.2453             nan     0.1000   -0.0316\n    60       55.0295             nan     0.1000   -0.3002\n    80       50.7657             nan     0.1000   -0.5118\n   100       45.9635             nan     0.1000   -0.8059\n   120       43.1070             nan     0.1000   -0.7733\n   140       41.9800             nan     0.1000   -0.2094\n   150       41.0957             nan     0.1000   -0.9779\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      164.3948             nan     0.1000   16.1591\n     2      150.9784             nan     0.1000   14.6262\n     3      139.6622             nan     0.1000   12.3297\n     4      129.8071             nan     0.1000   10.0627\n     5      123.4221             nan     0.1000    7.3753\n     6      111.8749             nan     0.1000    9.3193\n     7      104.3024             nan     0.1000    1.5306\n     8      100.1580             nan     0.1000    4.6165\n     9       96.3054             nan     0.1000    2.3956\n    10       92.3789             nan     0.1000    3.8957\n    20       69.2260             nan     0.1000    0.6737\n    40       58.4196             nan     0.1000   -0.9028\n    60       53.7310             nan     0.1000   -0.7241\n    80       47.6826             nan     0.1000   -0.3408\n   100       44.9627             nan     0.1000   -0.0520\n   120       41.2660             nan     0.1000   -0.5121\n   140       38.9858             nan     0.1000   -0.4687\n   150       37.7115             nan     0.1000   -0.6076\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      403.7030             nan     0.1000   26.8494\n     2      363.3934             nan     0.1000   13.9571\n     3      342.8664             nan     0.1000   22.6869\n     4      327.8147             nan     0.1000   18.2234\n     5      318.9084             nan     0.1000   11.6547\n     6      297.8754             nan     0.1000    4.2287\n     7      277.3595             nan     0.1000    4.6935\n     8      264.0592             nan     0.1000   -0.4886\n     9      259.3368             nan     0.1000    6.6711\n    10      250.4391             nan     0.1000   -0.2043\n    20      215.2187             nan     0.1000   -4.7196\n    40      193.6296             nan     0.1000   -0.5517\n    50      180.6413             nan     0.1000   -5.3035\n\n# Storing results in a list for later access\nresults_named &lt;- setNames(results, models)\n\n# Comparing model results as a summary\ncomparison_table &lt;- data.frame(\n    Model=names(results_named),\n    MSE=sapply(results_named, function(res) res$mse),\n    R2=sapply(results_named, function(res) res$r2)\n)\n\nprint(comparison_table)\n\n    Model       MSE        R2\nlm     lm 2432.1813 0.1315024\nrf     rf  522.8015 0.3484045\ngbm   gbm  131.9947 0.5636952\n\n# Tuning the random forest model\ntuneGrid &lt;- expand.grid(mtry = c(3, 4, 5))\nrf_tuned &lt;- train(X_train, y_train, method=\"rf\", trControl=trainControl(method=\"cv\", number=5), tuneGrid=tuneGrid)\n\nprint(rf_tuned)\n\nRandom Forest \n\n322 samples\n  7 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 258, 258, 257, 258, 257 \nResampling results across tuning parameters:\n\n  mtry  RMSE      Rsquared   MAE     \n  3     14.18944  0.5708586  5.917032\n  4     14.26144  0.5751936  5.951982\n  5     14.34681  0.5682486  6.013841\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 3.\n\n\nAccording to this result, it is shown that the third model, GBM (Gradient Boosting Model) has the highest R2 value and lowest MSE, which make this model the optimal choice for our research, now we will refine and improve this model.\n\n# Loading necessary libraries\nlibrary(dplyr)\nlibrary(randomForest)\nlibrary(caret)\nlibrary(gbm)\n\nLoaded gbm 2.1.9\n\n\nThis version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\n# Reading the CSV file\nmodel_data &lt;- read.csv(\"model_data.csv\")\n\n# Preprocessing\nfeatures &lt;- c(\"White\", \"Black.or.African.American\", \"Hispanic.or.Latino\", \"Asian\", \"American.Indian.and.Alaska.Native\")\nX &lt;- model_data[features]\ny &lt;- model_data$Fatalities\n\n# Splitting data\nset.seed(42)\ntrain_indices = createDataPartition(y, p=0.8, list=FALSE)\nX_train = X[train_indices, ]\ny_train = y[train_indices]\nX_test = X[-train_indices, ]\ny_test = y[-train_indices]\n\n# Define the tuning grid\ntuneGrid_gbm = expand.grid(\n    interaction.depth = c(1, 2, 3),\n    n.trees = c(50, 100, 150),\n    shrinkage = c(0.1, 0.05),\n    n.minobsinnode = c(10, 20)\n)\n\ntrain_control = trainControl(method=\"cv\", number=5, verboseIter=FALSE)\n\n# Training the model\ngbm_tuned = train(X_train, y_train, method=\"gbm\", trControl=train_control, tuneGrid=tuneGrid_gbm)\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      384.3660             nan     0.0500    9.1819\n     2      377.5942             nan     0.0500    7.7717\n     3      370.0759             nan     0.0500    8.5188\n     4      360.1835             nan     0.0500    9.6751\n     5      348.5169             nan     0.0500    8.5045\n     6      338.7526             nan     0.0500    8.3904\n     7      334.4754             nan     0.0500    5.5454\n     8      330.5636             nan     0.0500    4.8957\n     9      318.7963             nan     0.0500    0.8059\n    10      309.4562             nan     0.0500    3.8067\n    20      256.0243             nan     0.0500    2.3047\n    40      225.0129             nan     0.0500    1.2107\n    60      209.2471             nan     0.0500   -1.2327\n    80      202.8875             nan     0.0500   -1.4084\n   100      199.9014             nan     0.0500   -0.6131\n   120      197.4637             nan     0.0500   -1.8714\n   140      195.8902             nan     0.0500   -0.0335\n   150      194.6219             nan     0.0500   -0.4865\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      394.7953             nan     0.0500    8.1705\n     2      385.1497             nan     0.0500    9.5268\n     3      375.4007             nan     0.0500    6.3284\n     4      368.7444             nan     0.0500    7.2775\n     5      358.6050             nan     0.0500    4.5845\n     6      353.4248             nan     0.0500    6.0992\n     7      347.4107             nan     0.0500    5.6310\n     8      342.7586             nan     0.0500    5.1441\n     9      334.9104             nan     0.0500    2.8231\n    10      332.4136             nan     0.0500    3.3253\n    20      296.0470             nan     0.0500    2.0301\n    40      262.7947             nan     0.0500    0.4026\n    60      250.3771             nan     0.0500   -0.6265\n    80      247.3011             nan     0.0500   -0.3660\n   100      245.2036             nan     0.0500   -0.0370\n   120      244.7284             nan     0.0500   -1.7799\n   140      244.4014             nan     0.0500   -0.1338\n   150      244.1294             nan     0.0500   -1.1304\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      391.9459             nan     0.0500   12.6743\n     2      385.4888             nan     0.0500    8.8233\n     3      369.5138             nan     0.0500   11.6105\n     4      357.8129             nan     0.0500   11.2831\n     5      348.8754             nan     0.0500    8.4591\n     6      337.6559             nan     0.0500    8.9262\n     7      334.8160             nan     0.0500    3.9193\n     8      327.0446             nan     0.0500    6.1999\n     9      320.5144             nan     0.0500    7.4888\n    10      309.7957             nan     0.0500    3.8777\n    20      262.2985             nan     0.0500    1.2896\n    40      229.3831             nan     0.0500    0.8868\n    60      217.3649             nan     0.0500   -0.2927\n    80      204.3314             nan     0.0500   -0.2874\n   100      196.8021             nan     0.0500    0.5240\n   120      189.6031             nan     0.0500   -0.7104\n   140      187.3598             nan     0.0500   -1.8800\n   150      185.5092             nan     0.0500   -1.8588\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      389.2555             nan     0.0500    6.0719\n     2      379.2185             nan     0.0500    8.4930\n     3      370.6843             nan     0.0500    8.2049\n     4      360.4149             nan     0.0500    5.5172\n     5      353.1682             nan     0.0500    6.8862\n     6      347.4529             nan     0.0500    5.2011\n     7      344.2755             nan     0.0500    4.2698\n     8      337.7871             nan     0.0500    4.7347\n     9      334.8754             nan     0.0500    3.8810\n    10      331.8498             nan     0.0500    3.9163\n    20      302.0451             nan     0.0500    2.8549\n    40      265.3307             nan     0.0500    0.1807\n    60      252.3164             nan     0.0500   -2.4520\n    80      248.1160             nan     0.0500    0.1541\n   100      244.5305             nan     0.0500    0.1575\n   120      240.7568             nan     0.0500   -0.1082\n   140      238.1924             nan     0.0500   -0.3030\n   150      236.3421             nan     0.0500   -0.5191\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      382.2297             nan     0.0500   10.2819\n     2      368.1505             nan     0.0500    7.0828\n     3      353.5660             nan     0.0500    7.3918\n     4      343.9339             nan     0.0500    8.3609\n     5      338.6864             nan     0.0500    7.1939\n     6      324.5360             nan     0.0500    6.6082\n     7      316.3233             nan     0.0500    6.0417\n     8      307.2244             nan     0.0500    2.9209\n     9      301.1458             nan     0.0500    6.7510\n    10      298.0155             nan     0.0500    3.9315\n    20      256.2441             nan     0.0500    3.0781\n    40      220.7821             nan     0.0500   -0.6428\n    60      206.9822             nan     0.0500   -1.8067\n    80      199.0784             nan     0.0500   -1.5371\n   100      190.3637             nan     0.0500   -1.2862\n   120      183.1592             nan     0.0500   -2.1733\n   140      176.9374             nan     0.0500   -0.9907\n   150      175.3253             nan     0.0500   -1.0634\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      394.7759             nan     0.0500    8.3542\n     2      383.0879             nan     0.0500    9.8656\n     3      371.7051             nan     0.0500    6.6404\n     4      363.3170             nan     0.0500    7.1260\n     5      357.9939             nan     0.0500    5.8664\n     6      349.8447             nan     0.0500    6.6341\n     7      346.7714             nan     0.0500    4.0702\n     8      343.7941             nan     0.0500    3.9610\n     9      336.3915             nan     0.0500    3.6395\n    10      333.1342             nan     0.0500    3.9313\n    20      291.0478             nan     0.0500    2.0913\n    40      260.2270             nan     0.0500    0.8989\n    60      249.0420             nan     0.0500    0.2633\n    80      245.2774             nan     0.0500   -1.6123\n   100      242.6839             nan     0.0500    0.0260\n   120      237.4290             nan     0.0500   -0.5013\n   140      236.5265             nan     0.0500   -0.7443\n   150      234.9565             nan     0.0500   -0.3719\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      371.0343             nan     0.1000   28.9823\n     2      343.4752             nan     0.1000    7.3665\n     3      332.5956             nan     0.1000   12.8919\n     4      315.1558             nan     0.1000   17.1587\n     5      302.0449             nan     0.1000    8.2880\n     6      297.0026             nan     0.1000    6.6713\n     7      282.4094             nan     0.1000    6.0862\n     8      273.8759             nan     0.1000    1.8523\n     9      267.0609             nan     0.1000   -0.1857\n    10      263.3425             nan     0.1000    4.0797\n    20      225.9231             nan     0.1000    0.5319\n    40      203.8101             nan     0.1000   -0.8405\n    60      198.5662             nan     0.1000   -0.9128\n    80      194.0868             nan     0.1000   -0.5798\n   100      192.6957             nan     0.1000   -2.8436\n   120      189.1239             nan     0.1000   -2.9569\n   140      189.1198             nan     0.1000   -2.9948\n   150      186.6801             nan     0.1000   -6.0703\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      385.8465             nan     0.1000   18.5742\n     2      373.6384             nan     0.1000   15.0990\n     3      361.7033             nan     0.1000   12.7740\n     4      342.2885             nan     0.1000    6.1495\n     5      331.5527             nan     0.1000   10.2775\n     6      320.9951             nan     0.1000    8.3407\n     7      311.2746             nan     0.1000    3.6054\n     8      307.4725             nan     0.1000    5.3178\n     9      303.9086             nan     0.1000    4.3364\n    10      300.6663             nan     0.1000    4.2444\n    20      263.1784             nan     0.1000    1.2888\n    40      248.0341             nan     0.1000   -0.2316\n    60      245.9235             nan     0.1000   -0.0502\n    80      243.8022             nan     0.1000   -1.0476\n   100      244.2229             nan     0.1000   -2.4257\n   120      242.6995             nan     0.1000   -1.8228\n   140      242.7362             nan     0.1000   -1.3590\n   150      242.5425             nan     0.1000   -0.0999\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      382.4836             nan     0.1000   25.4923\n     2      358.0556             nan     0.1000   23.5264\n     3      348.0878             nan     0.1000   15.1697\n     4      322.7265             nan     0.1000   12.6913\n     5      307.0103             nan     0.1000   15.0523\n     6      292.1787             nan     0.1000    9.7631\n     7      283.3849             nan     0.1000    9.2691\n     8      278.7907             nan     0.1000    6.3207\n     9      276.4989             nan     0.1000    2.8842\n    10      263.0128             nan     0.1000    2.8440\n    20      220.3369             nan     0.1000   -8.1058\n    40      196.5205             nan     0.1000   -1.8191\n    60      184.0044             nan     0.1000   -1.6585\n    80      174.4242             nan     0.1000   -1.6459\n   100      171.6023             nan     0.1000   -3.1968\n   120      168.5963             nan     0.1000   -0.8415\n   140      159.8754             nan     0.1000   -3.3628\n   150      156.2671             nan     0.1000   -1.1934\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      377.1707             nan     0.1000   17.1999\n     2      356.2300             nan     0.1000    9.3934\n     3      346.9382             nan     0.1000   10.8995\n     4      332.0293             nan     0.1000    7.7814\n     5      323.1762             nan     0.1000    8.9550\n     6      315.0546             nan     0.1000    8.2076\n     7      306.2747             nan     0.1000    6.1549\n     8      295.6308             nan     0.1000    3.3007\n     9      288.2176             nan     0.1000    1.5938\n    10      281.5824             nan     0.1000    2.8366\n    20      258.0027             nan     0.1000    0.8658\n    40      247.1055             nan     0.1000   -0.2210\n    60      241.8720             nan     0.1000   -1.0607\n    80      238.1402             nan     0.1000   -1.1021\n   100      234.5470             nan     0.1000   -0.7005\n   120      232.1151             nan     0.1000   -1.0574\n   140      229.5525             nan     0.1000   -1.6605\n   150      228.6550             nan     0.1000   -0.9489\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      383.0229             nan     0.1000   22.7091\n     2      350.9114             nan     0.1000   13.7635\n     3      326.3238             nan     0.1000   13.2527\n     4      306.3799             nan     0.1000   12.0131\n     5      295.8990             nan     0.1000   11.2054\n     6      288.1286             nan     0.1000   10.5115\n     7      277.8128             nan     0.1000    8.8811\n     8      264.7252             nan     0.1000    4.8671\n     9      261.2134             nan     0.1000    5.0820\n    10      256.6130             nan     0.1000    5.2292\n    20      219.0708             nan     0.1000   -2.7651\n    40      192.0957             nan     0.1000   -2.6506\n    60      181.0561             nan     0.1000   -0.6188\n    80      173.6981             nan     0.1000   -2.6709\n   100      167.0465             nan     0.1000   -3.5480\n   120      160.6826             nan     0.1000   -5.1971\n   140      156.2484             nan     0.1000   -2.9647\n   150      153.9852             nan     0.1000   -0.0642\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      386.8550             nan     0.1000   17.2253\n     2      373.3493             nan     0.1000   15.9449\n     3      362.5252             nan     0.1000   12.4595\n     4      350.5227             nan     0.1000   13.7029\n     5      337.2905             nan     0.1000    9.8911\n     6      325.5130             nan     0.1000    4.4669\n     7      313.3761             nan     0.1000    1.7932\n     8      303.4958             nan     0.1000    1.9861\n     9      297.9236             nan     0.1000    4.5952\n    10      290.9275             nan     0.1000   -0.3584\n    20      263.6515             nan     0.1000    1.4153\n    40      242.7312             nan     0.1000   -2.5101\n    60      240.5060             nan     0.1000   -4.0722\n    80      235.2103             nan     0.1000   -0.2045\n   100      231.1129             nan     0.1000   -0.9603\n   120      228.5885             nan     0.1000   -0.1308\n   140      226.7049             nan     0.1000   -2.6646\n   150      224.8894             nan     0.1000   -0.6853\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      447.9631             nan     0.0500   12.8924\n     2      428.7196             nan     0.0500    6.3003\n     3      421.0701             nan     0.0500   10.0189\n     4      402.8818             nan     0.0500    4.4938\n     5      391.3419             nan     0.0500   11.7581\n     6      380.9863             nan     0.0500   10.1602\n     7      369.4521             nan     0.0500    7.8717\n     8      364.1000             nan     0.0500    6.6220\n     9      352.0091             nan     0.0500    0.8433\n    10      346.1088             nan     0.0500    6.1505\n    20      289.2402             nan     0.0500    3.6702\n    40      240.0784             nan     0.0500   -2.7336\n    60      229.4447             nan     0.0500    0.5638\n    80      220.5938             nan     0.0500    0.1982\n   100      215.2734             nan     0.0500   -0.2341\n   120      213.7333             nan     0.0500   -1.8729\n   140      211.2321             nan     0.0500   -2.3827\n   150      211.1281             nan     0.0500   -3.1622\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      449.4141             nan     0.0500   11.1807\n     2      438.0600             nan     0.0500   10.9715\n     3      424.4395             nan     0.0500    7.4070\n     4      412.5433             nan     0.0500    7.3418\n     5      401.2607             nan     0.0500    5.4973\n     6      390.0457             nan     0.0500    4.2153\n     7      383.7462             nan     0.0500    6.2583\n     8      376.4638             nan     0.0500    6.8053\n     9      367.0300             nan     0.0500    0.7573\n    10      361.8239             nan     0.0500    4.9969\n    20      325.3038             nan     0.0500    2.6154\n    40      293.0755             nan     0.0500    0.5418\n    60      283.5344             nan     0.0500   -0.8627\n    80      278.8277             nan     0.0500   -0.8094\n   100      276.8678             nan     0.0500   -1.0502\n   120      275.9956             nan     0.0500   -0.8564\n   140      274.6089             nan     0.0500   -0.1375\n   150      274.5465             nan     0.0500   -0.0204\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      453.3111             nan     0.0500   11.4769\n     2      438.2070             nan     0.0500   16.4703\n     3      421.7315             nan     0.0500   14.0431\n     4      405.8043             nan     0.0500   12.2516\n     5      397.8328             nan     0.0500   10.3294\n     6      390.0222             nan     0.0500    9.0196\n     7      383.9828             nan     0.0500    7.1938\n     8      373.9306             nan     0.0500   10.5069\n     9      359.5134             nan     0.0500    5.8028\n    10      351.3179             nan     0.0500    7.5814\n    20      291.6982             nan     0.0500    2.8632\n    40      253.3468             nan     0.0500   -1.3466\n    60      226.6296             nan     0.0500   -3.3724\n    80      214.1011             nan     0.0500   -1.8797\n   100      203.6627             nan     0.0500   -2.6617\n   120      199.3682             nan     0.0500   -0.5294\n   140      192.8688             nan     0.0500   -0.8939\n   150      192.8593             nan     0.0500    0.2625\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      446.4458             nan     0.0500   10.5406\n     2      435.1021             nan     0.0500   10.7425\n     3      428.7181             nan     0.0500    8.4902\n     4      420.7466             nan     0.0500    9.2901\n     5      410.8795             nan     0.0500    7.7744\n     6      401.3660             nan     0.0500    6.4071\n     7      393.0825             nan     0.0500    5.8523\n     8      389.5761             nan     0.0500    4.6865\n     9      384.8567             nan     0.0500    5.7758\n    10      375.2382             nan     0.0500    3.6282\n    20      336.1785             nan     0.0500    3.2291\n    40      302.7461             nan     0.0500    0.6279\n    60      288.4021             nan     0.0500   -0.9244\n    80      278.5964             nan     0.0500    0.0319\n   100      275.4749             nan     0.0500   -0.0592\n   120      271.6024             nan     0.0500   -0.3847\n   140      268.3159             nan     0.0500   -1.9894\n   150      267.5155             nan     0.0500   -1.5837\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      443.8878             nan     0.0500   14.5177\n     2      435.6499             nan     0.0500   10.3403\n     3      425.7111             nan     0.0500   12.3114\n     4      410.7714             nan     0.0500   15.0494\n     5      403.9020             nan     0.0500    9.4786\n     6      391.9411             nan     0.0500   10.5630\n     7      379.5061             nan     0.0500   11.4427\n     8      374.7767             nan     0.0500    6.1386\n     9      359.7846             nan     0.0500    3.6418\n    10      354.1958             nan     0.0500    7.5537\n    20      297.8805             nan     0.0500    3.8773\n    40      241.5079             nan     0.0500    1.1031\n    60      221.4660             nan     0.0500   -2.9868\n    80      210.4812             nan     0.0500   -2.0757\n   100      197.3729             nan     0.0500   -2.0753\n   120      192.5466             nan     0.0500   -0.2784\n   140      186.4684             nan     0.0500   -1.7298\n   150      184.3709             nan     0.0500   -1.6150\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      451.4868             nan     0.0500   11.8772\n     2      439.4256             nan     0.0500   11.5315\n     3      429.9731             nan     0.0500   10.5723\n     4      418.0381             nan     0.0500    9.0577\n     5      407.1354             nan     0.0500    7.2788\n     6      398.1682             nan     0.0500    8.3128\n     7      390.3458             nan     0.0500    7.5284\n     8      381.1442             nan     0.0500    5.0051\n     9      374.5842             nan     0.0500    5.8069\n    10      368.7965             nan     0.0500    5.2364\n    20      325.7124             nan     0.0500    0.2753\n    40      298.7653             nan     0.0500    0.6871\n    60      285.1371             nan     0.0500    0.1029\n    80      280.7377             nan     0.0500   -1.3792\n   100      273.4159             nan     0.0500   -1.9080\n   120      269.2496             nan     0.0500   -0.1087\n   140      267.2104             nan     0.0500   -1.1191\n   150      265.5254             nan     0.0500   -0.0390\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      441.6617             nan     0.1000   27.1063\n     2      427.7810             nan     0.1000   20.0481\n     3      416.8084             nan     0.1000   14.3720\n     4      401.7663             nan     0.1000   20.6349\n     5      388.2614             nan     0.1000   18.6584\n     6      379.9552             nan     0.1000   12.7112\n     7      354.6215             nan     0.1000    9.6554\n     8      332.0755             nan     0.1000    8.6930\n     9      325.9638             nan     0.1000    8.1681\n    10      320.5864             nan     0.1000    6.9563\n    20      243.7206             nan     0.1000    1.4964\n    40      221.3933             nan     0.1000   -3.0724\n    60      219.0030             nan     0.1000   -2.2184\n    80      214.3167             nan     0.1000   -8.2274\n   100      210.6157             nan     0.1000   -0.4773\n   120      208.0415             nan     0.1000   -1.5926\n   140      206.2711             nan     0.1000   -0.8637\n   150      203.9220             nan     0.1000   -1.8669\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      440.3547             nan     0.1000   23.8380\n     2      420.1974             nan     0.1000   18.3911\n     3      404.9672             nan     0.1000   17.9832\n     4      397.1787             nan     0.1000   10.3185\n     5      388.2268             nan     0.1000   11.6953\n     6      380.9778             nan     0.1000    9.9187\n     7      370.9013             nan     0.1000   11.5587\n     8      358.5469             nan     0.1000    8.7821\n     9      351.4960             nan     0.1000    8.1586\n    10      344.0001             nan     0.1000    6.9831\n    20      304.3208             nan     0.1000    1.7758\n    40      287.0096             nan     0.1000   -3.6194\n    60      284.3670             nan     0.1000    0.2876\n    80      279.2862             nan     0.1000    0.8872\n   100      279.1684             nan     0.1000   -1.5390\n   120      276.1321             nan     0.1000   -4.7253\n   140      275.3122             nan     0.1000   -0.8013\n   150      274.5112             nan     0.1000   -0.5260\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      439.0908             nan     0.1000   30.4240\n     2      406.2006             nan     0.1000   28.1427\n     3      375.3042             nan     0.1000   14.7456\n     4      364.3633             nan     0.1000   13.1425\n     5      338.0049             nan     0.1000    8.2488\n     6      333.4039             nan     0.1000    6.3937\n     7      326.5283             nan     0.1000    9.4046\n     8      320.4744             nan     0.1000    8.9911\n     9      303.9619             nan     0.1000    1.6915\n    10      300.1834             nan     0.1000    5.1193\n    20      243.6753             nan     0.1000   -2.3011\n    40      207.7881             nan     0.1000    0.0227\n    60      190.6820             nan     0.1000   -1.6849\n    80      181.8311             nan     0.1000   -1.0738\n   100      177.3448             nan     0.1000   -1.0922\n   120      168.3678             nan     0.1000   -1.4987\n   140      161.7412             nan     0.1000   -1.4110\n   150      159.8650             nan     0.1000   -0.9836\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      434.4674             nan     0.1000   24.4880\n     2      415.0723             nan     0.1000   17.5846\n     3      401.6318             nan     0.1000   16.2191\n     4      381.2218             nan     0.1000    7.1720\n     5      369.7833             nan     0.1000   12.0251\n     6      355.9763             nan     0.1000    8.9565\n     7      350.8814             nan     0.1000    6.7109\n     8      340.3422             nan     0.1000    4.3109\n     9      336.7272             nan     0.1000    4.9151\n    10      331.0612             nan     0.1000    4.2292\n    20      293.6723             nan     0.1000    0.9993\n    40      274.7952             nan     0.1000   -5.0912\n    60      268.7117             nan     0.1000   -0.9043\n    80      264.8539             nan     0.1000    0.1493\n   100      260.2633             nan     0.1000   -3.8358\n   120      257.9804             nan     0.1000   -0.4540\n   140      255.6105             nan     0.1000   -5.7408\n   150      253.8068             nan     0.1000   -0.1609\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      436.2792             nan     0.1000   29.4460\n     2      408.9771             nan     0.1000   27.8642\n     3      388.4553             nan     0.1000   21.8782\n     4      364.1148             nan     0.1000   17.6866\n     5      347.0775             nan     0.1000   17.9785\n     6      338.4199             nan     0.1000   10.5537\n     7      332.7120             nan     0.1000    8.3428\n     8      314.2268             nan     0.1000    8.2472\n     9      309.1977             nan     0.1000    5.8716\n    10      306.1077             nan     0.1000    4.2846\n    20      247.0932             nan     0.1000   -6.8802\n    40      207.6828             nan     0.1000   -3.0826\n    60      193.4126             nan     0.1000   -5.9876\n    80      178.7415             nan     0.1000   -1.3103\n   100      173.0646             nan     0.1000   -3.2601\n   120      168.1281             nan     0.1000   -4.4168\n   140      162.0908             nan     0.1000   -0.8339\n   150      159.8209             nan     0.1000    0.0172\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      439.3285             nan     0.1000   22.5820\n     2      418.7210             nan     0.1000   20.3865\n     3      402.2993             nan     0.1000   15.5639\n     4      381.5382             nan     0.1000    8.2230\n     5      374.3743             nan     0.1000    9.3159\n     6      363.2054             nan     0.1000    9.4613\n     7      347.4157             nan     0.1000    3.5685\n     8      342.2672             nan     0.1000    6.2032\n     9      334.6805             nan     0.1000    5.9262\n    10      328.9431             nan     0.1000    5.9818\n    20      302.4618             nan     0.1000   -2.2111\n    40      276.5166             nan     0.1000   -0.3316\n    60      267.1512             nan     0.1000   -2.8504\n    80      263.9872             nan     0.1000   -0.6006\n   100      260.1905             nan     0.1000   -1.1580\n   120      257.0558             nan     0.1000   -0.3586\n   140      255.0391             nan     0.1000   -3.0796\n   150      252.9640             nan     0.1000   -0.4298\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      270.6688             nan     0.0500    7.3072\n     2      261.6980             nan     0.0500    8.0600\n     3      253.4006             nan     0.0500    7.8474\n     4      246.8683             nan     0.0500    8.2228\n     5      241.1236             nan     0.0500    6.4106\n     6      232.3802             nan     0.0500    2.6844\n     7      223.4514             nan     0.0500    5.0559\n     8      217.4989             nan     0.0500    3.0943\n     9      212.1975             nan     0.0500    3.0488\n    10      207.1106             nan     0.0500    5.8577\n    20      171.2636             nan     0.0500    2.2423\n    40      138.0344             nan     0.0500    0.4617\n    60      128.9236             nan     0.0500   -0.8998\n    80      125.6729             nan     0.0500   -1.8128\n   100      123.0072             nan     0.0500   -0.1713\n   120      121.4572             nan     0.0500   -1.0899\n   140      119.9248             nan     0.0500   -0.1774\n   150      119.5612             nan     0.0500   -1.4032\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      273.6166             nan     0.0500    7.4168\n     2      264.8862             nan     0.0500    8.7273\n     3      259.6879             nan     0.0500    6.4230\n     4      252.5357             nan     0.0500    6.3438\n     5      246.6450             nan     0.0500    6.6588\n     6      243.0868             nan     0.0500    4.4493\n     7      235.8176             nan     0.0500    4.8479\n     8      228.8460             nan     0.0500    3.6348\n     9      223.6303             nan     0.0500    3.9368\n    10      218.5970             nan     0.0500    3.5910\n    20      191.1708             nan     0.0500    0.9337\n    40      163.4506             nan     0.0500    0.5213\n    60      156.1073             nan     0.0500    0.0908\n    80      152.0117             nan     0.0500    0.0599\n   100      150.4443             nan     0.0500   -1.7848\n   120      149.3927             nan     0.0500   -0.3631\n   140      148.7125             nan     0.0500   -0.1254\n   150      148.4269             nan     0.0500   -0.0594\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      276.0338             nan     0.0500    8.3634\n     2      264.4675             nan     0.0500   11.2133\n     3      253.4095             nan     0.0500   11.4602\n     4      246.2287             nan     0.0500    8.6669\n     5      239.3797             nan     0.0500    7.2219\n     6      228.0897             nan     0.0500    8.8000\n     7      224.4092             nan     0.0500    4.7802\n     8      215.3527             nan     0.0500    6.9881\n     9      210.4706             nan     0.0500    4.6117\n    10      204.9192             nan     0.0500    6.1988\n    20      162.5428             nan     0.0500    2.5169\n    40      137.5338             nan     0.0500   -0.4695\n    60      126.2605             nan     0.0500    0.4228\n    80      119.0657             nan     0.0500   -0.5708\n   100      113.6024             nan     0.0500   -0.9581\n   120      110.0560             nan     0.0500   -0.9732\n   140      106.8103             nan     0.0500   -1.5641\n   150      105.5507             nan     0.0500   -0.6585\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      275.3495             nan     0.0500    8.0612\n     2      264.0525             nan     0.0500    6.1548\n     3      254.3855             nan     0.0500    7.0120\n     4      249.8778             nan     0.0500    5.5941\n     5      243.1666             nan     0.0500    6.3490\n     6      239.4467             nan     0.0500    4.3792\n     7      236.2944             nan     0.0500    3.9230\n     8      230.5837             nan     0.0500    5.1763\n     9      226.5532             nan     0.0500    4.5278\n    10      221.6096             nan     0.0500    4.7086\n    20      187.9500             nan     0.0500    1.8183\n    40      164.3316             nan     0.0500    0.7008\n    60      155.2474             nan     0.0500    0.0996\n    80      151.7566             nan     0.0500   -0.0596\n   100      149.0973             nan     0.0500   -0.0149\n   120      147.6097             nan     0.0500   -0.0833\n   140      145.6569             nan     0.0500   -1.5158\n   150      145.1820             nan     0.0500   -0.7621\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      270.1766             nan     0.0500   14.3046\n     2      259.3161             nan     0.0500   11.2371\n     3      248.2493             nan     0.0500    9.1540\n     4      235.6996             nan     0.0500    6.0845\n     5      230.9384             nan     0.0500    5.8399\n     6      225.9361             nan     0.0500    6.4928\n     7      220.5782             nan     0.0500    5.4120\n     8      212.5308             nan     0.0500    5.9863\n     9      203.3051             nan     0.0500    4.0408\n    10      197.5005             nan     0.0500    5.0494\n    20      162.1587             nan     0.0500    1.3646\n    40      133.4842             nan     0.0500    0.6825\n    60      121.9777             nan     0.0500    0.1179\n    80      115.9032             nan     0.0500   -0.6501\n   100      111.9807             nan     0.0500   -0.0849\n   120      106.9748             nan     0.0500   -0.0653\n   140      104.6377             nan     0.0500   -0.6336\n   150      102.9460             nan     0.0500   -0.4421\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      273.0890             nan     0.0500    7.7958\n     2      262.6993             nan     0.0500    5.8207\n     3      251.9062             nan     0.0500    5.5587\n     4      243.8010             nan     0.0500    5.8188\n     5      236.3224             nan     0.0500    5.2367\n     6      232.2588             nan     0.0500    5.1361\n     7      224.5027             nan     0.0500    3.4018\n     8      218.4931             nan     0.0500    3.4859\n     9      215.0941             nan     0.0500    3.6139\n    10      212.3592             nan     0.0500    3.0375\n    20      184.9082             nan     0.0500    1.9515\n    40      161.8235             nan     0.0500   -0.8924\n    60      152.7575             nan     0.0500    0.1416\n    80      149.3321             nan     0.0500   -0.6767\n   100      147.3111             nan     0.0500   -0.8275\n   120      143.9308             nan     0.0500   -0.6145\n   140      141.2799             nan     0.0500   -0.1460\n   150      140.3404             nan     0.0500   -0.6705\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      265.2016             nan     0.1000   20.0047\n     2      249.4876             nan     0.1000   20.0444\n     3      237.4430             nan     0.1000   13.9510\n     4      222.9589             nan     0.1000   13.4351\n     5      211.8938             nan     0.1000    6.1904\n     6      201.5372             nan     0.1000   10.9050\n     7      189.2254             nan     0.1000    6.5897\n     8      182.7931             nan     0.1000    2.7358\n     9      174.5555             nan     0.1000    3.8751\n    10      172.5439             nan     0.1000    2.2395\n    20      141.1564             nan     0.1000    0.7906\n    40      127.1671             nan     0.1000   -1.8169\n    60      121.3489             nan     0.1000    0.1065\n    80      119.5214             nan     0.1000   -2.9906\n   100      117.8125             nan     0.1000   -2.8533\n   120      114.0456             nan     0.1000   -1.5474\n   140      111.6857             nan     0.1000   -0.9395\n   150      110.5517             nan     0.1000   -2.9284\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      266.0284             nan     0.1000   18.3286\n     2      250.5064             nan     0.1000   14.1740\n     3      243.7259             nan     0.1000    9.1837\n     4      234.4061             nan     0.1000   10.9991\n     5      222.8597             nan     0.1000    7.3142\n     6      215.4369             nan     0.1000    7.8502\n     7      210.1738             nan     0.1000    5.5927\n     8      204.6410             nan     0.1000    6.3654\n     9      198.6309             nan     0.1000    4.2533\n    10      190.1177             nan     0.1000    1.8078\n    20      168.5373             nan     0.1000    1.4512\n    40      154.4099             nan     0.1000   -0.4751\n    60      150.4198             nan     0.1000   -0.3474\n    80      148.9961             nan     0.1000   -0.0479\n   100      148.0487             nan     0.1000   -0.2797\n   120      146.7553             nan     0.1000   -1.5412\n   140      145.7622             nan     0.1000   -1.6317\n   150      145.5262             nan     0.1000   -0.2116\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      268.1385             nan     0.1000   19.4753\n     2      243.3248             nan     0.1000   18.7869\n     3      226.0960             nan     0.1000   14.1642\n     4      213.7763             nan     0.1000   13.1826\n     5      200.9001             nan     0.1000   12.1706\n     6      191.8387             nan     0.1000   10.1465\n     7      179.0294             nan     0.1000    9.1253\n     8      174.9278             nan     0.1000    5.7187\n     9      169.0531             nan     0.1000    6.5060\n    10      167.9047             nan     0.1000    0.5727\n    20      134.6147             nan     0.1000   -0.0974\n    40      116.9645             nan     0.1000   -3.9706\n    60      108.9838             nan     0.1000   -1.7755\n    80      101.3957             nan     0.1000   -1.2588\n   100       97.4642             nan     0.1000   -3.9997\n   120       94.0929             nan     0.1000   -0.7213\n   140       89.7122             nan     0.1000   -0.9274\n   150       88.3296             nan     0.1000   -2.4944\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      265.1891             nan     0.1000   17.5130\n     2      251.4104             nan     0.1000   15.5406\n     3      241.9360             nan     0.1000   11.8005\n     4      229.2590             nan     0.1000    8.5500\n     5      219.0436             nan     0.1000    9.3825\n     6      214.7020             nan     0.1000    5.9648\n     7      209.6820             nan     0.1000    5.9246\n     8      203.4835             nan     0.1000    5.3492\n     9      196.5259             nan     0.1000    4.3167\n    10      191.8668             nan     0.1000    4.6187\n    20      165.5096             nan     0.1000    0.6230\n    40      152.0077             nan     0.1000   -0.1171\n    60      148.8602             nan     0.1000   -0.1614\n    80      144.6198             nan     0.1000   -0.2367\n   100      142.4795             nan     0.1000   -0.5343\n   120      140.5903             nan     0.1000   -0.5990\n   140      137.9142             nan     0.1000   -0.2007\n   150      137.7881             nan     0.1000   -1.4127\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      266.7084             nan     0.1000   17.3479\n     2      248.3872             nan     0.1000   18.8963\n     3      236.1170             nan     0.1000   15.8373\n     4      220.2463             nan     0.1000    7.3461\n     5      200.8204             nan     0.1000   10.5280\n     6      190.9980             nan     0.1000    9.3988\n     7      184.1341             nan     0.1000    7.9895\n     8      178.5652             nan     0.1000    6.2769\n     9      169.4485             nan     0.1000    5.6547\n    10      159.7724             nan     0.1000    0.8979\n    20      133.8008             nan     0.1000    0.4678\n    40      119.1228             nan     0.1000   -1.5615\n    60      108.3330             nan     0.1000   -2.2059\n    80      101.9238             nan     0.1000   -1.9875\n   100       94.8383             nan     0.1000   -0.8486\n   120       90.5823             nan     0.1000   -2.3680\n   140       86.7000             nan     0.1000   -0.8366\n   150       85.1030             nan     0.1000   -0.3927\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      259.2463             nan     0.1000   16.9225\n     2      243.7781             nan     0.1000   15.0212\n     3      228.3393             nan     0.1000    6.1323\n     4      218.0402             nan     0.1000    8.8547\n     5      209.1841             nan     0.1000    5.6195\n     6      203.2116             nan     0.1000    5.9219\n     7      193.9820             nan     0.1000    4.3908\n     8      189.6893             nan     0.1000    3.7334\n     9      183.3714             nan     0.1000    2.5525\n    10      180.4510             nan     0.1000    2.9397\n    20      160.7962             nan     0.1000   -1.0359\n    40      147.9455             nan     0.1000   -0.0958\n    60      144.4508             nan     0.1000   -1.4801\n    80      142.1891             nan     0.1000   -0.4428\n   100      138.4743             nan     0.1000   -0.1112\n   120      135.4922             nan     0.1000   -0.8195\n   140      133.9605             nan     0.1000   -0.9437\n   150      133.2003             nan     0.1000   -1.8442\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      515.4948             nan     0.0500   13.5926\n     2      495.3501             nan     0.0500   20.4108\n     3      473.8711             nan     0.0500   19.5799\n     4      454.8551             nan     0.0500   14.6077\n     5      441.7600             nan     0.0500   13.0286\n     6      422.7601             nan     0.0500    7.1385\n     7      404.4165             nan     0.0500    7.1586\n     8      393.8271             nan     0.0500    8.9795\n     9      380.6509             nan     0.0500    7.5696\n    10      372.9071             nan     0.0500    8.0921\n    20      308.5188             nan     0.0500    2.3015\n    40      251.9329             nan     0.0500   -1.9007\n    60      239.2231             nan     0.0500    0.0886\n    80      233.5077             nan     0.0500   -0.6625\n   100      229.6637             nan     0.0500   -0.1938\n   120      228.0709             nan     0.0500   -1.6059\n   140      226.5744             nan     0.0500   -2.1526\n   150      225.3991             nan     0.0500   -4.0771\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      515.7228             nan     0.0500   12.5516\n     2      506.9640             nan     0.0500   11.5502\n     3      494.4920             nan     0.0500   11.3338\n     4      483.5697             nan     0.0500   12.3317\n     5      470.6341             nan     0.0500   11.3700\n     6      459.0847             nan     0.0500   10.1654\n     7      453.3616             nan     0.0500    7.0631\n     8      449.1088             nan     0.0500    5.2976\n     9      436.5430             nan     0.0500    4.9590\n    10      429.1679             nan     0.0500    7.1930\n    20      378.9990             nan     0.0500    3.7208\n    40      333.7602             nan     0.0500    1.4506\n    60      311.6156             nan     0.0500    0.1881\n    80      307.2575             nan     0.0500    0.0531\n   100      304.4675             nan     0.0500   -1.6765\n   120      304.4877             nan     0.0500   -0.8158\n   140      302.4214             nan     0.0500   -3.6289\n   150      300.9500             nan     0.0500   -2.0537\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      501.4446             nan     0.0500    8.0417\n     2      478.8249             nan     0.0500   19.1348\n     3      467.5422             nan     0.0500   12.8963\n     4      454.2387             nan     0.0500   13.9638\n     5      436.5553             nan     0.0500   12.0976\n     6      426.9150             nan     0.0500   10.6720\n     7      415.3835             nan     0.0500   12.9293\n     8      408.3450             nan     0.0500    9.3314\n     9      398.8125             nan     0.0500    9.4035\n    10      390.6857             nan     0.0500    9.1239\n    20      320.1592             nan     0.0500    3.5108\n    40      260.0502             nan     0.0500    0.4583\n    60      240.1340             nan     0.0500   -1.6100\n    80      230.9493             nan     0.0500   -3.0546\n   100      221.6668             nan     0.0500   -0.7369\n   120      213.1030             nan     0.0500   -0.7049\n   140      206.4353             nan     0.0500   -5.5520\n   150      203.0339             nan     0.0500   -3.5531\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      505.2404             nan     0.0500   13.0519\n     2      493.2266             nan     0.0500   13.0598\n     3      486.8939             nan     0.0500    8.6257\n     4      481.3093             nan     0.0500    7.3573\n     5      473.8629             nan     0.0500    9.3223\n     6      463.5212             nan     0.0500   10.2202\n     7      456.9517             nan     0.0500    8.2458\n     8      441.2870             nan     0.0500    4.2719\n     9      430.2795             nan     0.0500    7.2293\n    10      425.6857             nan     0.0500    5.8233\n    20      370.8108             nan     0.0500    2.4577\n    40      327.0011             nan     0.0500    1.5919\n    60      305.0594             nan     0.0500   -0.7852\n    80      296.1499             nan     0.0500   -0.7445\n   100      291.7308             nan     0.0500   -1.4099\n   120      288.8090             nan     0.0500   -0.3646\n   140      286.4107             nan     0.0500   -0.6550\n   150      284.6704             nan     0.0500   -0.3651\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      500.4740             nan     0.0500   16.3605\n     2      487.2526             nan     0.0500   17.0528\n     3      471.2841             nan     0.0500   18.7267\n     4      457.7679             nan     0.0500   16.6612\n     5      442.6809             nan     0.0500   15.4365\n     6      432.6263             nan     0.0500   12.9705\n     7      421.8605             nan     0.0500   13.3275\n     8      406.1928             nan     0.0500   11.5225\n     9      398.0222             nan     0.0500    7.3353\n    10      382.2499             nan     0.0500    5.3981\n    20      306.7173             nan     0.0500    3.4597\n    40      263.7565             nan     0.0500    1.8580\n    60      233.7769             nan     0.0500   -0.4072\n    80      223.4181             nan     0.0500   -1.9418\n   100      211.9348             nan     0.0500   -0.8884\n   120      204.7448             nan     0.0500   -2.9835\n   140      195.7127             nan     0.0500    0.0938\n   150      193.4665             nan     0.0500   -0.2871\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      512.5015             nan     0.0500   13.5072\n     2      496.9712             nan     0.0500   12.8894\n     3      478.5444             nan     0.0500   10.2031\n     4      462.8227             nan     0.0500    6.0580\n     5      454.7360             nan     0.0500    9.4974\n     6      444.1364             nan     0.0500    8.9335\n     7      432.3266             nan     0.0500    7.3340\n     8      425.4413             nan     0.0500    7.7037\n     9      414.2155             nan     0.0500    3.8672\n    10      407.0022             nan     0.0500    6.4744\n    20      360.5261             nan     0.0500    2.4893\n    40      318.2763             nan     0.0500    0.6644\n    60      305.9431             nan     0.0500   -3.0436\n    80      296.3283             nan     0.0500   -2.7188\n   100      290.9334             nan     0.0500   -0.0939\n   120      287.2325             nan     0.0500    0.4109\n   140      282.4971             nan     0.0500    0.0370\n   150      279.8896             nan     0.0500   -3.3168\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      495.6776             nan     0.1000   39.3756\n     2      448.4507             nan     0.1000   24.5585\n     3      427.2685             nan     0.1000   25.6857\n     4      412.0903             nan     0.1000   19.9738\n     5      388.1484             nan     0.1000   13.7611\n     6      374.3101             nan     0.1000   18.0161\n     7      356.2477             nan     0.1000   18.1923\n     8      339.6080             nan     0.1000   14.0661\n     9      329.3178             nan     0.1000   11.4359\n    10      316.8507             nan     0.1000    5.2794\n    20      250.2211             nan     0.1000   -9.0735\n    40      238.1120             nan     0.1000   -0.6687\n    60      235.5334             nan     0.1000   -9.1001\n    80      229.8206             nan     0.1000   -3.1005\n   100      221.4958             nan     0.1000   -2.4760\n   120      219.0097             nan     0.1000   -2.2243\n   140      215.9922             nan     0.1000   -2.7063\n   150      214.1854             nan     0.1000   -0.7490\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      506.9111             nan     0.1000   23.7829\n     2      479.2226             nan     0.1000   23.7469\n     3      452.6927             nan     0.1000   16.4389\n     4      435.5822             nan     0.1000   17.5878\n     5      424.5551             nan     0.1000   13.4274\n     6      418.8744             nan     0.1000    7.9912\n     7      410.7207             nan     0.1000   10.5276\n     8      396.8020             nan     0.1000    9.9706\n     9      385.3436             nan     0.1000    8.9619\n    10      372.1675             nan     0.1000    1.4913\n    20      332.5268             nan     0.1000    0.8795\n    40      308.7313             nan     0.1000   -2.2106\n    60      299.7539             nan     0.1000   -1.0448\n    80      298.1406             nan     0.1000   -0.7403\n   100      295.0316             nan     0.1000   -3.1524\n   120      292.3501             nan     0.1000   -0.2316\n   140      293.0412             nan     0.1000   -0.6254\n   150      292.3660             nan     0.1000   -0.2493\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      498.1977             nan     0.1000   37.3699\n     2      452.1571             nan     0.1000   32.2807\n     3      416.2105             nan     0.1000   25.3333\n     4      381.2366             nan     0.1000   19.2684\n     5      354.1117             nan     0.1000   10.4739\n     6      329.5265             nan     0.1000    0.1939\n     7      319.7589             nan     0.1000    7.3029\n     8      309.1044             nan     0.1000    8.7125\n     9      305.2439             nan     0.1000    5.1079\n    10      298.1541             nan     0.1000    7.9306\n    20      248.3441             nan     0.1000    2.1574\n    40      213.7902             nan     0.1000   -1.5314\n    60      196.1962             nan     0.1000   -0.1801\n    80      188.1208             nan     0.1000   -3.5896\n   100      186.5756             nan     0.1000    0.0653\n   120      181.0807             nan     0.1000   -0.2222\n   140      175.4194             nan     0.1000   -0.7088\n   150      173.0476             nan     0.1000   -0.9847\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      501.4198             nan     0.1000   28.5584\n     2      487.9044             nan     0.1000   16.8755\n     3      468.7586             nan     0.1000   22.7003\n     4      451.2732             nan     0.1000   16.7951\n     5      432.7331             nan     0.1000   16.3040\n     6      419.5093             nan     0.1000   14.4230\n     7      407.3609             nan     0.1000   12.4545\n     8      401.7973             nan     0.1000    7.3755\n     9      387.9096             nan     0.1000   10.6886\n    10      381.8566             nan     0.1000    7.1068\n    20      325.5841             nan     0.1000   -3.1170\n    40      301.2414             nan     0.1000   -1.0115\n    60      293.4964             nan     0.1000   -2.3755\n    80      284.7468             nan     0.1000    0.7800\n   100      278.7650             nan     0.1000   -1.6305\n   120      274.9706             nan     0.1000   -1.5282\n   140      271.5686             nan     0.1000   -1.0285\n   150      269.4051             nan     0.1000   -2.5295\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      484.4908             nan     0.1000   47.0165\n     2      459.1532             nan     0.1000   32.7884\n     3      411.4341             nan     0.1000   15.1576\n     4      397.3610             nan     0.1000   18.1434\n     5      385.2504             nan     0.1000   17.1943\n     6      371.6408             nan     0.1000   16.5860\n     7      363.3890             nan     0.1000   11.0551\n     8      340.2203             nan     0.1000    3.8069\n     9      326.7024             nan     0.1000   10.4298\n    10      318.5981             nan     0.1000    7.6646\n    20      258.1279             nan     0.1000   -5.0597\n    40      234.0316             nan     0.1000   -4.8682\n    60      210.7783             nan     0.1000    0.7027\n    80      197.4233             nan     0.1000   -8.0101\n   100      184.4614             nan     0.1000   -2.8236\n   120      177.1702             nan     0.1000   -3.8994\n   140      175.6088             nan     0.1000   -7.1664\n   150      173.0000             nan     0.1000   -6.0800\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      495.7508             nan     0.1000   28.0731\n     2      471.0871             nan     0.1000   24.2579\n     3      443.5972             nan     0.1000   13.8223\n     4      420.7948             nan     0.1000   12.3956\n     5      400.4959             nan     0.1000   10.2990\n     6      395.5695             nan     0.1000    6.6717\n     7      386.3309             nan     0.1000   10.5688\n     8      382.6318             nan     0.1000    4.8405\n     9      372.4409             nan     0.1000    8.0270\n    10      368.6189             nan     0.1000    5.3088\n    20      319.1959             nan     0.1000    1.7456\n    40      289.1361             nan     0.1000   -4.3657\n    60      284.2839             nan     0.1000    0.2216\n    80      278.9552             nan     0.1000   -0.2699\n   100      272.9025             nan     0.1000   -0.1078\n   120      271.4837             nan     0.1000   -2.2803\n   140      267.3010             nan     0.1000   -8.1097\n   150      266.2038             nan     0.1000   -1.3534\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      494.1470             nan     0.0500   14.4951\n     2      480.8022             nan     0.0500   16.0806\n     3      467.2408             nan     0.0500   12.5449\n     4      450.3315             nan     0.0500   16.4406\n     5      436.4743             nan     0.0500    8.9174\n     6      425.6020             nan     0.0500    3.9853\n     7      413.9692             nan     0.0500    7.1404\n     8      397.1110             nan     0.0500    4.7293\n     9      387.5373             nan     0.0500    0.3898\n    10      384.7174             nan     0.0500    4.0023\n    20      319.2720             nan     0.0500    0.0944\n    40      247.7880             nan     0.0500    0.8318\n    60      229.7626             nan     0.0500   -1.8751\n    80      221.5428             nan     0.0500    0.4448\n   100      216.4465             nan     0.0500   -2.6231\n   120      213.0474             nan     0.0500   -0.9084\n   140      209.4085             nan     0.0500   -3.1658\n   150      208.7149             nan     0.0500   -2.9333\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      496.1452             nan     0.0500   11.6185\n     2      486.2210             nan     0.0500   11.3407\n     3      473.0711             nan     0.0500   10.0920\n     4      466.6607             nan     0.0500    8.3720\n     5      457.3050             nan     0.0500    8.7417\n     6      444.6991             nan     0.0500    5.1829\n     7      440.6397             nan     0.0500    5.5825\n     8      431.8646             nan     0.0500    6.6043\n     9      427.6738             nan     0.0500    5.6150\n    10      423.8989             nan     0.0500    5.0790\n    20      377.1667             nan     0.0500    1.4370\n    40      327.3447             nan     0.0500    1.2494\n    60      311.6292             nan     0.0500   -1.8985\n    80      306.0609             nan     0.0500   -0.4964\n   100      304.0250             nan     0.0500    0.1569\n   120      301.9966             nan     0.0500   -0.8212\n   140      300.4978             nan     0.0500   -2.2557\n   150      299.9391             nan     0.0500   -1.5191\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      493.2894             nan     0.0500   16.2181\n     2      474.9891             nan     0.0500   18.5574\n     3      458.7080             nan     0.0500   16.6511\n     4      437.6541             nan     0.0500   13.5351\n     5      418.5381             nan     0.0500    7.2100\n     6      402.1320             nan     0.0500    6.8330\n     7      394.7852             nan     0.0500    8.5069\n     8      391.9181             nan     0.0500    3.8212\n     9      380.8959             nan     0.0500    8.1158\n    10      376.0623             nan     0.0500    6.7181\n    20      311.9364             nan     0.0500    3.5364\n    40      259.4485             nan     0.0500    1.2194\n    60      236.6428             nan     0.0500    1.4031\n    80      212.9736             nan     0.0500   -0.8422\n   100      202.2401             nan     0.0500    0.3386\n   120      189.1797             nan     0.0500   -0.0823\n   140      178.9500             nan     0.0500   -2.0501\n   150      174.6864             nan     0.0500    0.0413\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      491.7041             nan     0.0500   11.3042\n     2      482.5412             nan     0.0500   10.8221\n     3      471.8447             nan     0.0500   10.6187\n     4      465.7620             nan     0.0500    7.7380\n     5      456.7663             nan     0.0500    8.9541\n     6      448.1323             nan     0.0500    8.6091\n     7      442.8677             nan     0.0500    6.8480\n     8      432.2391             nan     0.0500    5.8421\n     9      420.1190             nan     0.0500    2.0474\n    10      416.9401             nan     0.0500    4.4951\n    20      369.4904             nan     0.0500    2.9045\n    40      332.1179             nan     0.0500    0.6837\n    60      309.5832             nan     0.0500    0.5121\n    80      300.2725             nan     0.0500   -0.9496\n   100      296.1961             nan     0.0500   -2.6610\n   120      291.7474             nan     0.0500   -3.7030\n   140      285.3343             nan     0.0500   -0.0028\n   150      284.1849             nan     0.0500   -0.1697\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      483.9000             nan     0.0500   16.2393\n     2      458.4423             nan     0.0500   11.7301\n     3      445.2305             nan     0.0500   13.0421\n     4      434.7086             nan     0.0500   11.3459\n     5      421.3924             nan     0.0500   13.2643\n     6      407.5367             nan     0.0500   11.5703\n     7      392.4734             nan     0.0500    8.1855\n     8      380.4260             nan     0.0500    7.4834\n     9      367.3359             nan     0.0500    3.5845\n    10      362.2337             nan     0.0500    5.5398\n    20      316.3492             nan     0.0500    4.2763\n    40      253.9593             nan     0.0500    0.5995\n    60      229.9501             nan     0.0500   -3.3904\n    80      209.8942             nan     0.0500   -0.8803\n   100      193.9835             nan     0.0500   -1.9297\n   120      183.0993             nan     0.0500   -1.6678\n   140      177.1832             nan     0.0500    0.1335\n   150      172.2051             nan     0.0500   -1.9114\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      492.9910             nan     0.0500   12.5071\n     2      483.1682             nan     0.0500   10.3808\n     3      469.8706             nan     0.0500   10.0270\n     4      459.1862             nan     0.0500    9.5167\n     5      451.1130             nan     0.0500    7.8964\n     6      443.4928             nan     0.0500    8.5407\n     7      431.9378             nan     0.0500    3.8253\n     8      425.2084             nan     0.0500    6.1283\n     9      416.9015             nan     0.0500    6.6139\n    10      412.6919             nan     0.0500    4.9776\n    20      362.5965             nan     0.0500    3.0548\n    40      324.2364             nan     0.0500    0.3753\n    60      305.3182             nan     0.0500   -0.4216\n    80      295.5415             nan     0.0500   -0.0465\n   100      291.4679             nan     0.0500    0.2950\n   120      287.5842             nan     0.0500   -0.5602\n   140      283.9186             nan     0.0500   -0.4795\n   150      281.1872             nan     0.0500   -2.0739\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      463.2776             nan     0.1000   31.5502\n     2      452.6912             nan     0.1000   13.7769\n     3      439.5924             nan     0.1000   15.9127\n     4      412.5546             nan     0.1000   24.0640\n     5      390.9418             nan     0.1000   19.8218\n     6      378.6830             nan     0.1000   15.4347\n     7      359.1075             nan     0.1000   11.6625\n     8      354.8509             nan     0.1000    5.7622\n     9      350.9049             nan     0.1000    5.9391\n    10      332.0390             nan     0.1000    6.7609\n    20      262.9596             nan     0.1000    4.8239\n    40      225.0601             nan     0.1000   -1.9432\n    60      219.9295             nan     0.1000   -3.5129\n    80      218.7235             nan     0.1000   -5.5097\n   100      212.3788             nan     0.1000    0.3139\n   120      208.1212             nan     0.1000   -3.6553\n   140      206.0737             nan     0.1000   -1.3464\n   150      205.8242             nan     0.1000   -0.0468\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      483.6211             nan     0.1000   25.4034\n     2      472.1249             nan     0.1000   15.7283\n     3      450.2856             nan     0.1000   17.6376\n     4      435.0307             nan     0.1000   14.4224\n     5      424.0408             nan     0.1000   12.6007\n     6      416.7661             nan     0.1000    9.9604\n     7      402.8060             nan     0.1000    9.1763\n     8      388.4084             nan     0.1000    3.8711\n     9      383.5590             nan     0.1000    6.3147\n    10      371.6039             nan     0.1000    4.2871\n    20      326.9103             nan     0.1000    2.3054\n    40      311.4776             nan     0.1000   -1.1582\n    60      304.5661             nan     0.1000   -1.4632\n    80      301.9401             nan     0.1000   -0.3411\n   100      298.4863             nan     0.1000    0.0396\n   120      295.9189             nan     0.1000   -0.1266\n   140      291.9749             nan     0.1000   -2.4359\n   150      291.5701             nan     0.1000   -1.7786\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      458.4045             nan     0.1000   30.1853\n     2      443.6321             nan     0.1000   19.7649\n     3      406.7395             nan     0.1000   14.7190\n     4      385.0979             nan     0.1000   18.3271\n     5      358.6986             nan     0.1000    7.7692\n     6      342.1346             nan     0.1000   10.8272\n     7      334.4099             nan     0.1000   10.4801\n     8      327.2207             nan     0.1000    9.2749\n     9      309.8708             nan     0.1000    5.1204\n    10      309.6676             nan     0.1000   -1.1778\n    20      261.2927             nan     0.1000    2.6898\n    40      222.6954             nan     0.1000   -2.0278\n    60      198.2351             nan     0.1000   -5.2380\n    80      168.3334             nan     0.1000   -0.9023\n   100      155.5179             nan     0.1000   -2.9976\n   120      146.1538             nan     0.1000   -5.7995\n   140      139.0896             nan     0.1000   -0.3749\n   150      136.6473             nan     0.1000   -1.9428\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      473.3514             nan     0.1000   18.0111\n     2      450.7682             nan     0.1000   20.5029\n     3      439.3945             nan     0.1000   14.3061\n     4      424.2238             nan     0.1000   15.2161\n     5      414.4217             nan     0.1000   11.6357\n     6      403.5750             nan     0.1000   11.7041\n     7      389.8907             nan     0.1000    8.7721\n     8      378.6190             nan     0.1000    7.1362\n     9      367.9444             nan     0.1000    3.0638\n    10      364.0416             nan     0.1000    5.1525\n    20      327.8997             nan     0.1000    1.3383\n    40      293.6615             nan     0.1000   -0.7966\n    60      285.4966             nan     0.1000   -1.6737\n    80      277.3091             nan     0.1000   -4.3152\n   100      273.3986             nan     0.1000   -4.0827\n   120      270.0016             nan     0.1000   -1.5000\n   140      266.6565             nan     0.1000   -0.2313\n   150      264.8085             nan     0.1000   -0.9240\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      489.2388             nan     0.1000   25.5797\n     2      458.6206             nan     0.1000   31.2135\n     3      443.7615             nan     0.1000   20.0193\n     4      414.8784             nan     0.1000   26.9650\n     5      383.4696             nan     0.1000   14.3124\n     6      364.3929             nan     0.1000    1.7713\n     7      354.3771             nan     0.1000   13.2923\n     8      336.4877             nan     0.1000    8.5427\n     9      326.4056             nan     0.1000    8.7564\n    10      319.5598             nan     0.1000    9.0447\n    20      252.6052             nan     0.1000   -2.9263\n    40      202.1616             nan     0.1000   -2.6598\n    60      184.4906             nan     0.1000   -2.9866\n    80      161.4190             nan     0.1000   -1.2092\n   100      146.6592             nan     0.1000   -0.9238\n   120      135.2206             nan     0.1000   -1.7554\n   140      127.6935             nan     0.1000   -2.9619\n   150      125.4082             nan     0.1000   -0.4512\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      475.7059             nan     0.1000   21.7135\n     2      448.0852             nan     0.1000   12.1944\n     3      426.2264             nan     0.1000   12.3582\n     4      417.7031             nan     0.1000   10.2242\n     5      404.2886             nan     0.1000   12.4386\n     6      393.4743             nan     0.1000    8.2722\n     7      381.9160             nan     0.1000    6.1413\n     8      371.7549             nan     0.1000    2.5500\n     9      364.6727             nan     0.1000    4.9672\n    10      356.8863             nan     0.1000    4.7460\n    20      327.5802             nan     0.1000    2.5199\n    40      299.8564             nan     0.1000   -2.0108\n    60      283.1671             nan     0.1000   -0.3516\n    80      275.0219             nan     0.1000   -0.6790\n   100      267.3035             nan     0.1000   -2.3006\n   120      264.0318             nan     0.1000   -0.1567\n   140      261.4434             nan     0.1000   -2.4537\n   150      257.5156             nan     0.1000   -0.9746\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1      417.7278             nan     0.0500   14.0035\n     2      399.7797             nan     0.0500   16.3937\n     3      386.7966             nan     0.0500   15.1340\n     4      367.8199             nan     0.0500    4.7096\n     5      360.6254             nan     0.0500   10.3438\n     6      346.1674             nan     0.0500    9.8387\n     7      330.0253             nan     0.0500    5.4990\n     8      315.4214             nan     0.0500    5.3985\n     9      308.7437             nan     0.0500    6.1170\n    10      298.1822             nan     0.0500    4.3389\n    20      250.6134             nan     0.0500    2.0981\n    40      210.3771             nan     0.0500   -1.4932\n    50      200.9115             nan     0.0500    0.5636\n\n# Extracting importance directly\nimportance &lt;- relative.influence(gbm_tuned$finalModel)\n\nn.trees not given. Using 50 trees.\n\n# Converting to a data frame\nimportance_df &lt;- data.frame(Feature=names(importance), Relative_Influence=importance)\n\n# Plotting graph with labels\nggplot(importance_df, aes(x=reorder(Feature, Relative_Influence), y=Relative_Influence)) +\n  geom_bar(stat=\"identity\", fill=\"blue\") +\n  coord_flip() +\n  labs(title=\"Relative Influence of Features\", x=\"Features\", y=\"Relative Influence\")\n\n\n\n\nAs we can see in the chart, Black has the lowest relative influence on the prediction, which may contradict our findings. However, there are a few explanations: “Influence” reflects how much each feature contributes to the model’s predictive power, but might not directly correlate with a feature’s impact in the real world. Complex interactions between demographics and fatalities might not align directly with simple correlations. Which means there are still more work need to be done in order to prefectly predict with this model.\n\n# Convert all columns except 'City' to numeric\nnumeric_columns &lt;- c(\"Fatalities\", \"Median.Age\", \"Male.Population\", \"Female.Population\",\n                     \"Foreign.born\", \"Average.Household.Size\", \"White\", \"Black.or.African.American\",\n                     \"Hispanic.or.Latino\", \"Asian\", \"American.Indian.and.Alaska.Native\")\n\nmodel_data[numeric_columns] &lt;- lapply(model_data[numeric_columns], as.numeric)\n\n# Checking correlation\ncorrelations &lt;- cor(model_data[numeric_columns])\nprint(correlations)\n\n                                    Fatalities   Median.Age Male.Population\nFatalities                         1.000000000 -0.074184297      0.55336919\nMedian.Age                        -0.074184297  1.000000000     -0.04164571\nMale.Population                    0.553369192 -0.041645713      1.00000000\nFemale.Population                  0.528890626 -0.038774164      0.99918619\nForeign.born                       0.463391208 -0.009282113      0.96256363\nAverage.Household.Size            -0.003754532 -0.201400398     -0.03885526\nWhite                              0.617814804 -0.046379442      0.97716817\nBlack.or.African.American          0.353806901 -0.053287841      0.89630327\nHispanic.or.Latino                 0.651232343 -0.072295106      0.93382335\nAsian                              0.388249572  0.019503818      0.91398224\nAmerican.Indian.and.Alaska.Native  0.544014138 -0.074523424      0.82770167\n                                  Female.Population Foreign.born\nFatalities                               0.52889063  0.463391208\nMedian.Age                              -0.03877416 -0.009282113\nMale.Population                          0.99918619  0.962563634\nFemale.Population                        1.00000000  0.964610827\nForeign.born                             0.96461083  1.000000000\nAverage.Household.Size                  -0.04102717  0.035162326\nWhite                                    0.97122505  0.913091187\nBlack.or.African.American                0.90740791  0.837147775\nHispanic.or.Latino                       0.92635689  0.922147546\nAsian                                    0.91569242  0.955500287\nAmerican.Indian.and.Alaska.Native        0.82157536  0.768832056\n                                  Average.Household.Size       White\nFatalities                                  -0.003754532  0.61781480\nMedian.Age                                  -0.201400398 -0.04637944\nMale.Population                             -0.038855255  0.97716817\nFemale.Population                           -0.041027171  0.97122505\nForeign.born                                 0.035162326  0.91309119\nAverage.Household.Size                       1.000000000 -0.05646048\nWhite                                       -0.056460475  1.00000000\nBlack.or.African.American                   -0.115697953  0.81597892\nHispanic.or.Latino                           0.115517490  0.94345615\nAsian                                        0.023223332  0.85188531\nAmerican.Indian.and.Alaska.Native           -0.047227375  0.84503756\n                                  Black.or.African.American Hispanic.or.Latino\nFatalities                                       0.35380690         0.65123234\nMedian.Age                                      -0.05328784        -0.07229511\nMale.Population                                  0.89630327         0.93382335\nFemale.Population                                0.90740791         0.92635689\nForeign.born                                     0.83714777         0.92214755\nAverage.Household.Size                          -0.11569795         0.11551749\nWhite                                            0.81597892         0.94345615\nBlack.or.African.American                        1.00000000         0.73603510\nHispanic.or.Latino                               0.73603510         1.00000000\nAsian                                            0.78218843         0.83076721\nAmerican.Indian.and.Alaska.Native                0.65871233         0.79384212\n                                       Asian American.Indian.and.Alaska.Native\nFatalities                        0.38824957                        0.54401414\nMedian.Age                        0.01950382                       -0.07452342\nMale.Population                   0.91398224                        0.82770167\nFemale.Population                 0.91569242                        0.82157536\nForeign.born                      0.95550029                        0.76883206\nAverage.Household.Size            0.02322333                       -0.04722737\nWhite                             0.85188531                        0.84503756\nBlack.or.African.American         0.78218843                        0.65871233\nHispanic.or.Latino                0.83076721                        0.79384212\nAsian                             1.00000000                        0.73308589\nAmerican.Indian.and.Alaska.Native 0.73308589                        1.00000000\n\ncor_black_fatalities &lt;- cor.test(model_data$Fatalities, model_data$Black.or.African.American)\nprint(cor_black_fatalities)\n\n\n    Pearson's product-moment correlation\n\ndata:  model_data$Fatalities and model_data$Black.or.African.American\nt = 7.556, df = 399, p-value = 2.871e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2650615 0.4366085\nsample estimates:\n      cor \n0.3538069 \n\n\nAccording to the summarization above, the correlation analysis reveals a moderate positive relationship between the “Black or African-American” demographic and fatality rates, with a correlation of 0.35 and a p-value indicating statistical significance. This suggests that as the percentage of this demographic increases, fatality rates tend to increase as well. However, the “Hispanic or Latino” demographic shows a higher correlation with fatalities, at 0.65. The feature importance results from the GBM model reflect these relationships, with “Black or African-American” ranking fourth in influence. These findings offer insights into how different demographic features contribute to predicting fatality rates."
  },
  {
    "objectID": "draft for analysis.html",
    "href": "draft for analysis.html",
    "title": "draft for analysis",
    "section": "",
    "text": "##Introduction In recent years, police-related fatalities have become a focal point of public and academic debate, underscoring critical concerns about racial disparities within law enforcement across the United States. This analysis seeks to explore the extent to which racial disparity, especially against Black individuals, contributes to the demographics of these fatalities. The urgency of this issue is amplified by ongoing societal calls for justice and reform, making it essential to understand the patterns and factors that drive these disparities.\n#Thesis Statement This research posits that racial disparity is a significant factor influencing the demographics of police-related fatalities, with Black individuals experiencing disproportionately high mortality rates in comparison to other races across various cities.\n#Research Questions To provide a structured exploration of this issue, this analysis will address the following key questions:\n\nHow do mortality rates from police-related fatalities among Black individuals compare to those of other races across different cities?\nWhat factors might contribute to any observed disparities in these mortality rates?\nAre there specific geographic or demographic contexts in which these disparities are more pronounced?\n\n#Significance of the Analysis The findings of this analysis are intended to contribute to the broader dialogue on racial equity and law enforcement practices. By examining the intersection of race and police-related fatalities, this study aims to provide data-driven insights that could inform policy decisions and advocacy efforts aimed at reducing these disparities. Additionally, this analysis will serve as an educational resource for scholars, policymakers, and the public, fostering a deeper understanding of how racial dynamics shape outcomes in law enforcement encounters.\nThrough a combination of quantitative methods and statistical modeling, this analysis page will offer a detailed examination of the available data, aiming to present a clear and comprehensive picture of how race influences the risk of fatality in police interactions. The next sections will describe the data used for this analysis, the methodology employed, and the results obtained,thereby framing the context for a rigorous discussion on this critical social issue.\n##Data Description This analysis leverages two key datasets:\nPolice Fatality Dataset & U.S. Cities Demographic Dataset\nTo load the dataset, run the attached R script “combining-datasets.R”.\n\nsource(\"combining-datasets.R\")\nrm(police_f,us_dem,us_dem_wid)\n\nAfter running this R script, the two datasets we used will be merged, cleared and loaded into your environment called police_dem. For more information on these two datasets and merging process, see the data page. data page.\n##Exploratory Data Analysis (EDA)\nStarted by exploring the distribution of police-related fatalities by race.\n\nfatalities_by_race &lt;- police_dem |&gt;\n  group_by(Race) |&gt;\n  summarize(Fatalities = n()) |&gt;\n  ungroup()\n\nggplot(fatalities_by_race, aes(x = reorder(Race, -Fatalities), y = Fatalities, fill = Race)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Distribution of Police-Related Fatalities by Race\",\n    x = \"Race\",\n    y = \"Number of Fatalities\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nAs expected, the highest number of deaths occurred among blacks or African Americans.\nThe next step is to compare this with demographic data, and it would be interesting to examine the relationship between each racial group’s percentage of total deaths and their percentage of the total population of the sampled city. This would help to determine how specific racial groups are disproportionately affected.\n\nfatalities_by_race_city_state &lt;- police_dem |&gt;\n  group_by(State, City, Race) |&gt;\n  summarize(Fatalities = n(), .groups = 'drop')\n\npopulation_by_race_city_state &lt;- police_dem |&gt;\n  group_by(State, City) |&gt;\n  summarise(\n    White = first(White),\n    `Black or African-American` = first(`Black or African-American`),\n    `Hispanic or Latino` = first(`Hispanic or Latino`),\n    Asian = first(Asian),\n    `American Indian and Alaska Native` = first(`American Indian and Alaska Native`),\n    .groups = 'drop'\n  )|&gt;\n  pivot_longer(cols = -c(City, State), names_to = \"Race\", values_to = \"Population\")\n\ncomparison_df &lt;- \n  left_join(fatalities_by_race_city_state, population_by_race_city_state, by = c(\"State\", \"City\", \"Race\"))|&gt;\n  mutate(\n    FatalityPercentage = (Fatalities / sum(Fatalities)) * 100,\n    PopulationPercentage = (Population / sum(Population)) * 100,\n    diffinPercentage = FatalityPercentage - PopulationPercentage\n  )|&gt;\n  arrange(desc(diffinPercentage))\nprint(comparison_df)\n\n# A tibble: 922 × 8\n   State      City        Race          Fatalities Population FatalityPercentage\n   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;              &lt;int&gt;      &lt;int&gt;              &lt;dbl&gt;\n 1 California Los Angeles Black or Afr…         87     404868              2.16 \n 2 Texas      Houston     Black or Afr…         82     529431              2.04 \n 3 California Los Angeles Hispanic or …        122    1936732              3.03 \n 4 Nevada     Las Vegas   White                 56     429142              1.39 \n 5 Illinois   Chicago     Black or Afr…         67     873316              1.67 \n 6 Nevada     Las Vegas   Black or Afr…         36      84987              0.895\n 7 Nevada     Las Vegas   Hispanic or …         38     204913              0.945\n 8 Texas      Dallas      Black or Afr…         38     322570              0.945\n 9 California Oakland     Black or Afr…         30     118228              0.746\n10 California Fresno      Hispanic or …         33     256145              0.820\n# ℹ 912 more rows\n# ℹ 2 more variables: PopulationPercentage &lt;dbl&gt;, diffinPercentage &lt;dbl&gt;\n\n\n\nsum_df&lt;- comparison_df|&gt;\n  group_by(Race)|&gt;\n  summarise(sumfatalityp = sum(FatalityPercentage),\n          sumpopulationp = sum(PopulationPercentage))|&gt;\n  pivot_longer(cols = c(sumfatalityp, sumpopulationp), names_to = \"Type\", values_to = \"Percentage\") %&gt;%\n  mutate(Type = recode(Type, 'sumfatalityp' = 'Fatality Percentage', 'sumpopulationp' = 'Population Percentage'))\n\nggplot(sum_df, aes(x = Type, y = Percentage, fill = Type)) +\n  geom_col(position = position_dodge(width = 0.8)) +\n  facet_wrap(~ Race) +\n  labs(\n    title = \"Comparison of Fatality and Population Percentages by Race\",\n    x = NULL,\n    y = \"Percentage\"\n  ) +\n  scale_fill_manual(values = c(\"Fatality Percentage\" = \"red\", \"Population Percentage\" = \"blue\")) +\n  theme(\n    axis.text.x = element_blank(),  \n    axis.ticks.x = element_blank(), \n    strip.background = element_blank(),\n    strip.text.x = element_text(size = 10)\n  ) \n\n\n\n\n\nlibrary(tidycensus)\nlibrary(sf)\n\n\nstates_sf &lt;- get_decennial(geography = \"state\", \n                           year = 2020, \n                           variables = \"H1_001N\", \n                           geometry = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\nstate_level_comparison_df &lt;- comparison_df %&gt;%\n  group_by(State, Race) %&gt;%\n  summarize(Diff_Percentage = mean(diffinPercentage, na.rm = TRUE), .groups = 'drop')\n\nstate_data_sf &lt;- inner_join(states_sf, state_level_comparison_df, by = c(\"NAME\" = \"State\"))\n\n\nstate_data_sf %&gt;%\n  ggplot() +\n  geom_sf(aes(fill = Diff_Percentage), color = NA) +\n  scale_fill_gradient2(\n    low = \"blue\", mid = \"white\", high = \"red\",  # Adjust colors if needed\n    midpoint = 0, \n    limit = c(min(state_data_sf$Diff_Percentage, na.rm = TRUE), max(state_data_sf$Diff_Percentage, na.rm = TRUE)), \n    name = \"Diff in Percentage\",\n    na.value = \"white\"\n  ) +\n  facet_wrap(~ Race) +\n  labs(\n    title = \"Difference in Police Fatality and Population Percentages by Race and State\",\n    subtitle = \"Faceted by Race\",\n    fill = \"Diff in Percentage\"\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(size = 14, hjust = 0.5), \n    plot.subtitle = element_text(size = 12),\n    strip.text = element_text(size = 10)  # Adjust facet label text size\n  ) +\n  coord_sf(xlim = c(-125, -67), ylim = c(24, 50), expand = FALSE)\n\n\n\n\n##Modelling\n\nmodel_data&lt;- police_dem|&gt;\n  group_by(City)|&gt;\n  summarise(Fatalities = n(),\n            Median.Age = first(Median.Age),\n            Male.Population = first(Male.Population),\n            Female.Population = first(Female.Population),\n            Foreign.born = first(Foreign.born),\n            Average.Household.Size = first(Average.Household.Size),\n            White = first(White),\n            `Black or African-American` = first(`Black or African-American`),\n            `Hispanic or Latino` = first(`Hispanic or Latino`),\n            Asian = first(Asian),\n            `American Indian and Alaska Native` = first(`American Indian and Alaska Native`)\n            )|&gt;\n  arrange(desc(Fatalities))\nmodel_data &lt;- na.omit(model_data)\nprint(model_data)\n\n# A tibble: 401 × 12\n   City     Fatalities Median.Age Male.Population Female.Population Foreign.born\n   &lt;chr&gt;         &lt;int&gt;      &lt;dbl&gt;           &lt;int&gt;             &lt;int&gt;        &lt;int&gt;\n 1 Los Ang…        252       35           1958998           2012898      1485425\n 2 Houston         179       32.6         1149686           1148942       696210\n 3 Las Veg…        133       37.5          310568            313201       127609\n 4 Chicago          91       34.2         1320015           1400541       573463\n 5 Dallas           78       32.6          639019            661063       326825\n 6 Phoenix          75       33.8          786833            776168       300702\n 7 Fresno           67       30            256130            263942       103453\n 8 Kansas …         61       35.9          228430            246931        37787\n 9 San Die…         59       34.5          693826            701081       373842\n10 Long Be…         52       34.6          238159            236013       127764\n# ℹ 391 more rows\n# ℹ 6 more variables: Average.Household.Size &lt;dbl&gt;, White &lt;int&gt;,\n#   `Black or African-American` &lt;int&gt;, `Hispanic or Latino` &lt;int&gt;, Asian &lt;int&gt;,\n#   `American Indian and Alaska Native` &lt;int&gt;\n\n\n\nggplot(model_data, aes(x = Fatalities)) + \n  geom_histogram(binwidth = 1, fill = 'blue', color = 'black')\n\n\n\n\nThe distrubution is highly skewed\n\nggplot(model_data, aes(x = Median.Age, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nggplot(model_data, aes(x = Male.Population, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nggplot(model_data, aes(x = Female.Population, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nggplot(model_data, aes(x = Foreign.born, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nggplot(model_data, aes(x = Average.Household.Size, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nggplot(model_data, aes(x = White, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nggplot(model_data, aes(x = `Black or African-American`, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  }
]