[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This comes from the file data.qmd.\nYour first steps in this project will be to find data to work on.\nI recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset_ignore folder. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Add the files just like you would any other file.\nIf you create a folder named data this will cause problems.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour load_and_clean_data.R file is how you will load and clean your data. Here is a an example of a very simple one.\n\nsource(\n  \"scripts/load_and_clean_data.R\",\n  echo = TRUE # Use echo=FALSE or omit it to avoid code output  \n)\n\n\n&gt; library(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n&gt; loan_data &lt;- read_csv(here::here(\"dataset\", \"loan_refusal.csv\"))\n\n\nRows: 20 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): bank\ndbl (4): min, white, himin, hiwhite\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n&gt; loan_data_clean &lt;- loan_data\n\n&gt; write_csv(loan_data_clean, file = here::here(\"dataset\", \n+     \"loan_refusal_clean.csv\"))\n\n&gt; save(loan_data_clean, file = here::here(\"dataset/loan_refusal.RData\"))\n\n\nYou should never use absolute paths (eg. /Users/danielsussman/path/to/project/ or C:\\MA415\\\\Final_Project\\).\nYou might consider using the here function from the here package to avoid path problems.\n\n\nLoad and clean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which could be plain text files or .RData files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use [cleaning script](/scripts/load_and_clean_data.R) wich appears as cleaning script."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones and summarize the rest.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your load_and_clean_data.R file.\nRrename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your load_and_clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "data.html#background",
    "href": "data.html#background",
    "title": "Data",
    "section": "Background",
    "text": "Background\n\nPolice Fatality Dataset\nThe source of the original dataset is Fatal Encounters, a blog created by Dr. Brian Burghart, a journalist and part-time researcher for the University of Southern California. Dr. Burghart and his associates compiled media reports and police records to create the database. The data represents deaths that occurred when police were present or were caused by police, including “on-duty, off-duty, criminal, line-of-duty, local, federal, intentional, [and] accidental.” The goal of Fatal Encounters is to create a national database that anyone can use for whatever purpose. This dataset has 36 variables, though a few are redundant, such as Unique ID and Unique identifier (redundant), or temporary. Some of the main variables are listed in the table below. Many of the variables are self-evident and therefore are lacking a description.\n\n\n\n\n\n\n\n\n\nCategory\nType\nName\nDescription\n\n\n\n\nVictim\nCharacter\nName\n-\n\n\nVictim\nInteger\nAge\n-\n\n\nVictim\nCharacter\nGender\n-\n\n\nVictim\nCharacter\nURL of image (PLS NO HOTLINKS)\nPhoto of the victim\n\n\nIncident\nNumeric (Date)\nDate of injury resulting in death (month/day/year)\n-\n\n\nLocation\nCharacter\nLocation of injury (address)\n-\n\n\nLocation\nCharacter\nLocation of death (city)\n-\n\n\nLocation\nCharacter\nState\nInputted as state abbreviations\n\n\nIncident\nCharacter\nAgency or agencies involved\nPolice departments, shrieff’s offices, etc involved in the incident\n\n\nIncident\nCharacter\nArmed/Unarmed\nWhether the victim was armed or unarmed during the incident\n\n\nIncident\nCharacter\nAlleged weapon\nIf the victim was armed, categorizes the alleged weapon\n\n\nIncident\nCharacter\nAggressive physical movement\nDescription of aggressive physical movement the victim performed (e.g. Advanced toward officer(s))\n\n\nIncident\nCharacter\nFleeing/Not fleeing\nWhether the victim attempted to flee, and if so by what method (e.g. Vehicle)\n\n\nIncident\nCharacter\nBrief description\nA brief description of the incident\n\n\nIncident\nCharacter\nSupporting document link\nA link to a new article reporting the incident or a photo of an official document describing the incident\n\n\n\nThe cleaned dataset was posted by Chris Awram to the data.world site. He cleaned the Fatal Encounters dataset and combined it with additional data from Gun Violence Archive and Data Society, which sourced its data from the Washington Post. The purpose of this cleaned dataset was to shed light on altercations with the police in which individuals were killed. This dataset subsets the Fatal Encounters dataset, discarding observations where the deceased individual was not killed by police or by suicide, but only in the presence of a police officer. For example, the death recorded by Unique ID 31495 in the Fatal Encounters dataset resulted from a car accident that was witnessed by a police officer. This observation was filtered out from the cleaned dataset because the deceased was neither killed by a police officer nor a victim of suicide.\nThis dataset has 12 variables, including a unique ID for each observation (uid). The variables are listed in the table below.\n\n\n\nCategory\nType\nName\nDescription\n\n\n\n\nVictim\nCharacter\nname\n\n\n\nVictim\nInteger\nage\n\n\n\nVictim\nCharacter\ngender\n\n\n\nVictim\nCharacter\nrace\n\n\n\nIncident\nCharacter\ndate\n\n\n\nLocation\nCharacter\ncity\n\n\n\nLocation\nCharacter\nstate\n\n\n\nIncident\nCharacter\nmanner_of_death\n\n\n\nIncident\nCharacter\narmed\n\n\n\nIncident\nLogical\nmental_illness\n\n\n\nIncident\nLogical\nflee\n\n\n\n\nThe variables name, age, gender, and race describe attributes of the deceased. Date, city, and state describe the location and time of the altercation. Manner_of_death indicates whether the individual died by being shot, tasered, both, or some other method. Armed represents what weapon, if any, the individual was carrying, whether a gun, knife, or some other weapon. Mental_illness indicates whether the individual was mentally ill and flee indicates whether the individual attempted to flee the altercation. This cleaned dataset removed some supplemental details from the Fatal Encounter dataset, such as the address of incident, the corresponding latitude and longitude, the agencies involved, and a description of the incident.\n\n\nU.S. Cities Demographic Dataset\nWe supplemented our police fatality dataset by joining it with a U.S. cities demographic dataset. This demographic dataset was sourced from the U.S. Census Bureau’s 2015 American Community Survey and posted to the opendatasoft site by the U.S. Census Bureau. U.S. cities with populations greater than or equal to 65,000 are included in this dataset. There are 12 variables, which cover location, age, sex, and race, among other subjects. The variables City, State, and State Code describe location. Many of the variables are self-explanatory: Total Population, Male Population, Female Population, Number of Veterans, Median Age, and Average Household Size. Foreign-born provides a count of the number of individuals not born in the U.S. The variables Race and Count are connected, with Count describing the number of individuals of a certain race. There are multiple observations per City, each with a different Race and Count."
  },
  {
    "objectID": "data.html#cleaning",
    "href": "data.html#cleaning",
    "title": "Data",
    "section": "Cleaning",
    "text": "Cleaning\nThe police fatality dataset from data.world required no additional cleaning when first downloaded. Our process of loading the police fatality data can be found here. Before we could combine datasets, we first needed to pivot the U.S. demographic dataset so that there was only one observation for each city. To do so, we used pivot_longer to change the Race variable into multiple Race variables (e.g. Asian, White), each with different values from Count. We then combined the police and demographic datasets by using inner_join with City as the primary key. We set the relationship to “many-to-many” so that fatalities from the same city all had the demographic data added. In the join, the State variables from each dataset got renamed to State.x and State.y. We used select to remove the redundant State.y variable and rename to rename State.x back to State. Some of the observations were missing Race values for the deceased individuals. As race is our main subject of interest, we removed these observations with filter. Our process of combining the datasets can be found here."
  },
  {
    "objectID": "big_picture.html#introduction",
    "href": "big_picture.html#introduction",
    "title": "Big Picture",
    "section": "Introduction:",
    "text": "Introduction:\nOn a windy fall evening in Minneapolis, Minnesota in 2021, a community gathered, candles in hand, and their faces revealed their pain. They were not just mourning for a loss, but also protesting for a pattern. Among them was one old lady, who held a placard in hand that read, ‘Justice for Daunte.’ Daunte Wright, a 20-year-old young Black man, had been stopped and shot by the police during a traffic violation earlier this month. The officer involved resigned and was charged with second-degree manslaughter pending investigation, but to those gathered, this was a familiar end to a grievously familiar story.\nDaunte’s story is not isolated. Across the United States, the tapestry of names, faces, and faded candles tells a similar tale. Data collected nationally reveals a stark reality: racial disparities pervade police-related fatalities, with Black individuals disproportionately represented. These are not just statistics; they are sons and daughters, friends and neighbors, whose stories demand a closer look.\nThis analysis aims to peel back the layers of data surrounding police-related fatalities to uncover how deeply race is intertwined with these fatal encounters. By examining the numbers, we seek to understand not only the scope of the disparities but also the human stories behind the statistics."
  },
  {
    "objectID": "big_picture.html#racial-breakdown-in-police-fatalities",
    "href": "big_picture.html#racial-breakdown-in-police-fatalities",
    "title": "Big Picture",
    "section": "Racial Breakdown in Police Fatalities:",
    "text": "Racial Breakdown in Police Fatalities:\n\n\n\n\n\nThis bar chart provides a stark visual comparison between the racial composition of police-related fatalities and the general population. The proportional representation of different racial groups in fatal encounters with law enforcement is measured against their proportion in the general U.S. population. The bars in red represent the general population percentages, while the blue bars represent those of police fatalities.\nFrom a glance, it is evident that certain groups appear more frequently in police fatality statistics than would be expected from their numbers in the general population. The discrepancy is particularly notable among Black or African-American individuals, whose representation in fatalities is substantially higher than in the population at large. This contrast suggests a troubling disparity that warrants a deeper investigation into the causes and conditions that lead to such a disproportionate impact."
  },
  {
    "objectID": "big_picture.html#dissecting-disparities",
    "href": "big_picture.html#dissecting-disparities",
    "title": "Big Picture",
    "section": "Dissecting Disparities",
    "text": "Dissecting Disparities\nThe implications of this visualization are profound. They highlight a need to examine not only the interactions leading to these fatal outcomes but also the broader systemic issues at play. Such disparities may stem from a complex interplay of socioeconomic factors, biases within law enforcement, and the lived realities of racialized communities.\nUnderstanding the ‘why’ behind these numbers is crucial. It beckons policymakers, social scientists, and community leaders to grapple with uncomfortable questions about equity, justice, and the role of policing in society. As we embark on this analytical journey, the data becomes more than just numbers; it becomes a narrative of lives affected and a roadmap for potential reform.\nThis plot serves as the opening chapter in our broader analysis, a quantifiable backdrop against which we’ll explore individual stories, regional specifics, and temporal shifts. It’s a visualization that doesn’t just inform but also challenges us to seek explanations and solutions for a more equitable future."
  },
  {
    "objectID": "big_picture.html#mapping-the-racial-divide-across-the-united-states",
    "href": "big_picture.html#mapping-the-racial-divide-across-the-united-states",
    "title": "Big Picture",
    "section": "Mapping the Racial Divide Across the United States",
    "text": "Mapping the Racial Divide Across the United States\n\n\n\n\n\nIn a revealing portrait of racial disparities, a new heat map of the United States uncovers the disproportionate representation of Black or African-American individuals in police fatalities across states. The visualization, borne out of recent comprehensive data analysis, brings to light the percentage of Black fatalities in relation to total police-involved deaths state by state, painting a picture of contrast that traverses the nation’s expanse.\nThe darkest shades on the map highlight regions with the highest percentages, pointing to a grim reality where the likelihood of police fatalities involving Black individuals far exceeds their population proportion. States like Illinois and Maryland emerge with a notably higher incidence, a reflection that compels introspection into the social and systemic underpinnings that perpetuate this national crisis.\nThe data-driven approach offers a clear indication of the underlying patterns of racial bias in law enforcement, raising questions about the factors driving such disparities. From the legislation corridors to community forums, the map fuels an ongoing debate on the urgent need for reform and accountability in policing methods. It not only serves as a tool for activists and policymakers to pinpoint areas for change but also acts as a stark reminder of the ongoing struggles against racial injustice that continue to divide communities across the country.\nThis heat map stands as a critical piece in the larger mosaic of racial dynamics within the United States, providing visual evidence that underscores the stark reality faced by the Black community. It beckons a call to action for a concerted effort to address and dismantle the systemic inequities that cast a long shadow over the promise of justice and equality for all."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a website for the final project for MA[46]15 Data Science with R by Team TEAM 2. The members of this team are below.\n\nChongjun Shan\nChongjun is an undergraduate majoring in Computer Science.\n\n\nCarolyn Beigh\nCarolyn is an undergraduate majoring in Marine Science with Biology and Statistics minors.\n\n\nHaotian Wu\nHaotian is an undergraduate double majoring in Pure & Applied Mathematics and Computer Science.\n\n\nChen Yang\nChen is a graduate MA from economic department."
  },
  {
    "objectID": "posts/2024-03-04-post-1/post-1.html",
    "href": "posts/2024-03-04-post-1/post-1.html",
    "title": "Post 1",
    "section": "",
    "text": "US Police Fatalities\nOriginal source link, Cleaned data link\nThe original data has 36 columns and 31,498 rows, including a header. The cleaned data has 12 columns and 12,491 rows, not including a header. The original data was compiled by Fatal Encounters which used media reports and police records to create the database The data includes deaths that happened when police were present or were caused by police. The goal of Fatal Encounters is to create a national database of people killed during interactions that anyone can use for whatever purpose. The cleaned data also lists the Gun Violence Archive and Data Society as sources. Both the Fatal Encounters and the cleaned data are able to be loaded into R without issue. Some obvious questions center around race and whether certain races experience more police fatalities than others. Similar questions can be asked about gender. We might also be interested in how police encounters change overtime, and that might be indicative of changes in police policy or training. Mental health might be another variable to study. There’s been some debate on whether police should be called in response to mental health crises and a closer look at the data might shed some light on that issue. The main challenge would be cleaning the data if we chose to use the original data set. Another challenge would be combining that data with other datasets that might add new perspectives. For example, adding demographic data for cities in the dataset or additional information about police training.\nCOVID-19 National Case Surveillance\nSource: https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data/vbim-akqf/about_data\nThis data set has 104,544,006 rows, 12 columns. This dataset is sourced from database of CDCP, Centers for Disease Control and Prevention, provided by CDC Data, Analytics and Visualization Task Force. The dataset is collected mainly for the tracking purpose of COVID-19 cases, and it includes detailed information of each infected patient, like demographics, any exposure history, disease severity indicators and outcomes, presence of any underlying medical conditions and risk behaviors. The only possible challenge might be the huge sample size.\nMain questions to address:\nAre there any correlation between COVID-19 cases and geographical differences? Which race and aged group are most likely to be infected by COVID-19?\nUSA Unemployment Rates by Demographics & Race Origin: https://fred.stlouisfed.org/series/CNP16OV, https://www.epi.org/data/ cleaned:https://www.kaggle.com/datasets/asaniczka/unemployment-rates-by-demographics-1978-2023\nRows:537, columns: 122 The data is sourced from the Economic Policy Institute’s State of Working America Data Library and economic research conducted by the Federal Reserve Bank of St. Louis. Questions: how unemployment rates have changed for different groups of people over time. Look into how education levels can affect unemployment rates. Use the data to create visuals that show how unemployment rates differ across all sorts of factors.\nHomicides data over the past decade in 50 US cities Original source: https://www.kaggle.com/datasets/joebeachcapital/homicides This dataset has 52179 rows and 12 columns. The Washington Post collected data on more than 52,000 criminal homicides over the past decade in 50 of the largest American cities. The data included the location of the killing, whether an arrest was made and, in most cases, basic demographic information about each victim. Reporters received data in many formats, including paper, and worked for months to clean and standardize it, comparing homicide counts and aggregate closure rates with FBI data to ensure the records were as accurate as possible.\nOn the geographical side, the main question I hope to address would probably be finding the cities that have the highest and lowest homicide rates over the past decade. Also I might want to address how homicide rates correlate with other city-specific factors, such as population density, economic indicators, or police force size.\nFor the data quality and completeness of this dataset, I think some of the challenges would be to verify the accuracy of demographic information and the potential underreporting of certain categories of homicides."
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2024-04-07-blog-post-4/blog-post-4.html",
    "href": "posts/2024-04-07-blog-post-4/blog-post-4.html",
    "title": "blog post 4",
    "section": "",
    "text": "Impact of Gender on Fatalities in Different States This plot focuses to explore the relationship between gender and fatalities across different states. This could help identify if certain states have more pronounced gender disparities in police-related fatalities.\n\nsuppressPackageStartupMessages(library(tidyverse))\n\nprc &lt;- read_csv(\"dataset/police_fatalities.csv\", show_col_types = FALSE)\n\nprc %&gt;%\n  na.omit() %&gt;%\n  count(State, Gender) %&gt;%\n  ggplot(aes(x = reorder(State, n), y = n, fill = Gender)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Police Fatalities by State and Gender\",\n       x = \"State\",\n       y = \"Number of Fatalities\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nYearly Trend of Police Fatalities by Race This plot the trend of police fatalities over the years, broken down by race. This can show if certain races have been more affected over time and how the trends have changed.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(readr)\n\npolice_data_clean &lt;- read_csv(\"dataset/police_fatalities.csv\", show_col_types = FALSE)\n\nannual_trends_by_race &lt;- police_data_clean %&gt;%\n  mutate(Date = mdy(Date), Year = year(Date)) %&gt;%\n  drop_na(Year, Race) %&gt;%\n  count(Year, Race) %&gt;%\n  spread(key = Race, value = n, fill = 0) %&gt;%\n  gather(key = 'Race', value = 'Fatalities', -Year)\n\nannual_trends_by_race %&gt;%\n  ggplot(aes(x = Year, y = Fatalities, color = Race)) +\n  geom_line() + \n  geom_point() + \n  labs(title = \"Yearly Trend of Police Fatalities by Race\",\n       x = \"Year\",\n       y = \"Number of Fatalities\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.title = element_blank()) \n\n\n\n\nAge Histogram: Displays the frequency distribution of individuals’ ages involved in the incidents.\nBar Plot of Manner of Death by Gender and Race: Compares the number of incidents by manner of death across different races and genders.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(broom)\n\nlibrary(nnet)\n\ndata &lt;- read.csv(\"dataset/police_fatalities.csv\") %&gt;%\n  mutate(\n    Date = mdy(Date), \n    Gender = as.factor(Gender),\n    Race = as.factor(Race),\n    Manner_of_death = as.factor(Manner_of_death),\n    Armed = as.factor(Armed),\n    Mental_illness = as.factor(Mental_illness),\n    Flee = as.factor(Flee)\n  )\n\nsummary_statistics &lt;- data %&gt;%\n  summarise(\n    Average_Age = mean(Age, na.rm = TRUE),\n    SD_Age = sd(Age, na.rm = TRUE),\n    Count = n()\n  )\n\nage_histogram &lt;- ggplot(data, aes(x = Age)) +\n  geom_histogram(binwidth = 5, fill = \"blue\", color = \"black\") +\n  theme_minimal() +\n  labs(title = \"Distribution of Age\", x = \"Age\", y = \"Count\")\n\ndeath_by_demographics &lt;- ggplot(data, aes(x = Race, fill = Gender)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal() +\n  labs(title = \"Manner of Death by Gender and Race\", x = \"Race\", y = \"Count\")\n\nlogistic_model &lt;- glm(Armed ~ Age + Gender + Race + Mental_illness, \n                      data = data, family = \"binomial\")\nlogistic_model_summary &lt;- summary(logistic_model)\n\nmultinom_model &lt;- multinom(Manner_of_death ~ Age + Gender + Race + Mental_illness, data = data)\n\n# weights:  48 (33 variable)\ninitial  value 16993.196279 \niter  10 value 3526.384408\niter  20 value 3018.452673\niter  30 value 2924.878850\niter  40 value 2899.497411\niter  50 value 2899.477616\niter  60 value 2899.473481\niter  60 value 2899.473459\niter  60 value 2899.473459\nfinal  value 2899.473459 \nconverged\n\nmultinom_model_summary &lt;- summary(multinom_model)\n\nlogistic_model_table &lt;- tidy(logistic_model)\n\nprint(age_histogram)\n\nWarning: Removed 233 rows containing non-finite values (`stat_bin()`).\n\n\n\n\nprint(death_by_demographics)\n\n\n\nprint(logistic_model_table)\n\n# A tibble: 11 × 5\n   term               estimate std.error statistic  p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)        -0.693     0.919      -0.754 4.51e- 1\n 2 Age                 0.00396   0.00146     2.71  6.70e- 3\n 3 GenderFemale        0.374     0.922       0.406 6.85e- 1\n 4 GenderMale          0.518     0.918       0.564 5.73e- 1\n 5 RaceAsian           0.0773    0.157       0.492 6.23e- 1\n 6 RaceBlack           0.248     0.0523      4.73  2.24e- 6\n 7 RaceHispanic        0.124     0.0583      2.12  3.38e- 2\n 8 RaceNative          0.238     0.180       1.32  1.87e- 1\n 9 RaceOther          -0.262     0.293      -0.894 3.72e- 1\n10 RaceWhite           0.350     0.0465      7.53  5.10e-14\n11 Mental_illnessTRUE  0.216     0.0456      4.73  2.23e- 6"
  },
  {
    "objectID": "posts/2024-03-29-blog-post-3/blog-post-3.html",
    "href": "posts/2024-03-29-blog-post-3/blog-post-3.html",
    "title": "Blog Post 3",
    "section": "",
    "text": "Data for Equity\nPrinciple 1: One of the important principles in the process of data processing analysis is to be transparent about the limits of the data. While we are examining and cleaning the data, we noticed that there are many limitations with our data set in the research of US Police Involved Fatalities. Specifically, our data is effective while we are trying to examine the relationship between the police involvement and the fatalities of citizens. However, the data is definitely time-sensitive and our finding through this data set may not imply the Police involved fatalities in the past years. Besides, there is also missing data like whether the suspect is armed. If ARMED shows as blank then there is no record of whether the suspect is armed or not, which may play an important role in our data analysis and our result.\nPrinciple 2: For the beneficence principle regarding our dataset, it contains only the name of each police fatalities with no other sensitive information displayed. This could ensure the privacy of the person as well as their families. All other columns of the dataset are only served for the purpose of analyzing without disclosing much information about a single person. The dataset also contains a column called “uid” that can be uniquely identified across the dataset and can be used to substitute the function of names in certain occasions.\nPrinciple 3: another important principle is the Inclusivity of the data in representation. We use this principle to emphasize the importance of representing all affected communities fairly and accurately in the dataset. In the US Police Fatalities dataset, it literally means all the data reflected the demographic characteristics of those killed by police deaths, including race, age, gender, and mental health status. It can be essential in that it is an effective way to gain an insight into potential biases or differences in the ways diverse social groups may be affected by police action.\nPrinciple 4: We also need to mention accountability when we use data in real world practice. In the practice of data, we need to address any harm that the dataset and its analysis may cause, particularly to the minority communities. In the dataset US Police Fatalities, there’s a significant risk that improper interpretation of the data could underscore the stereotypes or contribute to unjust narratives about certain groups.\nWeekly Summary:\nWe finished the Data Equity part.\nWe thought about 3-4 principles and how they could be relevant to our US Police Fatalities dataset. These principles help our data set to be used ethically and constructively.\nThe main aspect of our data principles is the potential for abuse or misuse based on the potential biased narratives in our dataset.\nWe started to do some data exploration with plots and tables. It might be interesting if we combined our dataset with another containing more population information for the states. We have a homicide dataset that we might want to add to our police fatality dataset. We also thought about how to construct our data plan and what questions we want to ask of our dataset.\nExploration\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nprc &lt;- read_csv(\"dataset/police_fatalities.csv\")\n\nRows: 12491 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Name, Gender, Race, Date, City, State, Manner_of_death, Armed\ndbl (2): UID, Age\nlgl (2): Mental_illness, Flee\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nprc |&gt; \n  na.omit() |&gt;  \n  ggplot(aes(x = Race, fill = Gender)) + \n  geom_bar()\n\n\n\n\n\nprc |&gt; na.omit() |&gt; group_by(State) |&gt; summarize(n = n()) |&gt; arrange(-n)\n\n# A tibble: 51 × 2\n   State     n\n   &lt;chr&gt; &lt;int&gt;\n 1 CA      966\n 2 TX      494\n 3 FL      310\n 4 AZ      194\n 5 NV      153\n 6 NY      137\n 7 WA      137\n 8 CO      125\n 9 OH      123\n10 IL      121\n# ℹ 41 more rows\n\n\nBelow is a plot displaying the average age of police fatalities each year, separated by race.\n\nsuppressWarnings({\nlibrary(tidyverse)\nlibrary(lubridate)\n\npolice_data_clean &lt;- read_csv(here::here(\"dataset\", \"police_fatalities.csv\"))\n\npolice_fatalities_summary &lt;- police_data_clean %&gt;%\n  mutate(Date = dmy(Date),  \n         Year = year(Date)) %&gt;%  \n  group_by(Year, Race) %&gt;% \n  summarise(average_age = mean(Age, na.rm = TRUE)) \n\npolice_fatalities_summary %&gt;%\n  ggplot(aes(x = Year, y = average_age, color = Race)) +\n  geom_line() +  \n  geom_point() + \n  labs(\n    title = \"Average Age of Police Fatalities by Year and Race\",\n    x = \"Year\",\n    y = \"Average Age\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) \n})\n\nRows: 12491 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Name, Gender, Race, Date, City, State, Manner_of_death, Armed\ndbl (2): UID, Age\nlgl (2): Mental_illness, Flee\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'Year'. You can override using the `.groups` argument.\n\n\nWarning: Removed 7 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 7 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2024-04-24-blog-post-7/blog-post-7.html",
    "href": "posts/2024-04-24-blog-post-7/blog-post-7.html",
    "title": "Blog Post 7",
    "section": "",
    "text": "source(\"combining-datasets.R\")\nlibrary(dplyr)\nlibrary(tibble)  # For more robust data frame operations\n\n# Assuming police_dem is already loaded in your R environment\n\n# Calculating total deaths and racial composition percentages\ndeath_counts &lt;- police_dem %&gt;%\n  group_by(State, City) %&gt;%\n  summarize(\n    TotalDeaths = n(),  # Count of deaths\n    .groups = 'drop'  # Drop grouping structure after summarizing\n  )\n\nracial_composition &lt;- police_dem %&gt;%\n  group_by(State, City, Race) %&gt;%\n  summarize(\n    Count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  group_by(State, City) %&gt;%\n  mutate(\n    Percentage = Count / sum(Count) * 100\n  )\n\n# Calculate discrepancies (Example: comparing racial percentages with total deaths)\ndiscrepancies &lt;- racial_composition %&gt;%\n  left_join(death_counts, by = c(\"State\", \"City\")) %&gt;%\n  mutate(\n    Discrepancy = abs(Percentage - (TotalDeaths / sum(TotalDeaths) * 100))\n  ) %&gt;%\n  arrange(desc(Discrepancy))\n\n# Identify and display locations with the highest discrepancies\ntop_discrepancies &lt;- discrepancies %&gt;%\n  top_n(1, Discrepancy)\n\nprint(top_discrepancies)\n\n# A tibble: 558 × 7\n# Groups:   State, City [456]\n   State                City      Race  Count Percentage TotalDeaths Discrepancy\n   &lt;chr&gt;                &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;      &lt;dbl&gt;       &lt;int&gt;       &lt;dbl&gt;\n 1 District of Columbia Washingt… Blac…    24       92.3          26        59.0\n 2 Pennsylvania         Philadel… Blac…    15       83.3          18        58.3\n 3 Louisiana            New Orle… Blac…    25       86.2          29        52.9\n 4 California           Oakland   Blac…    30       75            40        50  \n 5 California           Santa Ana Hisp…    18       75            24        50  \n 6 Nevada               Henderson White    14       73.7          19        48.7\n 7 Oregon               Portland  White    28       73.7          38        48.7\n 8 Illinois             Chicago   Blac…    67       73.6          91        48.6\n 9 Nevada               Reno      White    19       73.1          26        48.1\n10 Washington           Spokane   White    14       77.8          18        44.4\n# ℹ 548 more rows\n\n\n\nlibrary(dplyr)\n\n# Assuming police_dem is already loaded in your R environment\n\n# Calculating total deaths for each race\nracial_composition &lt;- police_dem %&gt;%\n  group_by(Race) %&gt;%\n  summarize(\n    TotalDeaths = n(),  # Count of deaths per race\n    .groups = 'drop'  # Drop grouping structure after summarizing\n  )\n\n# Calculate the death rate for each race\nracial_composition &lt;- racial_composition %&gt;%\n  mutate(\n    DeathRate = TotalDeaths / sum(TotalDeaths) * 100  # Convert to percentage\n  ) %&gt;%\n  arrange(desc(DeathRate))\n\n# Display the death rates for all races\nprint(racial_composition)\n\n# A tibble: 5 × 3\n  Race                              TotalDeaths DeathRate\n  &lt;chr&gt;                                   &lt;int&gt;     &lt;dbl&gt;\n1 Black or African-American                1441     35.8 \n2 White                                    1369     34.0 \n3 Hispanic or Latino                       1070     26.6 \n4 Asian                                     101      2.51\n5 American Indian and Alaska Native          41      1.02\n\n# Identify and display the race with the highest death rate\nhighest_death_rate_race &lt;- racial_composition %&gt;%\n  slice_max(DeathRate, n = 1)  # Get the race with the highest death rate\n\nprint(highest_death_rate_race)\n\n# A tibble: 1 × 3\n  Race                      TotalDeaths DeathRate\n  &lt;chr&gt;                           &lt;int&gt;     &lt;dbl&gt;\n1 Black or African-American        1441      35.8"
  },
  {
    "objectID": "posts/2024-04-24-blog-6/blog-6.html",
    "href": "posts/2024-04-24-blog-6/blog-6.html",
    "title": "Post 6",
    "section": "",
    "text": "We further cleaned our data since we found that there are repeated police filing for the same event, which caused our dataset to have overlapped individual records but different details. Since we cannot confirm which filing is more accurate, we decided to remove all the data that has repeated filing. Looking at our combined dataset with the demographic information of each city, we thought about calculating the mortality rate of each race in each city, and even the cities that have abnormal rates compared to the “general pattern”.\nOur thesis statement is:\nRace disparity contributes to a great factor of the demographics of police-related fatalities.\nRace Black has the highest mortality rate among all the cities on average."
  },
  {
    "objectID": "posts/2024-04-15-post-5-team-2/post-5-team-2.html",
    "href": "posts/2024-04-15-post-5-team-2/post-5-team-2.html",
    "title": "Post 5",
    "section": "",
    "text": "We combined police fatalities data set with us cities demographics data set with an R script called combining-data sets. The us cities demographics data set is originally one of the data sets comes from our brain storming in the Post 1 and we consider it is interested to see some interaction between our current data set. In order to make these two data set fit, we pivot the cities demographics data set so that there is only one observation for each city and count for the each races for their own column. Then, we inner join to keep only the data that has a match from each data set with city variable, and rename unnecessary name. Finally, we filter out all the observations with missing Race values as race is the main variable of interest."
  },
  {
    "objectID": "posts/2024-03-22-post-2-/post-2-.html",
    "href": "posts/2024-03-22-post-2-/post-2-.html",
    "title": "Post 2",
    "section": "",
    "text": "We are working with the US Police Fatalities data set. For this post, we Set up data loading and cleaning in the load_and_clean_data.R file. The required rds file was created. Since the data set we found was already cleaned, we did not do anything specific to clean the data further.\nAnswer all the questions from the data background part and update it to the data page in the website. \n\nWrite some paragraphs about data equity.(still working on it, so this is not updated)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nPost 7\n\n\n\n\n\nPost 7 from team 2\n\n\n\n\n\n\nMay 1, 2024\n\n\n\n\n\n\n  \n\n\n\n\nPost 6\n\n\n\n\n\nPost 6 from team 2\n\n\n\n\n\n\nApr 24, 2024\n\n\n\n\n\n\n  \n\n\n\n\nPost 5\n\n\n\n\n\nPost 5 from team 2\n\n\n\n\n\n\nApr 15, 2024\n\n\n\n\n\n\n  \n\n\n\n\nPost 4\n\n\n\n\n\nPost 4 from team 2\n\n\n\n\n\n\nApr 7, 2024\n\n\n\n\n\n\n  \n\n\n\n\nPost 3\n\n\n\n\n\nPost 3 from team 2\n\n\n\n\n\n\nMar 29, 2024\n\n\n\n\n\n\n  \n\n\n\n\nPost 2\n\n\n\n\n\nPost 2 from team 2\n\n\n\n\n\n\nMar 22, 2024\n\n\n\n\n\n\n  \n\n\n\n\nPost 1\n\n\n\n\n\nPost 1 from team 2\n\n\n\n\n\n\nMar 4, 2024\n\n\nTeam 2\n\n\n\n\n\n\n  \n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post.\n\n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nTest Post\n\n\n\n\n\nTesting\n\n\n\n\n\n\nFeb 23, 2024\n\n\nCarolyn Beigh\n\n\n\n\n\n\n  \n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting.\n\n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data.\n\n\n##Introduction In recent years, police-related fatalities have become a focal point of public and academic debate, underscoring critical concerns about racial disparities within law enforcement across the United States. This analysis seeks to explore the extent to which racial disparity, especially against Black individuals, contributes to the demographics of these fatalities. The urgency of this issue is amplified by ongoing societal calls for justice and reform, making it essential to understand the patterns and factors that drive these disparities.\n#Thesis Statement This research posits that racial disparity is a significant factor influencing the demographics of police-related fatalities, with Black individuals experiencing disproportionately high mortality rates in comparison to other races across various cities.\n#Research Questions To provide a structured exploration of this issue, this analysis will address the following key questions:\n\nHow do mortality rates from police-related fatalities among Black individuals compare to those of other races across different cities?\nWhat factors might contribute to any observed disparities in these mortality rates?\nAre there specific geographic or demographic contexts in which these disparities are more pronounced?\n\n#Significance of the Analysis The findings of this analysis are intended to contribute to the broader dialogue on racial equity and law enforcement practices. By examining the intersection of race and police-related fatalities, this study aims to provide data-driven insights that could inform policy decisions and advocacy efforts aimed at reducing these disparities. Additionally, this analysis will serve as an educational resource for scholars, policymakers, and the public, fostering a deeper understanding of how racial dynamics shape outcomes in law enforcement encounters.\nThrough a combination of quantitative methods and statistical modeling, this analysis page will offer a detailed examination of the available data, aiming to present a clear and comprehensive picture of how race influences the risk of fatality in police interactions. The next sections will describe the data used for this analysis, the methodology employed, and the results obtained,thereby framing the context for a rigorous discussion on this critical social issue.\n##Data Description This analysis leverages two key datasets:\nPolice Fatality Dataset & U.S. Cities Demographic Dataset\nTo load the dataset, run the attached R script “combining-datasets.R”.\n\nsource(\"combining-datasets.R\")\nrm(police_f,us_dem,us_dem_wid)\n\nAfter running this R script, the two datasets we used will be merged, cleared and loaded into your environment called police_dem. For more information on these two datasets and merging process, see the data page. data page.\n##Exploratory Data Analysis (EDA)\nStarted by exploring the distribution of police-related fatalities by race.\n\nfatalities_by_race &lt;- police_dem |&gt;\n  group_by(Race) |&gt;\n  summarize(Fatalities = n()) |&gt;\n  ungroup()\n\nggplot(fatalities_by_race, aes(x = reorder(Race, -Fatalities), y = Fatalities, fill = Race)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Distribution of Police-Related Fatalities by Race\",\n    x = \"Race\",\n    y = \"Number of Fatalities\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nAs expected, the highest number of deaths occurred among blacks or African Americans.\nThe next step is to compare this with demographic data, and it would be interesting to examine the relationship between each racial group’s percentage of total deaths and their percentage of the total population of the sampled city. This would help to determine how specific racial groups are disproportionately affected.\n\nfatalities_by_race_city_state &lt;- police_dem |&gt;\n  group_by(State, City, Race) |&gt;\n  summarize(Fatalities = n(), .groups = 'drop')\n\npopulation_by_race_city_state &lt;- police_dem |&gt;\n  group_by(State, City) |&gt;\n  summarise(\n    White = first(White),\n    `Black or African-American` = first(`Black or African-American`),\n    `Hispanic or Latino` = first(`Hispanic or Latino`),\n    Asian = first(Asian),\n    `American Indian and Alaska Native` = first(`American Indian and Alaska Native`),\n    .groups = 'drop'\n  )|&gt;\n  pivot_longer(cols = -c(City, State), names_to = \"Race\", values_to = \"Population\")\n\ncomparison_df &lt;- \n  left_join(fatalities_by_race_city_state, population_by_race_city_state, by = c(\"State\", \"City\", \"Race\"))|&gt;\n  mutate(\n    FatalityPercentage = (Fatalities / sum(Fatalities)) * 100,\n    PopulationPercentage = (Population / sum(Population)) * 100,\n    diffinPercentage = FatalityPercentage - PopulationPercentage\n  )|&gt;\n  arrange(desc(diffinPercentage))\n\n\nsum_df&lt;- comparison_df|&gt;\n  group_by(Race)|&gt;\n  summarise(sumfatalityp = sum(FatalityPercentage),\n          sumpopulationp = sum(PopulationPercentage))|&gt;\n  pivot_longer(cols = c(sumfatalityp, sumpopulationp), names_to = \"Type\", values_to = \"Percentage\") %&gt;%\n  mutate(Type = recode(Type, 'sumfatalityp' = 'Fatality Percentage', 'sumpopulationp' = 'Population Percentage'))\n\nggplot(sum_df, aes(x = Type, y = Percentage, fill = Type)) +\n  geom_col(position = position_dodge(width = 0.8)) +\n  facet_wrap(~ Race) +\n  labs(\n    title = \"Comparison of Fatality and Population Percentages by Race\",\n    x = NULL,\n    y = \"Percentage\"\n  ) +\n  scale_fill_manual(values = c(\"Fatality Percentage\" = \"red\", \"Population Percentage\" = \"blue\")) +\n  theme(\n    axis.text.x = element_blank(),  \n    axis.ticks.x = element_blank(), \n    strip.background = element_blank(),\n    strip.text.x = element_text(size = 10)\n  ) \n\n\n\n\n\nlibrary(tidycensus)\nlibrary(sf)\n\n\nstates_sf &lt;- get_decennial(geography = \"state\", \n                           year = 2020, \n                           variables = \"H1_001N\", \n                           geometry = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\nstate_level_comparison_df &lt;- comparison_df %&gt;%\n  group_by(State, Race) %&gt;%\n  summarize(Diff_Percentage = mean(diffinPercentage, na.rm = TRUE), .groups = 'drop')\n\nstate_data_sf &lt;- inner_join(states_sf, state_level_comparison_df, by = c(\"NAME\" = \"State\"))\n\n\nstate_data_sf %&gt;%\n  ggplot() +\n  geom_sf(aes(fill = Diff_Percentage), color = NA) +\n  scale_fill_gradient2(\n    low = \"blue\", mid = \"white\", high = \"red\",  # Adjust colors if needed\n    midpoint = 0, \n    limit = c(min(state_data_sf$Diff_Percentage, na.rm = TRUE), max(state_data_sf$Diff_Percentage, na.rm = TRUE)), \n    name = \"Diff in Percentage\",\n    na.value = \"white\"\n  ) +\n  facet_wrap(~ Race) +\n  labs(\n    title = \"Difference in Police Fatality and Population Percentages by Race and State\",\n    subtitle = \"Faceted by Race\",\n    fill = \"Diff in Percentage\"\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(size = 14, hjust = 0.5), \n    plot.subtitle = element_text(size = 12),\n    strip.text = element_text(size = 10)  # Adjust facet label text size\n  ) +\n  coord_sf(xlim = c(-125, -67), ylim = c(24, 50), expand = FALSE)\n\n\n\n\n##Modelling\n\nmodel_data&lt;- police_dem|&gt;\n  group_by(City)|&gt;\n  summarise(Fatalities = n(),\n            Median.Age = first(Median.Age),\n            Male.Population = first(Male.Population),\n            Female.Population = first(Female.Population),\n            Foreign.born = first(Foreign.born),\n            Average.Household.Size = first(Average.Household.Size),\n            White = first(White),\n            `Black or African-American` = first(`Black or African-American`),\n            `Hispanic or Latino` = first(`Hispanic or Latino`),\n            Asian = first(Asian),\n            `American Indian and Alaska Native` = first(`American Indian and Alaska Native`)\n            )|&gt;\n  arrange(desc(Fatalities))\nmodel_data &lt;- na.omit(model_data)\nprint(model_data)\n\n# A tibble: 401 × 12\n   City     Fatalities Median.Age Male.Population Female.Population Foreign.born\n   &lt;chr&gt;         &lt;int&gt;      &lt;dbl&gt;           &lt;int&gt;             &lt;int&gt;        &lt;int&gt;\n 1 Los Ang…        252       35           1958998           2012898      1485425\n 2 Houston         179       32.6         1149686           1148942       696210\n 3 Las Veg…        133       37.5          310568            313201       127609\n 4 Chicago          91       34.2         1320015           1400541       573463\n 5 Dallas           78       32.6          639019            661063       326825\n 6 Phoenix          75       33.8          786833            776168       300702\n 7 Fresno           67       30            256130            263942       103453\n 8 Kansas …         61       35.9          228430            246931        37787\n 9 San Die…         59       34.5          693826            701081       373842\n10 Long Be…         52       34.6          238159            236013       127764\n# ℹ 391 more rows\n# ℹ 6 more variables: Average.Household.Size &lt;dbl&gt;, White &lt;int&gt;,\n#   `Black or African-American` &lt;int&gt;, `Hispanic or Latino` &lt;int&gt;, Asian &lt;int&gt;,\n#   `American Indian and Alaska Native` &lt;int&gt;\n\n\n\nggplot(model_data, aes(x = Fatalities)) + \n  geom_histogram(binwidth = 1, fill = 'blue', color = 'black')\n\n\n\n\nThe distrubution is highly skewed\n\nggplot(model_data, aes(x = Median.Age, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(model_data, aes(x = Male.Population, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(model_data, aes(x = Female.Population, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(model_data, aes(x = Foreign.born, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(model_data, aes(x = Average.Household.Size, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(model_data, aes(x = White, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(model_data, aes(x = `Black or African-American`, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nggplot(model_data, aes(x = Male.Population, y = Median.Age)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(model_data, aes(x = Female.Population, y = Median.Age)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(model_data, aes(x = Foreign.born, y = Median.Age)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(model_data, aes(x = Average.Household.Size, y = Median.Age)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(model_data, aes(x = White, y = Median.Age)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(model_data, aes(x = `Black or African-American`, y = Median.Age)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nggplot(model_data, aes(x = Female.Population, y = Male.Population)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(model_data, aes(x = Foreign.born, y = Male.Population)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(model_data, aes(x = Average.Household.Size, y = Male.Population)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(model_data, aes(x = White, y = Male.Population)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(model_data, aes(x = `Black or African-American`, y = Male.Population)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "draft for analysis.html",
    "href": "draft for analysis.html",
    "title": "draft for analysis",
    "section": "",
    "text": "##Introduction In recent years, police-related fatalities have become a focal point of public and academic debate, underscoring critical concerns about racial disparities within law enforcement across the United States. This analysis seeks to explore the extent to which racial disparity, especially against Black individuals, contributes to the demographics of these fatalities. The urgency of this issue is amplified by ongoing societal calls for justice and reform, making it essential to understand the patterns and factors that drive these disparities.\n#Thesis Statement This research posits that racial disparity is a significant factor influencing the demographics of police-related fatalities, with Black individuals experiencing disproportionately high mortality rates in comparison to other races across various cities.\n#Research Questions To provide a structured exploration of this issue, this analysis will address the following key questions:\n\nHow do mortality rates from police-related fatalities among Black individuals compare to those of other races across different cities?\nWhat factors might contribute to any observed disparities in these mortality rates?\nAre there specific geographic or demographic contexts in which these disparities are more pronounced?\n\n#Significance of the Analysis The findings of this analysis are intended to contribute to the broader dialogue on racial equity and law enforcement practices. By examining the intersection of race and police-related fatalities, this study aims to provide data-driven insights that could inform policy decisions and advocacy efforts aimed at reducing these disparities. Additionally, this analysis will serve as an educational resource for scholars, policymakers, and the public, fostering a deeper understanding of how racial dynamics shape outcomes in law enforcement encounters.\nThrough a combination of quantitative methods and statistical modeling, this analysis page will offer a detailed examination of the available data, aiming to present a clear and comprehensive picture of how race influences the risk of fatality in police interactions. The next sections will describe the data used for this analysis, the methodology employed, and the results obtained,thereby framing the context for a rigorous discussion on this critical social issue.\n##Data Description This analysis leverages two key datasets:\nPolice Fatality Dataset & U.S. Cities Demographic Dataset\nTo load the dataset, run the attached R script “combining-datasets.R”.\n\nsource(\"combining-datasets.R\")\nrm(police_f,us_dem,us_dem_wid)\n\nAfter running this R script, the two datasets we used will be merged, cleared and loaded into your environment called police_dem. For more information on these two datasets and merging process, see the data page. data page.\n##Exploratory Data Analysis (EDA)\nStarted by exploring the distribution of police-related fatalities by race.\n\nfatalities_by_race &lt;- police_dem |&gt;\n  group_by(Race) |&gt;\n  summarize(Fatalities = n()) |&gt;\n  ungroup()\n\nggplot(fatalities_by_race, aes(x = reorder(Race, -Fatalities), y = Fatalities, fill = Race)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Distribution of Police-Related Fatalities by Race\",\n    x = \"Race\",\n    y = \"Number of Fatalities\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nAs expected, the highest number of deaths occurred among blacks or African Americans.\nThe next step is to compare this with demographic data, and it would be interesting to examine the relationship between each racial group’s percentage of total deaths and their percentage of the total population of the sampled city. This would help to determine how specific racial groups are disproportionately affected.\n\nfatalities_by_race_city_state &lt;- police_dem |&gt;\n  group_by(State, City, Race) |&gt;\n  summarize(Fatalities = n(), .groups = 'drop')\n\npopulation_by_race_city_state &lt;- police_dem |&gt;\n  group_by(State, City) |&gt;\n  summarise(\n    White = first(White),\n    `Black or African-American` = first(`Black or African-American`),\n    `Hispanic or Latino` = first(`Hispanic or Latino`),\n    Asian = first(Asian),\n    `American Indian and Alaska Native` = first(`American Indian and Alaska Native`),\n    .groups = 'drop'\n  )|&gt;\n  pivot_longer(cols = -c(City, State), names_to = \"Race\", values_to = \"Population\")\n\ncomparison_df &lt;- \n  left_join(fatalities_by_race_city_state, population_by_race_city_state, by = c(\"State\", \"City\", \"Race\"))|&gt;\n  mutate(\n    FatalityPercentage = (Fatalities / sum(Fatalities)) * 100,\n    PopulationPercentage = (Population / sum(Population)) * 100,\n    diffinPercentage = FatalityPercentage - PopulationPercentage\n  )|&gt;\n  arrange(desc(diffinPercentage))\nprint(comparison_df)\n\n# A tibble: 922 × 8\n   State      City        Race          Fatalities Population FatalityPercentage\n   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;              &lt;int&gt;      &lt;int&gt;              &lt;dbl&gt;\n 1 California Los Angeles Black or Afr…         87     404868              2.16 \n 2 Texas      Houston     Black or Afr…         82     529431              2.04 \n 3 California Los Angeles Hispanic or …        122    1936732              3.03 \n 4 Nevada     Las Vegas   White                 56     429142              1.39 \n 5 Illinois   Chicago     Black or Afr…         67     873316              1.67 \n 6 Nevada     Las Vegas   Black or Afr…         36      84987              0.895\n 7 Nevada     Las Vegas   Hispanic or …         38     204913              0.945\n 8 Texas      Dallas      Black or Afr…         38     322570              0.945\n 9 California Oakland     Black or Afr…         30     118228              0.746\n10 California Fresno      Hispanic or …         33     256145              0.820\n# ℹ 912 more rows\n# ℹ 2 more variables: PopulationPercentage &lt;dbl&gt;, diffinPercentage &lt;dbl&gt;\n\n\n\nsum_df&lt;- comparison_df|&gt;\n  group_by(Race)|&gt;\n  summarise(sumfatalityp = sum(FatalityPercentage),\n          sumpopulationp = sum(PopulationPercentage))|&gt;\n  pivot_longer(cols = c(sumfatalityp, sumpopulationp), names_to = \"Type\", values_to = \"Percentage\") %&gt;%\n  mutate(Type = recode(Type, 'sumfatalityp' = 'Fatality Percentage', 'sumpopulationp' = 'Population Percentage'))\n\nggplot(sum_df, aes(x = Type, y = Percentage, fill = Type)) +\n  geom_col(position = position_dodge(width = 0.8)) +\n  facet_wrap(~ Race) +\n  labs(\n    title = \"Comparison of Fatality and Population Percentages by Race\",\n    x = NULL,\n    y = \"Percentage\"\n  ) +\n  scale_fill_manual(values = c(\"Fatality Percentage\" = \"red\", \"Population Percentage\" = \"blue\")) +\n  theme(\n    axis.text.x = element_blank(),  \n    axis.ticks.x = element_blank(), \n    strip.background = element_blank(),\n    strip.text.x = element_text(size = 10)\n  ) \n\n\n\n\n\nlibrary(tidycensus)\nlibrary(sf)\n\n\nstates_sf &lt;- get_decennial(geography = \"state\", \n                           year = 2020, \n                           variables = \"H1_001N\", \n                           geometry = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\nstate_level_comparison_df &lt;- comparison_df %&gt;%\n  group_by(State, Race) %&gt;%\n  summarize(Diff_Percentage = mean(diffinPercentage, na.rm = TRUE), .groups = 'drop')\n\nstate_data_sf &lt;- inner_join(states_sf, state_level_comparison_df, by = c(\"NAME\" = \"State\"))\n\n\nstate_data_sf %&gt;%\n  ggplot() +\n  geom_sf(aes(fill = Diff_Percentage), color = NA) +\n  scale_fill_gradient2(\n    low = \"blue\", mid = \"white\", high = \"red\",  # Adjust colors if needed\n    midpoint = 0, \n    limit = c(min(state_data_sf$Diff_Percentage, na.rm = TRUE), max(state_data_sf$Diff_Percentage, na.rm = TRUE)), \n    name = \"Diff in Percentage\",\n    na.value = \"white\"\n  ) +\n  facet_wrap(~ Race) +\n  labs(\n    title = \"Difference in Police Fatality and Population Percentages by Race and State\",\n    subtitle = \"Faceted by Race\",\n    fill = \"Diff in Percentage\"\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(size = 14, hjust = 0.5), \n    plot.subtitle = element_text(size = 12),\n    strip.text = element_text(size = 10)  # Adjust facet label text size\n  ) +\n  coord_sf(xlim = c(-125, -67), ylim = c(24, 50), expand = FALSE)\n\n\n\n\n##Modelling\n\nmodel_data&lt;- police_dem|&gt;\n  group_by(City)|&gt;\n  summarise(Fatalities = n(),\n            Median.Age = first(Median.Age),\n            Male.Population = first(Male.Population),\n            Female.Population = first(Female.Population),\n            Foreign.born = first(Foreign.born),\n            Average.Household.Size = first(Average.Household.Size),\n            White = first(White),\n            `Black or African-American` = first(`Black or African-American`),\n            `Hispanic or Latino` = first(`Hispanic or Latino`),\n            Asian = first(Asian),\n            `American Indian and Alaska Native` = first(`American Indian and Alaska Native`)\n            )|&gt;\n  arrange(desc(Fatalities))\nmodel_data &lt;- na.omit(model_data)\nprint(model_data)\n\n# A tibble: 401 × 12\n   City     Fatalities Median.Age Male.Population Female.Population Foreign.born\n   &lt;chr&gt;         &lt;int&gt;      &lt;dbl&gt;           &lt;int&gt;             &lt;int&gt;        &lt;int&gt;\n 1 Los Ang…        252       35           1958998           2012898      1485425\n 2 Houston         179       32.6         1149686           1148942       696210\n 3 Las Veg…        133       37.5          310568            313201       127609\n 4 Chicago          91       34.2         1320015           1400541       573463\n 5 Dallas           78       32.6          639019            661063       326825\n 6 Phoenix          75       33.8          786833            776168       300702\n 7 Fresno           67       30            256130            263942       103453\n 8 Kansas …         61       35.9          228430            246931        37787\n 9 San Die…         59       34.5          693826            701081       373842\n10 Long Be…         52       34.6          238159            236013       127764\n# ℹ 391 more rows\n# ℹ 6 more variables: Average.Household.Size &lt;dbl&gt;, White &lt;int&gt;,\n#   `Black or African-American` &lt;int&gt;, `Hispanic or Latino` &lt;int&gt;, Asian &lt;int&gt;,\n#   `American Indian and Alaska Native` &lt;int&gt;\n\n\n\nggplot(model_data, aes(x = Fatalities)) + \n  geom_histogram(binwidth = 1, fill = 'blue', color = 'black')\n\n\n\n\nThe distrubution is highly skewed\n\nggplot(model_data, aes(x = Median.Age, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nggplot(model_data, aes(x = Male.Population, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nggplot(model_data, aes(x = Female.Population, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nggplot(model_data, aes(x = Foreign.born, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nggplot(model_data, aes(x = Average.Household.Size, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nggplot(model_data, aes(x = White, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nggplot(model_data, aes(x = `Black or African-American`, y = Fatalities)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  }
]